{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yoshida-2002/thesis/blob/main/CEG_omo_on_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lefh-Mu8KRcM",
        "outputId": "19fa2fca-ae67-4784-975c-9604cf14f2e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "aSs33HuiK1M6",
        "outputId": "0bc82549-c191-4466-d21c-0104d3080c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning\n",
            "' CEG omo on colab'                            'd[3:4].tsv'       models                sample2.tsv\n",
            "' CEG omo on colab for train data extention1'   \u001b[0m\u001b[01;34mdata\u001b[0m/            'models のコピー'      \u001b[01;34msrc\u001b[0m/\n",
            "' CEG omo on colab for train data extention2'  'data for CEG'     \u001b[01;34mmodel_transformers\u001b[0m/\n",
            "' CEG omo on colab for train data extention3'   \u001b[01;34mlightning_logs\u001b[0m/   \u001b[01;34mplot_figures\u001b[0m/\n",
            "' CEG omo on colab のコピー'                    \u001b[01;34mmodel\u001b[0m/            \u001b[01;34mresult_log\u001b[0m/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning\n",
        "%ls\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRrJ4uFkLMCl",
        "outputId": "eb55dcb3-4c33-4bea-ed94-0c8fd2298893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.1.4-py3-none-any.whl (778 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.1/778.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (2.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch-lightning) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.10.1 pytorch-lightning-2.1.4 torchmetrics-1.3.0.post0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fm0w0jc01Pj"
      },
      "source": [
        "# pathを辞書にまとめて管理する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrElytlN06OI"
      },
      "outputs": [],
      "source": [
        "path_dict = {\n",
        "    # 生データのパス\n",
        "    'raw_train_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/train2.tsv',\n",
        "    'raw_test_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/test2.tsv',\n",
        "    'raw_val_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/val2.tsv',\n",
        "\n",
        "    # 欠損値処理を行った後のデータのパス\n",
        "    'missing_value_processed_train_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_train2.tsv',\n",
        "    'missing_value_processed_test_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_test2.tsv',\n",
        "    'missing_value_processed_val_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_val2.tsv',\n",
        "\n",
        "    # 欠損値処理を行った後、さらなる整形を行ったデータ\n",
        "    'input_for_lightning_model_train_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_train2.tsv',\n",
        "    'input_for_lightning_model_test_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_test2.tsv',\n",
        "    'input_for_lightning_model_val_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_val2.tsv',\n",
        "\n",
        "    # 集合に分割した後のデータ (main)\n",
        "    'main_input_for_lightning_model_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/train/true_set2.tsv',\n",
        "    'main_input_for_lightning_model_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/train/false_set2.tsv',\n",
        "    'main_input_for_lightning_model_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/test/true_set2.tsv',\n",
        "    'main_input_for_lightning_model_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/test/false_set2.tsv',\n",
        "    'main_input_for_lightning_model_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/val/true_set2.tsv',\n",
        "    'main_input_for_lightning_model_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/val/false_set2.tsv',\n",
        "\n",
        "    # 集合に分割した後のデータ (toy)\n",
        "    'toy_input_for_lightning_model_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/train/true_set2.tsv',\n",
        "    'toy_input_for_lightning_model_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/train/false_set2.tsv',\n",
        "    'toy_input_for_lightning_model_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/test/true_set2.tsv',\n",
        "    'toy_input_for_lightning_model_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/test/false_set2.tsv',\n",
        "    'toy_input_for_lightning_model_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/val/true_set2.tsv',\n",
        "    'toy_input_for_lightning_model_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/val/false_set2.tsv',\n",
        "\n",
        "    # モデルを保存するPath\n",
        "    'saved_model_path' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/src/models',\n",
        "    'TensorBoardLogger': 'lightning_logs',\n",
        "\n",
        "    # 拡張したデータの保存場所(main)\n",
        "    'main_extended_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/train/true_set2.tsv',\n",
        "    'main_extended_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/train/false_set2.tsv',\n",
        "    'main_extended_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/test/true_set2.tsv',\n",
        "    'main_extended_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/test/false_set2.tsv',\n",
        "    'main_extended_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/val/true_set2.tsv',\n",
        "    'main_extended_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/val/false_set2.tsv',\n",
        "\n",
        "\n",
        "    # 拡張したデータの保存場所(main)\n",
        "    'toy_extended_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/train/true_set2.tsv',\n",
        "    'toy_extended_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/train/false_set2.tsv',\n",
        "    'toy_extended_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/test/true_set2.tsv',\n",
        "    'toy_extended_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/test/false_set2.tsv',\n",
        "    'toy_extended_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/val/true_set2.tsv',\n",
        "    'toy_extended_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/val/false_set2.tsv',\n",
        "\n",
        "  }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-H8IgiRLeom"
      },
      "source": [
        "# data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2W3EWwDNM-t"
      },
      "source": [
        "## data_reader.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDRiQFePNLVS"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"data_reader module is written for read files\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def read_csv(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "\n",
        "    :param path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "def read_tsv(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "\n",
        "    :param path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return pd.read_csv(path, sep=\"\\t\", header=None)\n",
        "\n",
        "\n",
        "def read_npy(path: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    load a list of numpy elements into memory\n",
        "    :param path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return np.load(path, allow_pickle=True)\n",
        "\n",
        "\n",
        "def read_excel(path:str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    :param path: where to load data from\n",
        "    :return: pd.DataFrame\n",
        "    \"\"\"\n",
        "    return pd.read_excel(path, engine=\"openpyxl\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sdEJztwNSgt"
      },
      "source": [
        "## data_writer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od80SyiCNVaE"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"data_writer module is written for write data in files\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def write_npy(path: str, data: list) -> None:\n",
        "    \"\"\"\n",
        "    save a list of numpy elements into disk\n",
        "    :param path:\n",
        "    :param data:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return np.save(path, data, allow_pickle=True)\n",
        "\n",
        "def write_dataframe_in_tsv(data: pd.DataFrame, path: str) -> None:\n",
        "\n",
        "  \"\"\"\n",
        "  save pd.Dataframe in tsv file\n",
        "  :param path:\n",
        "  :param data:\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  data.to_csv(path, sep='\\t', index=False, header=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lROVJGpnfD8U"
      },
      "source": [
        "## data_missing_value_processor\n",
        "- 欠損値の処理を行う\n",
        "- やること\n",
        "  1. 全ての値がNaNの列を削除する\n",
        "  2. NaNを0で補完する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFLX1BVOfi1v"
      },
      "outputs": [],
      "source": [
        "def data_missing_value_processor(data: pd.DataFrame, path: str) -> pd.DataFrame:\n",
        "\n",
        "  print('input data size : ' + str(len(data)))\n",
        "\n",
        "  data_without_invalid_row = data.dropna(how=\"all\")\n",
        "  print(str(len(data) - len(data_without_invalid_row)) + \" data was dropped because all of the values were NaN. \")\n",
        "\n",
        "  processed_data = data_without_invalid_row.fillna(0)\n",
        "  print(\"NaN in dataframe was filled with 0. \")\n",
        "  print(\"processed input data size : \" + str(len(processed_data)))\n",
        "  print(\"removed data size : \" + str(len(data) - len(processed_data)) )\n",
        "  print('\\n')\n",
        "\n",
        "  write_dataframe_in_tsv(processed_data, path)\n",
        "\n",
        "  return processed_data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phuOrNG7eh6i"
      },
      "source": [
        "##data_pre_processor_for_LIAR_PLUS_Dataset\n",
        "- LIAR_PLUS_Datasetクラスの入力に用いるdataframeを作成する\n",
        "- 作成したdataframeを保存する\n",
        "- statement, metadata, justification, credit_scoreの四つの列からなるdataframeを出力する\n",
        "- やること\n",
        "  1. 値として0(元々は欠損値)を持つ場合、\"None\"で補完する\n",
        "  2. 列を結合してstaetment, metadata, justification, credit_scoreを作成する\n",
        "  3. credit_scoreのnanを0.5で補完する\n",
        "  4. 作成したdataframeを保存する\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5D6XvWcefqX"
      },
      "outputs": [],
      "source": [
        "def data_pre_processor_for_LIAR_PLUS_Dataset(data: pd.DataFrame, path: str) -> pd.DataFrame:\n",
        "\n",
        "  # 1. 値として0(元々は欠損値)を持つ場合、\"None\"で補完する\n",
        "\n",
        "  data[4].replace(0, 'None', inplace=True)\n",
        "  data[5].replace(0, 'None', inplace=True)\n",
        "  data[6].replace(0, 'None', inplace=True)\n",
        "  data[7].replace(0, 'None', inplace=True)\n",
        "  data[8].replace(0, 'None', inplace=True)\n",
        "  data[14].replace(0, 'None', inplace=True)\n",
        "  data[15].replace(0, 'None', inplace=True)\n",
        "\n",
        "  # 2. 列を結合してstaetment, metadata, justificationを作成する\n",
        "\n",
        "  data_for_LIAR_PLUS_Dataset = pd.DataFrame(\n",
        "      data = {\n",
        "      'id' : data[0],\n",
        "      'label' : data[2],\n",
        "      'statement' : data[3],\n",
        "      'metadata' : data[4]+ ' ' + data[5] + ' ' + data[6] + ' ' + data[7] + ' ' + data[8] + ' ' + data[14],\n",
        "      'justification' : data[15],\n",
        "      'credit_score' : (data[12]*0.2 + data[11]*0.5 + data[9]*0.75 + data[10]*0.9 + data[13]*1)/(data[12] + data[11] + data[9] + data[10] + data[13]),\n",
        "      'unique_id' : data[1]\n",
        "      })\n",
        "\n",
        "  data_for_LIAR_PLUS_Dataset['credit_score'] = data_for_LIAR_PLUS_Dataset['credit_score'].fillna(0.5)\n",
        "\n",
        "  write_dataframe_in_tsv(data_for_LIAR_PLUS_Dataset, path)\n",
        "\n",
        "  print(\"dataframe was saved in\" + path + \"\\n\")\n",
        "\n",
        "  data_for_LIAR_PLUS_Dataset.info()\n",
        "\n",
        "\n",
        "  return data_for_LIAR_PLUS_Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeBeWogIddoI"
      },
      "source": [
        "# dataset_separator_on_labels\n",
        "- プロンプトを作成する前にラベルで条件付けられた集合を作成\n",
        "- 名前以外はgenerated_prompt_separatorと同じ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHRtX4hxdtCE"
      },
      "outputs": [],
      "source": [
        "def dataset_separator_on_labels(path: dict, data: pd.DataFrame) -> dict:\n",
        "\n",
        "  true_set = pd.DataFrame()\n",
        "  false_set = pd.DataFrame()\n",
        "  total_len = len(data)\n",
        "\n",
        "  for i in range(len(data)):\n",
        "\n",
        "    data_row = data.iloc[i]\n",
        "   # print(data_row)\n",
        "\n",
        "    if data_row[1] == \"true\":\n",
        "        true_set = true_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"half-true\":\n",
        "        true_set = true_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"mostly-true\":\n",
        "        true_set = true_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"false\":\n",
        "        false_set = false_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"barely-true\":\n",
        "        false_set = false_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"pants-fire\":\n",
        "        false_set = false_set.append(data_row, ignore_index=True)\n",
        "    else:\n",
        "        print(f\"Invalid label was found! {data_row[1]}\")\n",
        "\n",
        "  print(\"===================================================\\n\")\n",
        "  print(f\"total len : {total_len}\")\n",
        "  print(f\"true_set : {len(true_set)}\")\n",
        "  print(f\"false_set : {len(false_set)}\")\n",
        "  print(\"===================================================\\n\")\n",
        "\n",
        "  write_dataframe_in_tsv(true_set, path[\"true_set\"])\n",
        "  write_dataframe_in_tsv(false_set, path[\"false_set\"])\n",
        "\n",
        "\n",
        "  return dict(\n",
        "      true_set=true_set,\n",
        "      false_set=true_set\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbE1iUnbvV1w"
      },
      "source": [
        "# prompt_generator\n",
        "・プロンプトの作成を一任\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yghLEX3Uvc53"
      },
      "outputs": [],
      "source": [
        "# def prompt_generator(data: pd.DataFrame, path :str) -> pd.DataFrame :\n",
        "#   \"\"\"\n",
        "#   this function is for generating and saving prompts\n",
        "\n",
        "#   param  : stage(\"fine-tuning\" of \"completion\")\n",
        "#   param  : data(row data for prompt generation)\n",
        "#   param  : path(path to save generated promps dataframe)\n",
        "#   return : prompts(generated prompts)\n",
        "#   \"\"\"\n",
        "#   generated_prompts = \"Statement: \" + data[2] + \" Metadata: \" + data[3]\n",
        "\n",
        "#   prompts_df = pd.DataFrame(\n",
        "#       data = {\n",
        "#       'id' : data[0],\n",
        "#       'label' : data[1],\n",
        "#       'concat' : generated_prompts,\n",
        "#       'justification': data[4],\n",
        "#       'credit_score' : data[5],\n",
        "#       'unique_id' : data[6]\n",
        "#       })\n",
        "\n",
        "#   write_dataframe_in_tsv(prompts_df, path)\n",
        "\n",
        "#   return prompts_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMKn8TNcYi1g"
      },
      "outputs": [],
      "source": [
        "# def prompt_generator(data: pd.DataFrame, path:str, config_dict:dict) -> pd.DataFrame :\n",
        "#   \"\"\"\n",
        "#   this function is for generating and saving prompts\n",
        "\n",
        "#   param  : stage(\"fine-tuning\" of \"completion\")\n",
        "#   param  : data(row data for prompt generation)\n",
        "#   param  : path(path to save generated promps dataframe)\n",
        "#   return : prompts(generated prompts)\n",
        "#   \"\"\"\n",
        "\n",
        "\n",
        "#   generated_prompts = ''\n",
        "\n",
        "#   if 'statement' in config_dict['PROMPT_COMPONENTS']:\n",
        "#     generated_prompts = 'Statement: ' + data[2]\n",
        "\n",
        "#   if 'metadata' in config_dict['PROMPT_COMPONENTS']:\n",
        "#     generated_prompts += \" Metadata: \" + data[3]\n",
        "\n",
        "#   prompts_df = pd.DataFrame(\n",
        "#       data = {\n",
        "#       'id' : data[0],\n",
        "#       'label' : data[1],\n",
        "#       'concat' : generated_prompts,\n",
        "#       'justification': data[4],\n",
        "#       'credit_score' : data[5],\n",
        "#       'unique_id' : data[6]\n",
        "#       })\n",
        "\n",
        "#   return prompts_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UubLg77lkNfo"
      },
      "source": [
        "# concatinated_inputs_generator\n",
        "- concatとfull-promptを作成し諸々の情報を返す\n",
        "- 例外処理のコードはまだ書いていない"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVwpH7jzkvku"
      },
      "outputs": [],
      "source": [
        "def concatinated_inputs_generator(data: pd.Series, config_dict:dict, tokenizer, prepare_tokenizer, input_flip) -> dict :\n",
        "\n",
        "  concatinated_input = ''\n",
        "  full_input = ''\n",
        "\n",
        "  if input_flip == 0:\n",
        "\n",
        "    if 'statement' in config_dict['PROMPT_COMPONENTS']:\n",
        "        concatinated_input = 'Statement: ' + data[2]\n",
        "    if 'metadata' in config_dict['PROMPT_COMPONENTS']:\n",
        "        concatinated_input += ' Metadata: ' + data[3]\n",
        "    concatinated_input += prepare_tokenizer.exp_token\n",
        "    concatinated_input_length = len(tokenizer.tokenize(concatinated_input))\n",
        "    if 'justification' in config_dict['PROMPT_COMPONENTS']:\n",
        "        full_input = concatinated_input + data[4] + prepare_tokenizer.eos_token\n",
        "\n",
        "  elif input_flip == 1:\n",
        "\n",
        "    if 'metadata' in config_dict['PROMPT_COMPONENTS']:\n",
        "        concatinated_input += 'Metadata: ' + data[3]\n",
        "    if 'statement' in config_dict['PROMPT_COMPONENTS']:\n",
        "        concatinated_input += ' Statement: ' + data[2]\n",
        "    concatinated_input += prepare_tokenizer.exp_token\n",
        "    concatinated_input_length = len(tokenizer.tokenize(concatinated_input))\n",
        "    if 'justification' in config_dict['PROMPT_COMPONENTS']:\n",
        "        full_input = concatinated_input + data[4] + prepare_tokenizer.eos_token\n",
        "\n",
        "  else:\n",
        "    ValueError('wrong input_flip value')\n",
        "\n",
        "\n",
        "  full_input_length = len(tokenizer.tokenize(full_input))\n",
        "\n",
        "  # bartではspecial_token<s>が文頭に付くため、その分だけ入力長が1大きくなる\n",
        "  if config_dict[\"MODEL_NAME\"] == 'facebook/bart-base':\n",
        "      concatinated_input_length = concatinated_input_length + 1\n",
        "\n",
        "  if concatinated_input_length > config_dict['MAX_LENGTH']:\n",
        "      concatinated_input_length = config_dict['MAX_LENGTH']\n",
        "\n",
        "  if full_input_length > config_dict['MAX_LENGTH']:\n",
        "      full_input_length = config_dict['MAX_LENGTH']\n",
        "\n",
        "#   print(f'concatinated_input : {concatinated_input}')\n",
        "#   print(f'full_input_length : {full_input_length}')\n",
        "\n",
        "\n",
        "  return concatinated_input, full_input, concatinated_input_length, full_input_length\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlnS8xxD3yZv"
      },
      "outputs": [],
      "source": [
        "# train_prompt_path = \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts/main/train_prompts.tsv\"\n",
        "# test_prompt_path = \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts/main/test_prompts.tsv\"\n",
        "# val_prompt_path = \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts/main/val_prompts.tsv\"\n",
        "\n",
        "# train_prompts = read_tsv(\"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/main/input_train2.tsv\")\n",
        "# test_prompts = read_tsv(\"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/main/input_test2.tsv\")\n",
        "# val_prompts = read_tsv(\"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/main/input_val2.tsv\")\n",
        "\n",
        "# train_prompts = prompt_generator(train_prompts, train_prompt_path)\n",
        "# test_prompts = prompt_generator(test_prompts, test_prompt_path)\n",
        "# val_prompts = prompt_generator(val_prompts, val_prompt_path)\n",
        "\n",
        "# print(len(train_prompts))\n",
        "# train_prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQTF6thEEeYI"
      },
      "outputs": [],
      "source": [
        "# x = read_tsv(train_prompt_path)\n",
        "# x.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMUYg-qvy_Yw"
      },
      "source": [
        "# generated_prompts_separator\n",
        "- ラベルごとに分類する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rz-tzHX9B1yv"
      },
      "outputs": [],
      "source": [
        "# def generated_prompts_separator(path: dict, data: pd.DataFrame) -> dict:\n",
        "\n",
        "#   true_set = pd.DataFrame()\n",
        "#   false_set = pd.DataFrame()\n",
        "#   total_len = len(data)\n",
        "\n",
        "#   for i in range(len(data)):\n",
        "\n",
        "#     data_row = data.iloc[i]\n",
        "#    # print(data_row)\n",
        "\n",
        "#     if data_row[1] == \"true\":\n",
        "#         true_set = true_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"half-true\":\n",
        "#         true_set = true_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"mostly-true\":\n",
        "#         true_set = true_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"false\":\n",
        "#         false_set = false_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"barely-true\":\n",
        "#         false_set = false_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"pants-fire\":\n",
        "#         false_set = false_set.append(data_row, ignore_index=True)\n",
        "#     else:\n",
        "#         print(f\"Invalid label was found! {data_row[1]}\")\n",
        "\n",
        "#   print(\"===================================================\\n\")\n",
        "#   print(f\"total len : {total_len}\")\n",
        "#   print(f\"true_set : {len(true_set)}\")\n",
        "#   print(f\"false_set : {len(false_set)}\")\n",
        "#   print(\"===================================================\\n\")\n",
        "\n",
        "#   write_dataframe_in_tsv(true_set, path[\"true_set\"])\n",
        "#   write_dataframe_in_tsv(false_set, path[\"false_set\"])\n",
        "\n",
        "\n",
        "#   return dict(\n",
        "#       true_set=true_set,\n",
        "#       false_set=true_set\n",
        "#   )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGPWmYuKW7AT"
      },
      "outputs": [],
      "source": [
        "# fine_main_train_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/train/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/train/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_main_test_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/test/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/test/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_main_val_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/val/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/val/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_toy_train_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/train/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/train/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_toy_test_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/test/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/test/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_toy_val_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/val/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/val/false_sets.tsv\"\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGnq4oOKMIN-"
      },
      "outputs": [],
      "source": [
        "# main_train_df = read_tsv(train_prompt_path)\n",
        "# main_test_df = read_tsv(test_prompt_path)\n",
        "# main_val_df = read_tsv(val_prompt_path)\n",
        "\n",
        "# toy_train_df = main_train_df[:600]\n",
        "# toy_test_df = main_test_df[:200]\n",
        "# toy_val_df = main_val_df[:200]\n",
        "\n",
        "\n",
        "# main_train_set = generated_prompts_separator(fine_main_train_path_dict, main_train_df)\n",
        "# main_test_set = generated_prompts_separator(fine_main_test_path_dict, main_test_df)\n",
        "# main_val_set = generated_prompts_separator(fine_main_val_path_dict, main_val_df)\n",
        "\n",
        "# toy_train_set = generated_prompts_separator(fine_toy_train_path_dict, toy_train_df)\n",
        "# toy_test_set = generated_prompts_separator(fine_toy_test_path_dict, toy_test_df)\n",
        "# toy_val_set = generated_prompts_separator(fine_toy_val_path_dict, toy_val_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pyPfFDRkUFP"
      },
      "outputs": [],
      "source": [
        "# y = read_tsv(\"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/train/false_sets.tsv\")\n",
        "# y.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxZNpMBC4_xf"
      },
      "source": [
        "# データ前処理の全行程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xrPuNRyL4Sv"
      },
      "outputs": [],
      "source": [
        "# path_dict = {\n",
        "#     # 生データのパス\n",
        "#     'raw_train_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/train2.tsv',\n",
        "#     'raw_test_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/test2.tsv',\n",
        "#     'raw_val_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/val2.tsv',\n",
        "\n",
        "#     # 欠損値処理を行った後のデータのパス\n",
        "#     'missing_value_processed_train_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_train2.tsv',\n",
        "#     'missing_value_processed_test_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_test2.tsv',\n",
        "#     'missing_value_processed_val_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_val2.tsv',\n",
        "\n",
        "#     # 欠損値処理を行った後、さらなる整形を行ったデータ\n",
        "#     'input_for_lightning_model_train_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_train2.tsv',\n",
        "#     'input_for_lightning_model_test_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_test2.tsv',\n",
        "#     'input_for_lightning_model_val_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_val2.tsv',\n",
        "\n",
        "#     # 集合に分割した後のデータ (main)\n",
        "#     'main_input_for_lightning_model_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/train/true_set2.tsv',\n",
        "#     'main_input_for_lightning_model_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/train/false_set2.tsv',\n",
        "#     'main_input_for_lightning_model_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/test/true_set2.tsv',\n",
        "#     'main_input_for_lightning_model_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/test/false_set2.tsv',\n",
        "#     'main_input_for_lightning_model_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/val/true_set2.tsv',\n",
        "#     'main_input_for_lightning_model_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/val/false_set2.tsv',\n",
        "\n",
        "#     # 集合に分割した後のデータ (toy)\n",
        "#     'toy_input_for_lightning_model_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/train/true_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/train/false_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/test/true_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/test/false_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/val/true_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/val/false_set2.tsv',\n",
        "\n",
        "#   }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBQLyeNu4_Lw"
      },
      "outputs": [],
      "source": [
        "# # 生データの読み込み\n",
        "# raw_train_data = read_tsv(path_dict['raw_train_data'])\n",
        "# raw_test_data = read_tsv(path_dict['raw_test_data'])\n",
        "# raw_val_data = read_tsv(path_dict['raw_val_data'])\n",
        "\n",
        "# # 基本的な前処理\n",
        "# processed_train_data = data_missing_value_processor(\n",
        "#     raw_train_data,\n",
        "#     path_dict['missing_value_processed_train_data']\n",
        "# )\n",
        "# processed_test_data = data_missing_value_processor(\n",
        "#     raw_test_data,\n",
        "#     path_dict['missing_value_processed_test_data']\n",
        "# )\n",
        "# processed_val_data = data_missing_value_processor(\n",
        "#     raw_val_data,\n",
        "#     path_dict[\"missing_value_processed_val_data\"]\n",
        "# )\n",
        "\n",
        "# # タスクに向けた前処理と保存\n",
        "# train_data_for_LIAR_PLUS_Dataset = data_pre_processor_for_LIAR_PLUS_Dataset(\n",
        "#     processed_train_data,\n",
        "#     path_dict['input_for_lightning_model_train_base']\n",
        "#     )\n",
        "# test_data_for_LIAR_PLUS_Dataset = data_pre_processor_for_LIAR_PLUS_Dataset(\n",
        "#     processed_test_data,\n",
        "#     path_dict['input_for_lightning_model_test_base']\n",
        "#     )\n",
        "# val_data_for_LIAR_PLUS_Dataset = data_pre_processor_for_LIAR_PLUS_Dataset(\n",
        "#     processed_val_data,\n",
        "#     path_dict['input_for_lightning_model_val_base']\n",
        "#     )\n",
        "\n",
        "# # 集合に分割し保存 (main)\n",
        "# main_train_true_set, main_train_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['main_input_for_lightning_model_train_data_true'],\n",
        "#      'false_set': path_dict['main_input_for_lightning_model_train_data_false']},\n",
        "#     train_data_for_LIAR_PLUS_Dataset\n",
        "# )\n",
        "\n",
        "# main_test_true_set, main_test_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['main_input_for_lightning_model_test_data_true'],\n",
        "#      'false_set': path_dict['main_input_for_lightning_model_test_data_false']},\n",
        "#     test_data_for_LIAR_PLUS_Dataset\n",
        "# )\n",
        "\n",
        "# main_val_true_set, main_val_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['main_input_for_lightning_model_val_data_true'],\n",
        "#      'false_set': path_dict['main_input_for_lightning_model_val_data_false']},\n",
        "#     val_data_for_LIAR_PLUS_Dataset\n",
        "# )\n",
        "\n",
        "\n",
        "# # 集合に分割し保存 (toy)\n",
        "# toy_train_true_set, toy_train_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "#      'false_set': path_dict['toy_input_for_lightning_model_train_data_false']},\n",
        "#     train_data_for_LIAR_PLUS_Dataset[:120]\n",
        "# )\n",
        "\n",
        "# toy_test_true_set, toy_test_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "#      'false_set': path_dict['toy_input_for_lightning_model_test_data_false']},\n",
        "#     test_data_for_LIAR_PLUS_Dataset[:40]\n",
        "# )\n",
        "\n",
        "# toy_val_true_set, toy_val_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "#      'false_set': path_dict['toy_input_for_lightning_model_val_data_false']},\n",
        "#     val_data_for_LIAR_PLUS_Dataset[:40]\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ_ZwRAby_hP"
      },
      "source": [
        "# Google Driveへ接続しディレクトリを移動"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L31wefW7zMo9"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpFyWy0BzU0v"
      },
      "outputs": [],
      "source": [
        "# %cd /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning\n",
        "# %ls\n",
        "# %pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB5rz2m-y_uq"
      },
      "source": [
        "# TransformersとPytorch Lightningをインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izeLGyCTznqQ"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X4Le2kXY8K3",
        "outputId": "ceb2649d-99a8-4ee4-d18d-275ad96d7e88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.35.2\n",
            "2.1.4\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "import pytorch_lightning as pl\n",
        "import pandas as pd\n",
        "print(transformers.__version__)\n",
        "print(pl.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMvtLR2fs6j7"
      },
      "source": [
        "# BERT-scoreに関わるコード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz5Zrc0as-Eh",
        "outputId": "35290f1d-62ef-4fc1-c326-117ede636a22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: dill, responses, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-2.16.1 dill-0.3.7 evaluate-0.4.1 multiprocess-0.70.15 responses-0.18.0\n",
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.1.0+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.5.3)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.35.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.66.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2023.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Installing collected packages: bert_score\n",
            "Successfully installed bert_score-0.3.13\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate\n",
        "!pip install bert_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCTket3OpAsD"
      },
      "source": [
        "## BERTScore_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4YM0O1KpDnS"
      },
      "outputs": [],
      "source": [
        "BERTScore_config = {\n",
        "    'MODEL_TYPE' : 'distilbert-base-uncased',\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8176C4WpESf"
      },
      "source": [
        "## class BERTScore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qRpXSDwpHbh"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "class BERTScore():\n",
        "\n",
        "    def __init__(self, bertscore_config, data:pd.DataFrame):\n",
        "        self.data = data\n",
        "        self.data_size = len(self.data)\n",
        "        self.bertscore_config = bertscore_config\n",
        "        self.bertscore = evaluate.load(\"bertscore\")\n",
        "        self.extended_data = pd.DataFrame()\n",
        "        self.precision_mean = None\n",
        "        self.recall_mean = None\n",
        "        self.f1_mean = None\n",
        "\n",
        "    def compute(self):\n",
        "        num_data_size = 0\n",
        "        for idx in tqdm(range(self.data_size)):\n",
        "            data_row = self.data.iloc[idx]\n",
        "            # print(data_row)\n",
        "            # gold_exp = data_row[4]\n",
        "            # gen_exp  = data_row[7]\n",
        "            scores = self.bertscore.compute(predictions=[data_row[4]], references=[data_row[7]], model_type=self.bertscore_config['MODEL_TYPE'])\n",
        "            # print(f'scores : {scores}')\n",
        "            precision = pd.DataFrame(scores['precision'])\n",
        "            recall = pd.DataFrame(scores['recall'])\n",
        "            f1  = pd.DataFrame(scores['f1'])\n",
        "\n",
        "            concatinated_data_row = pd.concat([data_row, precision, recall, f1], ignore_index=True, axis=0).T\n",
        "            self.extended_data = pd.concat([self.extended_data, concatinated_data_row], axis=0).reset_index(drop=True)\n",
        "\n",
        "        return self.extended_data\n",
        "\n",
        "    def get_data_info(self):\n",
        "        print(f'data size : {self.data_size}')\n",
        "        self.data.head()\n",
        "\n",
        "    def get_means(self):\n",
        "        self.precision_mean = self.extended_data[8].mean()\n",
        "        self.recall_mean = self.extended_data[9].mean()\n",
        "        self.f1_mean = self.extended_data[10].mean()\n",
        "\n",
        "        return self.precision_mean, self.recall_mean, self.f1_mean\n",
        "\n",
        "    def get_hist_img(self, path=None):\n",
        "\n",
        "        plt.figure()\n",
        "        self.extended_data[8].plot.hist(title='Precision')\n",
        "        plt.show()\n",
        "        if path is not None:\n",
        "            plt.savefig(path)\n",
        "\n",
        "        plt.figure()\n",
        "        self.extended_data[9].plot.hist(title='Recall')\n",
        "        plt.show()\n",
        "        if path is not None:\n",
        "            plt.savefig(path)\n",
        "\n",
        "        plt.figure()\n",
        "        self.extended_data[10].plot.hist(title='F1')\n",
        "        plt.show()\n",
        "        if path is not None:\n",
        "            plt.savefig(path)\n",
        "\n",
        "\n",
        "    def save_extended_data(self, path):\n",
        "        write_dataframe_in_tsv(self.extended_data, path)\n",
        "        print(f'the extended data was save in {path}.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QC4IhmqMstUD"
      },
      "outputs": [],
      "source": [
        "# path1 = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 s only/gpt2-medium_2024-01-24 13:17 s only_true_batch_size=2_true.tsv'\n",
        "# path2 = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 s only/gpt2-medium_2024-01-24 13:17 s only_false_batch_size=2_false.tsv'\n",
        "\n",
        "# sample_data  = read_tsv(path1)\n",
        "# sample_data2 = read_tsv(path2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7AD4AtoMdTo"
      },
      "outputs": [],
      "source": [
        "# ex_path1 = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/sample.tsv'\n",
        "# ex_path2 = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/sample2.tsv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyloaipQziyB"
      },
      "outputs": [],
      "source": [
        "# predictions = [\"I have an apple\"]\n",
        "# references = [\"I have a pen\"]\n",
        "\n",
        "# results = bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\")\n",
        "\n",
        "# print(results)\n",
        "# # {'precision': [0.8849452137947083], 'recall': [0.8849452137947083],\n",
        "# # 'f1': [0.8849452137947083], 'hashcode': 'distilbert-base-uncased_L5_no-idf_version=0.3.11(hug_trans=4.11.0.dev0)'}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXUVsjmvlN2H"
      },
      "source": [
        "## def get_bertscore_rank()\n",
        "- ~/data/extended/~にあるgenerated_justificationを含むデータを受け取る\n",
        "- bertscoreの算出・データ拡張・ランクの算出を行う"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4C0MyVhoydU"
      },
      "outputs": [],
      "source": [
        "# ex_data1 = read_tsv(ex_path1)\n",
        "# ex_data2 = read_tsv(ex_path2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt0IoKnJouLj"
      },
      "outputs": [],
      "source": [
        "# data_dict = {\n",
        "#     'gpt2-medium_true_bs2' : sample_data,\n",
        "#     'gpt2-medium_false_bs2': sample_data2\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itGuO6LalOOt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def get_bertscore_rank(time, bertscore_config:dict, data_dict:dict):\n",
        "\n",
        "    p_rank = {}\n",
        "    r_rank = {}\n",
        "    f_rank = {}\n",
        "\n",
        "    base_path = f'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/BERTScore/{time}'\n",
        "\n",
        "    if not os.path.exists(base_path):\n",
        "        os.mkdir(base_path)\n",
        "\n",
        "    for model_name, data in zip(data_dict.keys(), data_dict.values()):\n",
        "\n",
        "        bertscore = BERTScore(bertscore_config, data)\n",
        "        print(f\"model_name : {model_name}\")\n",
        "        bertscore.compute()\n",
        "        # bertscore.get_data_info()\n",
        "        # bertscore.save_extended_data(f'{saved_data_path}/{model_name}.tsv')\n",
        "        p, r, f = bertscore.get_means()\n",
        "        p_rank[model_name] = p\n",
        "        r_rank[model_name] = r\n",
        "        f_rank[model_name] = f\n",
        "        # bertscore1.get_hist_img()\n",
        "\n",
        "    precision = pd.DataFrame.from_dict(p_rank, orient='index').sort_values(by=0, ascending=False)\n",
        "    recall = pd.DataFrame.from_dict(r_rank, orient='index').sort_values(by=0, ascending=False)\n",
        "    f1 = pd.DataFrame.from_dict(f_rank, orient='index').sort_values(by=0, ascending=False)\n",
        "\n",
        "    # print(precision)\n",
        "    # print(recall)\n",
        "    # print(f1)\n",
        "\n",
        "    precision.to_csv(f'{base_path}/precision_rank.tsv', sep='\\t', index=True, header=False)\n",
        "    recall.to_csv(f'{base_path}/recall_rank.tsv', sep='\\t', index=True, header=False)\n",
        "    f1.to_csv(f'{base_path}/f1_rank.tsv', sep='\\t', index=True, header=False)\n",
        "\n",
        "    return precision, recall, f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpLJEBIEmeWq"
      },
      "outputs": [],
      "source": [
        "# p, r, f = get_bertscore_rank('delete this', BERTScore_config, data_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciFDDUpS572n"
      },
      "source": [
        "# class LIAR_PLUS_Dataset_For_CEG(Dataset):を定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnU6wf8TntZG"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# pylint: disable-msg=import-error\n",
        "# pylint: disable-msg=no-member\n",
        "# ========================================================\n",
        "\"\"\"dataset module is written for create data module\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import time\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "class LIAR_PLUS_Dataset_For_CEG(Dataset):\n",
        "    \"\"\"\n",
        "    this class is for encoding input dataframe for GPT2\n",
        "\n",
        "    Input dataframe needs to be consist of 5 columns, \"id\", \"label\", \"prompt\", \"justification\" and \"credit_score\"\n",
        "\n",
        "    later we wrap a lightning data module around it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data: pd.DataFrame, config_dict: dict):\n",
        "      # max_token_length should be flexible depending on input sequence so it needs to be changed later.\n",
        "        self.data = data\n",
        "        self.config_dict = config_dict\n",
        "        self.prepare_tokenizer = Prepare_Tokenizer(self.config_dict)\n",
        "        self.tokenizer = self.prepare_tokenizer.get_tokenizer()\n",
        "        self.prepare_tokenizer.get_tokenizer_info()\n",
        "        self.max_token_len = config_dict['MAX_LENGTH']\n",
        "        # print(\"Before\")\n",
        "        # print(self.tokenizer.all_special_tokens)\n",
        "        # print(self.tokenizer.all_special_ids)\n",
        "        # special_tokens_dict = {\n",
        "        # 'additional_special_tokens': ['[EXP]'],\n",
        "        # \"eos_token\" : \"<|endoftext|>\",\n",
        "        # \"pad_token\" : \"<|endoftext|>\"\n",
        "        # }\n",
        "        # self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "        # self.exp_id = tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "        # self.eos_id = tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        # self.pad_id = tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        # print(\"After\")\n",
        "        # print(tokenizer.all_special_tokens)\n",
        "        # print(tokenizer.all_special_ids)\n",
        "        # print(f\"exp_id : {self.exp_id}\")\n",
        "        # print(f\"eos_id : {self.eos_id}\")\n",
        "        # print(f\"pad_id : {self.pad_id}\\n\")\n",
        "        # print(f\"len(tokenizer) : {len(self.tokenizer)}\")\n",
        "\n",
        "         # it could be 512\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "\n",
        "        data_row = self.data.iloc[index]\n",
        "        # print('------------------------------------------------------------------\\n')\n",
        "        # print(f'data_row : {data_row}\\n')\n",
        "        # print('------------------------------------------------------------------\\n')\n",
        "        concat_text, full_prompt_text, concat_text_length, full_prompt_text_length = concatinated_inputs_generator(data_row, self.config_dict, self.tokenizer, self.prepare_tokenizer, self.config_dict['INPUT_FLIP'])\n",
        "\n",
        "        # print('------------------------------------------------------------------\\n')\n",
        "        # print(f'concat_text : {concat_text}\\n')\n",
        "        # print(f'full_prompt_text : {full_prompt_text}\\n')\n",
        "        # print(f'concat_text_length : {concat_text_length}\\n')\n",
        "        # print(f'full_prompt_text_length : {full_prompt_text_length}\\n')\n",
        "        # print('------------------------------------------------------------------\\n')\n",
        "        concat_encoding = self.tokenizer.encode_plus(\n",
        "            concat_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length = self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        full_prompt_encoding = self.tokenizer.encode_plus(\n",
        "            full_prompt_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length = self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        # labels = full_prompt_encoding[\"input_ids\"].flatten()\n",
        "        # labels[:concat_text_length] = -100 # cross_entropy_ignore_index\n",
        "\n",
        "        # print(\"=================================================================\\n\")\n",
        "        # print(f\"concat_text : {concat_text}\\n\")\n",
        "        # print(f\"concat_text_length : {concat_text_length}\\n\")\n",
        "        # print(f\"justification_text : {justification_text}\\n\")\n",
        "        # print(f\"justification_text_length : {justification_text_length}\\n\")\n",
        "        # print(f\"full_prompt_text : {full_prompt_text}\\n\")\n",
        "        # print(f\"full_prompt_text_length : {full_prompt_text_length}\\n\")\n",
        "        # print(f\"labels before padding : {labels}\\n\")\n",
        "        # print(f\"labels after padding: {labels}\\n\")\n",
        "        # print(\"=================================================================\\n\")\n",
        "\n",
        "        return dict(\n",
        "            input_ids1=concat_encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask1=concat_encoding[\"attention_mask\"].flatten(),\n",
        "            input_ids2=full_prompt_encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask2=full_prompt_encoding[\"attention_mask\"].flatten(),\n",
        "            concat_text_length=concat_text_length,\n",
        "            full_prompt_text_length=full_prompt_text_length,\n",
        "            # labels = labels\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf6XZBBC58F9"
      },
      "source": [
        "# class LIAR_PLUS_DataModule_For_CEG(pl.LightningDataModule)を定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFsOZ6LW6zLM"
      },
      "outputs": [],
      "source": [
        "class LIAR_PLUS_DataModule_For_CEG(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, config_dict, train_df: pd.DataFrame, test_df: pd.DataFrame, val_df: pd.DataFrame):\n",
        "        super().__init__()\n",
        "        self.config_dict = config_dict\n",
        "        self.train_df = train_df\n",
        "        self.test_df = test_df\n",
        "        self.val_df = val_df\n",
        "        self.training_batch_size = config_dict[\"TRAINING_BATCH_SIZE\"]\n",
        "        self.validation_batch_size = config_dict[\"VALIDATION_BATCH_SIZE\"]\n",
        "        self.test_batch_size = config_dict[\"TEST_BATCH_SIZE\"]\n",
        "        self.max_token_len = self.config_dict[\"MAX_LENGTH\"]\n",
        "        self.train_dataset, self.test_dataset, self.val_dataset = None, None, None\n",
        "\n",
        "        # 以下の処理はDataset内でやってくれるのかも\n",
        "        # print(\"Before\")\n",
        "        # print(self.tokenizer.all_special_tokens)\n",
        "        # print(self.tokenizer.all_special_ids)\n",
        "        # special_tokens_dict = {'additional_special_tokens': ['[EXP]'], \"eos_token\" : \"[EOS]\"}\n",
        "        # self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "        # self.exp_id = tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "        # self.eos_id = tokenizer.convert_tokens_to_ids('[EOS]')\n",
        "        # print(\"After\")\n",
        "        # print(tokenizer.all_special_tokens)\n",
        "        # print(tokenizer.all_special_ids)\n",
        "        # print(f\"exp_id : {self.exp_id}\")\n",
        "        # print(f\"eos_id : {self.eos_id}\\n\")\n",
        "\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None) -> None:\n",
        "        self.train_dataset = LIAR_PLUS_Dataset_For_CEG(self.train_df, self.config_dict)\n",
        "        print(f\"train_dataset size : {len(self.train_dataset)}\\n\")\n",
        "        self.test_dataset = LIAR_PLUS_Dataset_For_CEG(self.test_df, self.config_dict)\n",
        "        print(f\"test_dataset size : {len(self.test_dataset)}\\n\")\n",
        "        self.val_dataset = LIAR_PLUS_Dataset_For_CEG(self.val_df, self.config_dict)\n",
        "        print(f\"val_dataset size : {len(self.val_dataset)}\\n\")\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.training_batch_size,\n",
        "            shuffle=True,\n",
        "            drop_last=True,\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.validation_batch_size,\n",
        "            drop_last=True,\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            drop_last=True,\n",
        "            num_workers=2\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FKESVDD58Nr"
      },
      "source": [
        "# build_checkpoint_callbackを定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVqrSIBr7hYX"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"helper module is written for write useful function in indexer package\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "def build_checkpoint_callback(config_dict, base_time, batch_size, set_type, filename=\"QTag-{epoch:02d}-{val_loss:.2f}\",\n",
        "                              monitor=\"val_loss\"):\n",
        "    \"\"\"\n",
        "\n",
        "    :param save_top_k:\n",
        "    :param filename:\n",
        "    :param monitor:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # dirpath = config_dict['SAVED_MODEL_PATH'] + f'/{base_time}/' + config_dict['TensorBoardLogger_NAME'] + f'/batch_size={batch_size}'\n",
        "    dirpath = config_dict['SAVED_MODEL_PATH'] + '/' + config_dict['TensorBoardLogger_NAME'] + f'/{base_time}/{set_type}_batch_size={batch_size}'\n",
        "    print(f'dirpath : {dirpath}')\n",
        "    checkpoint_callback = ModelCheckpoint(monitor=monitor,  # monitored quantity\n",
        "                                          filename=filename,\n",
        "                                          save_top_k=config_dict['SAVE_TOP_K'],  # save the top k models\n",
        "                                          dirpath=dirpath,\n",
        "                                          mode=\"min\",  # mode of the monitored quantity for optimization\n",
        "                                          )\n",
        "    print(f\"checkpoint_callback : {checkpoint_callback}\")\n",
        "    return checkpoint_callback, dirpath"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mqQ2UHy58SM"
      },
      "source": [
        "# class CEG(pl.LightningModule)を定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9axfgge7tV3"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# pylint: disable=too-many-arguments\n",
        "# pylint: disable=import-error\n",
        "# ========================================================\n",
        "\"\"\"This module is written for write BERT classifier.\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "from typing import List\n",
        "import pytorch_lightning as pl\n",
        "from torch import nn\n",
        "import torch\n",
        "import torchmetrics\n",
        "from transformers import GPT2Tokenizer, AutoModel, AutoConfig, AdamW, get_linear_schedule_with_warmup, GPT2LMHeadModel, T5ForConditionalGeneration, BartForConditionalGeneration\n",
        "\n",
        "class CEG(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    creates a pytorch lightning model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config_dict,\n",
        "                 n_warmup_steps: int = None,\n",
        "                 n_training_steps: int = None,\n",
        "                 n_classes: int = None,\n",
        "                 result_manager = None):\n",
        "        super().__init__()\n",
        "        self.config_dict = config_dict\n",
        "        self.result_manager = result_manager\n",
        "        self.prepare_tokenizer = Prepare_Tokenizer(self.config_dict)\n",
        "        self.tokenizer = self.prepare_tokenizer.get_tokenizer()\n",
        "        self.prepare_tokenizer.get_tokenizer_info()\n",
        "        self.prepare_model = Prepare_Model(config=self.config_dict, prepare_tokenizer=self.prepare_tokenizer)\n",
        "        self.model = self.prepare_model.get_model()\n",
        "        self.prepare_model.get_model_info()\n",
        "        # self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "        # print(\"Before\")\n",
        "        # print(self.tokenizer.all_special_tokens)\n",
        "        # print(self.tokenizer.all_special_ids)\n",
        "        # special_tokens_dict = {\n",
        "        #     'additional_special_tokens': ['[EXP]'],\n",
        "        #     \"eos_token\" : \"<|endoftext|>\",\n",
        "        #     \"pad_token\" : \"<|endoftext|>\"\n",
        "        #     }\n",
        "        # self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "        # exp_id = self.tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "        # eos_id = self.tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        # pad_id = self.tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        # print(\"After\")\n",
        "        # print(self.tokenizer.all_special_tokens)\n",
        "        # print(self.tokenizer.all_special_ids)\n",
        "        # print(f\"exp_id : {exp_id}\")\n",
        "        # print(f\"eos_id : {eos_id}\")\n",
        "        # print(f\"pad_id : {pad_id}\\n\")\n",
        "        # print(f\"len(self.tokenizer) : {len(self.tokenizer)}\")\n",
        "\n",
        "        # gpt2config = AutoConfig.from_pretrained('gpt2', bos_token_id=self.tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id, additional_special_tokens_id = exp_id, output_hidden_states=False)\n",
        "\n",
        "        # self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=gpt2config)\n",
        "        # self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "        # self.lm_head = self.model.lm_head\n",
        "        # new_weights = torch.cat([self.lm_head.weight[:-1, :], torch.zeros(1, self.lm_head.weight.shape[1]) -10000])\n",
        "        # self.model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.batch_size = config_dict['TRAINING_BATCH_SIZE']\n",
        "        self.n_training_steps = n_training_steps\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.config_dict['CROSS_ENTROPY_IGNORE_INDEX'])\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, concat_text_length, full_prompt_text_length):\n",
        "\n",
        "        # print(\"===============================forward=================================\\n\")\n",
        "        labels = input_ids2\n",
        "        # print(f'labels : {input_ids2}')\n",
        "        # print(f\"labels : {labels}\")\n",
        "        # print(f\"len(labels) : {len(labels)}\")\n",
        "        # print(full_prompt_text_length)\n",
        "        batch_max_length = max(full_prompt_text_length)\n",
        "        # print(f\"batch_max_length : {batch_max_length}\")\n",
        "\n",
        "\n",
        "        for i in range(len(labels)):\n",
        "            labels[i][:concat_text_length[i]] = -100 # cross_entropy_ignore_index\n",
        "            labels[i][full_prompt_text_length[i]:] = -100\n",
        "        # print(f\"input_ids1 before reshaping : {input_ids1.shape}\")\n",
        "        # print(f\"labels before reshaping : {labels.shape}\")\n",
        "        # print(f\"attention_mask1 before reshaping : {attention_mask1.shape}\")\n",
        "        labels = labels[:,:batch_max_length].contiguous()\n",
        "        # print(f'labels : {labels}')\n",
        "        input_ids1 = input_ids1[:,:batch_max_length].contiguous()\n",
        "        attention_mask1 = attention_mask1[:,:batch_max_length].contiguous()\n",
        "        # print(f\"input_ids1 after reshaping : {input_ids1.shape}\")\n",
        "        # print(f\"labels after reshaping : {labels.shape}\")\n",
        "        # print(f\"attention_mask1 after reshaping : {attention_mask1.shape}\")\n",
        "            # print(f\"labels after -100 padding : {labels[0]}\")\n",
        "        # print(f\"len(labels[0]) before shifting : {len(labels[0])}\\n\")\n",
        "        # print(f\"input_ids1 : {input_ids1}\\n\")\n",
        "        # print(f\"attention_mask1 : {attention_mask1}\\n\")\n",
        "        # print(f\"input_ids2 : {input_ids2}\\n\")\n",
        "        # print(f\"concat_text_length : {concat_text_length}\\n\")\n",
        "        # print(f\"full_prompt_length : {full_prompt_text_length}\\n\")\n",
        "        # print(f\"labels : {labels}\\n\")\n",
        "        # print(f\"labels before shifting : {labels}, labels.size() : {labels.size()}\\n\")\n",
        "        # labels = labels[0][1:]\n",
        "        # labels = torch.Tensor(labels)\n",
        "        # print(f\"labels after shifting : {labels}, labels.size() : {labels.size()}\\n\")\n",
        "        # additional_pad = tokenizer.tokenize(\"[PAD]\")\n",
        "        # additional_pad = torch.Tensor(tokenizer.convert_tokens_to_ids(additional_pad))\n",
        "        # additional_pad = torch.Tensor(additional_pad)\n",
        "        # labels = torch.cat((labels, additional_pad))\n",
        "        # print(f\"labels after shifting : {labels}, labels.size() : {labels.size()}\\n\")\n",
        "\n",
        "        #outputs = self.model(input_ids=input_ids1, attention_mask=attention_mask1, labels=labels)\n",
        "\n",
        "        outputs = self.model(input_ids=input_ids1, attention_mask=attention_mask1, labels=labels)\n",
        "        logits = outputs.logits\n",
        "        loss = 0\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # print(f\"outputs : {outputs}\\n\")\n",
        "        # print(f\"outputs type : {type(outputs)}\")\n",
        "        # print(f\"input.shape : {outputs.shape}\")\n",
        "        # print(f\"logits : {logits}\")\n",
        "        # print(f\"logits type : {type(logits)}\")\n",
        "        # print(f\"logits shape : {logits.shape}\")\n",
        "        # print(f\"loss type : {type(loss)}\")\n",
        "        # print(f\"loss : {loss}\")\n",
        "        softmax_output = self.softmax(logits)\n",
        "        arg = torch.argmax(softmax_output, dim=2)\n",
        "        # print(f\"argmax(ouput) : {arg}\") # --> tensor\n",
        "        # print(f\"argmax(ouput).size() : {arg.size()}\")\n",
        "        # print(f\"argmax(output).dtype() : {arg.dtype}\")\n",
        "        # print(arg)\n",
        "        generated = self.tokenizer.decode(arg[0])\n",
        "        # print(f\"generated text: {generated}\\n\")\n",
        "        # print(f\"len(generated) : {len(self.tokenizer.tokenize(generated))}\\n\")\n",
        "        # print(f\"loss type : {type(loss)}\\n\")\n",
        "        # print(f\"loss shape : {loss.shape}\\n\")\n",
        "\n",
        "        return loss, outputs\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "\n",
        "        :param batch:\n",
        "        :param batch_idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # print(f\"training batch : {batch}\")\n",
        "        input_ids1 = batch[\"input_ids1\"]\n",
        "        attention_mask1 = batch[\"attention_mask1\"]\n",
        "        input_ids2 = batch[\"input_ids2\"]\n",
        "        attention_mask2 = batch[\"attention_mask2\"]\n",
        "        concat_text_length = batch[\"concat_text_length\"]\n",
        "        full_prompt_text_length = batch[\"full_prompt_text_length\"]\n",
        "        # labels = batch[\"labels\"]\n",
        "\n",
        "        loss, output = self(input_ids1, attention_mask1, input_ids2, attention_mask2, concat_text_length, full_prompt_text_length)\n",
        "\n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "        # print(f'type(np_loss) : {type(np_loss)}')\n",
        "        # print(f'np_loss : {np_loss}')\n",
        "\n",
        "        self.result_manager.trainLoss_batch.append(np_loss)\n",
        "        # print(\"===============================training=================================\\n\")\n",
        "        # print(f\"output : {outputs}\\n\")\n",
        "        # print(f\"output type : {type(outputs)}\\n\")\n",
        "        # print(f\"loss : {loss}\\n\")\n",
        "        # print(f\"loss type : {type(loss)}\\n\")\n",
        "        # print(f\"loss shape : {loss.shape}\\n\")\n",
        "\n",
        "        return {\"loss\": loss, \"predictions\": output}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "\n",
        "        :param batch:\n",
        "        :param batch_idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # print(f\"validation batch : {batch}\")\n",
        "        input_ids1 = batch[\"input_ids1\"]\n",
        "        attention_mask1 = batch[\"attention_mask1\"]\n",
        "        input_ids2 = batch[\"input_ids2\"]\n",
        "        attention_mask2 = batch[\"attention_mask2\"]\n",
        "        concat_text_length = batch[\"concat_text_length\"]\n",
        "        full_prompt_text_length = batch[\"full_prompt_text_length\"]\n",
        "        # labels = batch[\"labels\"]\n",
        "\n",
        "        loss, output = self(input_ids1, attention_mask1, input_ids2, attention_mask2, concat_text_length, full_prompt_text_length)\n",
        "\n",
        "        self.log(\"val_loss\", loss, prog_bar=True, logger=True, on_step=False)\n",
        "\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "        # print(f'type(np_loss) : {type(np_loss)}')\n",
        "        # print(f'np_loss : {np_loss}')\n",
        "        self.result_manager.valLoss_batch.append(np_loss)\n",
        "        # print(\"===============================validating=================================\\n\")\n",
        "        # print(f\"output : {outputs}\\n\")\n",
        "        # print(f\"output type : {type(outputs)}\\n\")\n",
        "        # print(f\"loss : {loss}\\n\")\n",
        "        # print(f\"loss type : {type(loss)}\\n\")\n",
        "        # print(f\"loss shape : {loss.shape}\\n\")\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "\n",
        "        :param batch:\n",
        "        :param batch_idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        # print(\"test batch : {batch}\")\n",
        "        input_ids1 = batch[\"input_ids1\"]\n",
        "        attention_mask1 = batch[\"attention_mask1\"]\n",
        "        input_ids2 = batch[\"input_ids2\"]\n",
        "        attention_mask2 = batch[\"attention_mask2\"]\n",
        "        concat_text_length = batch[\"concat_text_length\"]\n",
        "        full_prompt_text_length = batch[\"full_prompt_text_length\"]\n",
        "        # labels = batch[\"labels\"]\n",
        "\n",
        "        loss, output = self(input_ids1, attention_mask1, input_ids2, attention_mask2, concat_text_length, full_prompt_text_length)\n",
        "\n",
        "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
        "\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "        # print(f'type(np_loss) : {type(np_loss)}')\n",
        "        # print(f'np_loss : {np_loss}')\n",
        "        self.result_manager.testLoss_batch.append(np_loss)\n",
        "\n",
        "        # print(\"===============================testing=================================\\n\")\n",
        "        # print(f\"output : {outputs}\\n\")\n",
        "        # print(f\"output type : {type(outputs)}\\n\")\n",
        "        # print(f\"loss : {loss}\\n\")\n",
        "        # print(f\"loss type : {type(loss)}\\n\")\n",
        "        # print(f\"loss shape : {loss.shape}\\n\")\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        optimizer = AdamW(self.parameters(), lr=self.config_dict['LR'])\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.n_warmup_steps,\n",
        "                                                    num_training_steps=self.n_training_steps)\n",
        "        return dict(optimizer=optimizer, lr_scheduler=dict(scheduler=scheduler, interval=\"step\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA2UW3Xf58Vh"
      },
      "source": [
        "# calculate_warmup_stepsを定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ljK1VdE8BP5"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"This module is written for write useful function.\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_warmup_steps(train_df: pd.DataFrame, num_epochs: int, batch_size: int):\n",
        "    steps_per_epoch = len(train_df) // batch_size\n",
        "    total_training_steps = steps_per_epoch * num_epochs\n",
        "    warmup_steps = total_training_steps // 5\n",
        "    return total_training_steps, warmup_steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8eAFYfq3-CD"
      },
      "source": [
        "# ModelConfigを定義する\n",
        "- colab上では辞書\n",
        "- モジュール分けするときはjsonに"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dPiXl1ej7b3"
      },
      "source": [
        "## GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpgeqGQOkCpL"
      },
      "outputs": [],
      "source": [
        "# GPT2-small\n",
        "gpt2_small_config = {\n",
        "    'MODEL_NAME' : 'gpt2',\n",
        "    'TOKENIZER_NAME' : 'gpt2',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/small',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'small'],\n",
        "    'INPUT_FLIP' : 1\n",
        "\n",
        "}\n",
        "\n",
        "# GPT2-medium\n",
        "gpt2_medium_config = {\n",
        "    'MODEL_NAME' : 'gpt2-medium',\n",
        "    'TOKENIZER_NAME' : 'gpt2-medium',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/medium',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'medium'],\n",
        "    'INPUT_FLIP' : 1\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXnvK6fjj9uf"
      },
      "source": [
        "## T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27MGdEADkWyL"
      },
      "outputs": [],
      "source": [
        "# T5-small\n",
        "t5_small_config = {\n",
        "    'MODEL_NAME' : 't5-small',\n",
        "    'TOKENIZER_NAME' : 't5-small',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 't5/small',\n",
        "    'MODEL_FOLDER' : ['t5', 'small'],\n",
        "    'INPUT_FLIP' : 0\n",
        "}\n",
        "\n",
        "# T5-base\n",
        "t5_base_config = {\n",
        "    'MODEL_NAME' : 't5-base',\n",
        "    'TOKENIZER_NAME' : 't5-base',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 't5/base',\n",
        "    'MODEL_FOLDER' : ['t5', 'base'],\n",
        "    'INPUT_FLIP' : 0\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy9zBiRqkAC3"
      },
      "source": [
        "## BART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d4CXx8T4BWp"
      },
      "outputs": [],
      "source": [
        "# BART_base\n",
        "bart_base_config = {\n",
        "    'MODEL_NAME' : 'facebook/bart-base',\n",
        "    'TOKENIZER_NAME' : 'facebook/bart-base',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 'bart/base',\n",
        "    'MODEL_FOLDER' : ['bart', 'base'],\n",
        "    'INPUT_FLIP' : 0\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoWojRc90I8H"
      },
      "source": [
        "## Model_Dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IILYrc640NFr"
      },
      "outputs": [],
      "source": [
        "Model_Config_Dict = {\n",
        "    'gpt2_small_config' : gpt2_small_config,\n",
        "    'gpt2_medium_config' : gpt2_medium_config,\n",
        "    't5_small_config' : t5_small_config,\n",
        "    't5_base_config' : t5_base_config,\n",
        "    'bart_base_config' : bart_base_config\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shph_NDW3HpB"
      },
      "source": [
        "# class Prepare_tokenizerを定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0mJ9pIG3MHe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import warnings\n",
        "\n",
        "class Prepare_Tokenizer():\n",
        "    def __init__(self, config:dict):\n",
        "        self.config = config\n",
        "        self.TOKENIZER_NAME = self.config['TOKENIZER_NAME']\n",
        "        self.tokenizer = None\n",
        "        self.tokenizer_length = None\n",
        "        self.additional_special_tokens = None\n",
        "        self.eos_token = None\n",
        "        self.eos_token_id = None\n",
        "        self.pad_token = None\n",
        "        self.pad_token_id = None\n",
        "        self.exp_token = '[EXP]'\n",
        "        self.exp_token_id = None\n",
        "        self.initialization_flag = 0\n",
        "\n",
        "    def get_tokenizer(self):\n",
        "        if self.initialization_flag != 0:\n",
        "            raise ValueError(f\"tokenizer is already initialized. This instance is for {self.tokenizer}\")\n",
        "\n",
        "        if self.TOKENIZER_NAME in ['gpt2', 'gpt2-medium']:\n",
        "            self.initialization_flag = 1\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.TOKENIZER_NAME)\n",
        "            print(f\"tokenizer : AutoTokenizer.from_pretrained({self.TOKENIZER_NAME})\")\n",
        "            print(\"Before adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            # special_tokens_dict = {\n",
        "            # 'additional_special_tokens': ['[EXP]'],\n",
        "            # \"eos_token\" : \"<|endoftext|>\",\n",
        "            # \"pad_token\" : \"<|endoftext|>\"\n",
        "            # }\n",
        "            # 追加\n",
        "            special_tokens_dict = {\n",
        "            \"eos_token\" : \"<|endoftext|>\",\n",
        "            \"pad_token\" : \"<|endoftext|>\"\n",
        "            }\n",
        "            self.eos_token = \"<|endoftext|>\"\n",
        "            self.pad_token = \"<|endoftext|>\"\n",
        "            self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "            # 追加\n",
        "            self.tokenizer.add_tokens(['[EXP]'])\n",
        "            self.exp_token_id = self.tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "            self.eos_token_id = self.tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "            self.pad_token_id = self.tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "            print(\"After adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            print(f\"exp_id : {self.exp_token_id}\")\n",
        "            print(f\"eos_id : {self.eos_token_id}\")\n",
        "            print(f\"pad_id : {self.pad_token_id}\")\n",
        "            self.tokenizer_length = len(self.tokenizer)\n",
        "            print(f\"len(tokenizer) : {self.tokenizer_length}\")\n",
        "\n",
        "            return self.tokenizer\n",
        "\n",
        "        elif self.TOKENIZER_NAME in ['t5-small','t5-base']:\n",
        "            self.initialization_flag = 1\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.TOKENIZER_NAME)\n",
        "            print(f\"tokenizer : AutoTokenizer.from_pretrained({self.TOKENIZER_NAME})\")\n",
        "            print(\"Before adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            special_tokens_dict = {\n",
        "            'additional_special_tokens': ['[EXP]'],\n",
        "            }\n",
        "            self.eos_token = '</s>'\n",
        "            self.pad_token = '<pad>'\n",
        "            self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "            self.exp_token_id = self.tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "            self.eos_token_id = self.tokenizer.convert_tokens_to_ids('</s>')\n",
        "            self.pad_token_id = self.tokenizer.convert_tokens_to_ids('<pad>')\n",
        "            print(\"After adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            print(f\"exp_id : {self.exp_token_id}\")\n",
        "            print(f\"eos_id : {self.eos_token_id}\")\n",
        "            print(f\"pad_id : {self.pad_token_id}\")\n",
        "            self.tokenizer_length = len(self.tokenizer)\n",
        "            print(f\"len(tokenizer) : {self.tokenizer_length}\")\n",
        "\n",
        "            return self.tokenizer\n",
        "\n",
        "        elif self.TOKENIZER_NAME in ['facebook/bart-base']:\n",
        "            self.initialization_flag = 1\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.TOKENIZER_NAME)\n",
        "            # print(f\"tokenizer : AutoTokenizer.from_pretrained({self.TOKENIZER_NAME})\")\n",
        "            # print(\"Before adding additional_special_tokens\")\n",
        "            # print(self.tokenizer.all_special_tokens)\n",
        "            # print(self.tokenizer.all_special_ids)\n",
        "            special_tokens_dict = {\n",
        "            'additional_special_tokens': ['[EXP]'],\n",
        "            }\n",
        "            self.eos_token = '</s>'\n",
        "            self.pad_token = '<pad>'\n",
        "            self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "            self.exp_token_id = self.tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "            self.eos_token_id = self.tokenizer.convert_tokens_to_ids('</s>')\n",
        "            self.pad_token_id = self.tokenizer.convert_tokens_to_ids('<pad>')\n",
        "            # print(\"After adding additional_special_tokens\")\n",
        "            # print(self.tokenizer.all_special_tokens)\n",
        "            # print(self.tokenizer.all_special_ids)\n",
        "            # print(f\"exp_id : {self.exp_token_id}\")\n",
        "            # print(f\"eos_id : {self.eos_token_id}\")\n",
        "            # print(f\"pad_id : {self.pad_token_id}\")\n",
        "            self.tokenizer_length = len(self.tokenizer)\n",
        "            # print(f\"len(tokenizer) : {self.tokenizer_length}\")\n",
        "\n",
        "            return self.tokenizer\n",
        "        else:\n",
        "            raise ValueError('Wrong Tokenizer Type : you have to indicate tokenizer type out of the folloing list.\\n [\"gpt2\", \"gpt2-medium\", \"t5-small\", \"t5-base\", \"bart-base\"]\\n')\n",
        "\n",
        "    def get_tokenizer_info(self):\n",
        "        if self.initialization_flag == 0:\n",
        "            print(\"there is no information. you need to call get_tokenizer() first.\")\n",
        "        else:\n",
        "            print(\"---------- tokenizer information ----------\")\n",
        "            print(f\"self.TOKENIZER_NAME : {self.TOKENIZER_NAME}\")\n",
        "            print(f\"self.tokenizer : {self.tokenizer}\")\n",
        "            print(f\"self.tokenizer_length : {self.tokenizer_length}\")\n",
        "            print(f\"self.additional_special_tokens : {self.additional_special_tokens}\")\n",
        "            print(f\"self.eos_token : {self.eos_token}\")\n",
        "            print(f\"self.eos_token_id : {self.eos_token_id}\")\n",
        "            print(f\"self.pad_token : {self.pad_token}\")\n",
        "            print(f\"self.pad_token_id : {self.pad_token_id}\")\n",
        "            print(f\"self.exp_token : {self.exp_token}\")\n",
        "            print(f\"self.exp_token_id : {self.exp_token_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkBEZqVYGCj7"
      },
      "source": [
        "# class Prepare_Modelを定義する\n",
        "- 今のところ、モデルごとに操作の大きな違いはなさそう\n",
        "- BARTに関して,文の頭に`<s>`が付く"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE6f_5MZGr0k"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, T5ForConditionalGeneration, BartForConditionalGeneration, AutoConfig\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Prepare_Model():\n",
        "\n",
        "    def __init__(self, config:dict, prepare_tokenizer):\n",
        "        self.MODEL_NAME = config['MODEL_NAME']\n",
        "        self.TOKENIZER_LENGTH = prepare_tokenizer.tokenizer_length\n",
        "        self.additional_model_config = None\n",
        "        self.prepare_tokenizer = prepare_tokenizer\n",
        "        self.model = None\n",
        "        self.initialization_flag = 0\n",
        "\n",
        "\n",
        "    def get_model(self):\n",
        "        if self.initialization_flag == 1:\n",
        "            raise ValueError('You cannot call get_model() because this is already initialized.\\n You are supossed to instantiate Prepare_Model_Class and then call get_model() again.')\n",
        "\n",
        "        if self.MODEL_NAME in ['gpt2', 'gpt2-medium']:\n",
        "\n",
        "            if self.prepare_tokenizer is None:\n",
        "                raise ValueError('prepare_tokenizer is None and this is not good for gpt2!')\n",
        "            self.initialization_flag = 1\n",
        "            self.additional_model_config = AutoConfig.from_pretrained(self.MODEL_NAME, eos_token_id=self.prepare_tokenizer.eos_token_id, pad_token_id=self.prepare_tokenizer.pad_token_id, additional_special_tokens_id = self.prepare_tokenizer.exp_token_id, output_hidden_states=False)\n",
        "            self.model = GPT2LMHeadModel.from_pretrained(self.MODEL_NAME, config=self.additional_model_config)\n",
        "            self.model.resize_token_embeddings(self.TOKENIZER_LENGTH)\n",
        "            self.lm_head = self.model.lm_head\n",
        "            # new_weights = torch.cat([self.lm_head.weight[:-1, :], torch.zeros(1, self.lm_head.weight.shape[1]) -10000])\n",
        "            # self.model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "\n",
        "            return self.model\n",
        "\n",
        "        elif self.MODEL_NAME in ['t5-small','t5-base']:\n",
        "\n",
        "            self.initialization_flag = 1\n",
        "            self.model = T5ForConditionalGeneration.from_pretrained(self.MODEL_NAME)\n",
        "            self.model.resize_token_embeddings(self.TOKENIZER_LENGTH)\n",
        "            # not sure the following 2 lines are actually necessary\n",
        "            print(f\"weights.shape : {self.model.lm_head.weight.shape}\")\n",
        "            # new_weights = torch.cat([self.model.lm_head.weight[:-1, :], torch.zeros(1, self.model.lm_head.weight.shape[1]) -10000])\n",
        "            # self.model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "\n",
        "\n",
        "            # print(f\"new_weights.shape : {self.model.lm_head.weight.shape}\")\n",
        "\n",
        "            return self.model\n",
        "\n",
        "        elif self.MODEL_NAME in ['facebook/bart-base']:\n",
        "\n",
        "            self.initialization_flag = 1\n",
        "            self.model = BartForConditionalGeneration.from_pretrained(self.MODEL_NAME)\n",
        "            self.model.resize_token_embeddings(self.TOKENIZER_LENGTH)\n",
        "            # not sure the following 2 lines are actually necessary\n",
        "            new_weights = torch.cat([self.model.lm_head.weight[:-1, :], torch.zeros(1, self.model.lm_head.weight.shape[1]) -10000])\n",
        "            self.model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "            return self.model\n",
        "        else:\n",
        "            raise ValueError('Wrong Model Type : you have to indicate model type out of the folloing list.\\n [\"gpt2\", \"gpt2-medium\", \"t5-small\", \"t5-base\", \"bart-base\"]\\n')\n",
        "\n",
        "\n",
        "    def get_model_info(self):\n",
        "        if self.initialization_flag == 0:\n",
        "            print(\"there is no information. you need to call get_model() first.\")\n",
        "        else:\n",
        "            print(\"---------- model information ----------\")\n",
        "            print(f\"self.MODEL_NAME : {self.MODEL_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WCJwC68PT4G"
      },
      "outputs": [],
      "source": [
        "# my_prepare_tokenizer = Prepare_Tokenizer(bart_base_config)\n",
        "# my_prepare_tokenizer.get_tokenizer_info()\n",
        "# my_tokenizer = my_prepare_tokenizer.get_tokenizer()\n",
        "# my_prepare_tokenizer.get_tokenizer_info()\n",
        "# my_prepare_model = Prepare_Model(config=bart_base_config, prepare_tokenizer=my_prepare_tokenizer)\n",
        "# my_prepare_model.get_model_info()\n",
        "# my_model = my_prepare_model.get_model()\n",
        "# my_prepare_model.get_model_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1HyKsIv73SD"
      },
      "source": [
        "# 結果を確認する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRHqmgR0F59h"
      },
      "outputs": [],
      "source": [
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir=lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "157MXEFW9qNZ"
      },
      "source": [
        "# class Fine_Tuned_Model\n",
        "- fine-tuning済みのモデルをロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmN_mlcS9xxq"
      },
      "outputs": [],
      "source": [
        "class Fine_Tuned_Model():\n",
        "    def __init__(self, config_dict, ckpt_file_name, file_name:str):\n",
        "\n",
        "        self.config_dict = config_dict\n",
        "        self.ckpt_file_name = ckpt_file_name\n",
        "        self.file_name = file_name\n",
        "        self.component_name = config_dict['COMPONENT_NAME']\n",
        "        self.MODEL_NAME = config_dict[\"MODEL_NAME\"]\n",
        "        self.transformers_model_path = f'./model_transformers/{self.MODEL_NAME}/{self.file_name}'\n",
        "        self.initialization_flag = 0\n",
        "        self.model = None\n",
        "        self.pretrained_model = None\n",
        "\n",
        "    def create_directory_for_transformers_model(self):\n",
        "        if self.initialization_flag == 1:\n",
        "            raise ValueError('self.initialization_flag == 1 : you are supposed to instantiate Fine_Tuned_Model once and call it again.')\n",
        "        else:\n",
        "            self.initialization_flag = 1\n",
        "\n",
        "        if self.component_name == 'CEG':\n",
        "            self.model = CEG.load_from_checkpoint(gpt2_small_config['SAVED_MODEL_PATH']+ f'/{self.ckpt_file_name}')\n",
        "            self.model.model.save_pretrained(self.transformers_model_path)\n",
        "            print(f'{self.MODEL_NAME} was saved as {self.component_name} in {self.transformers_model_path}.')\n",
        "        elif self.component_name == 'EP':\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(f'self.component_name : {self.component_name} is not supported.')\n",
        "\n",
        "\n",
        "    def load_model(self):\n",
        "        if self.initialization_flag == 0:\n",
        "            raise ValueError('self.initialization_flag == 0 : you are supposed to call create_directory_for_transformer_model() first.')\n",
        "\n",
        "        if self.MODEL_NAME in ['gpt2', 'gpt2-medium']:\n",
        "            self.pretrained_model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/' + f'{self.MODEL_NAME}/{self.file_name}')\n",
        "            print(self.pretrained_model)\n",
        "\n",
        "            return self.pretrained_model\n",
        "\n",
        "        elif self.MODEL_NAME in ['t5-small','t5-base']:\n",
        "            self.pretrained_model = T5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/' + f'{self.MODEL_NAME}/{self.file_name}')\n",
        "            print(self.pretrained_model)\n",
        "\n",
        "            return self.pretrained_model\n",
        "\n",
        "        elif self.MODEL_NAME in ['facebook/bart-base']:\n",
        "            self.pretrained_model = BartForConditionalGeneration.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/' + f'{self.MODEL_NAME}/{self.file_name}')\n",
        "            print(self.pretrained_model)\n",
        "\n",
        "            return self.pretrained_model\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Wrong Model Type : you have to indicate model type out of the folloing list.\\n [\"gpt2\", \"gpt2-medium\", \"t5-small\", \"t5-base\", \"bart-base\"]\\n')\n",
        "\n",
        "\n",
        "    def get_model_info(self):\n",
        "        if self.initialization_flag == 0:\n",
        "            print(\"there is no information. you need to call load_model() first.\")\n",
        "        else:\n",
        "            print(\"---------- model information ----------\")\n",
        "            print(f'self.config_dict : {self.config_dict}')\n",
        "            print(f'self.ckpt_file_name : {self.ckpt_file_name}')\n",
        "            print(f'self.file_name : {self.file_name}')\n",
        "            print(f'self.component_name : {self.component_name}')\n",
        "            print(f'self.MODEL_NAME : {self.MODEL_NAME}')\n",
        "            print(f'self.transformers_model_path : {self.transformers_model_path}')\n",
        "            print(f'self.initialization_flag : {self.initialization_flag}')\n",
        "            print(f'self.model : {self.model}')\n",
        "            print(f'self.pretrained_model : {self.pretrained_model}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylG8KaRfYSd2"
      },
      "source": [
        "# class Result_Manager\n",
        "- 結果の保存や表示を行う\n",
        "- 次の指標を保存する\n",
        "## バッチごと\n",
        "- trainAcc\n",
        "- trainF1\n",
        "- trainloss\n",
        "- valAcc\n",
        "- valF1\n",
        "- valloss\n",
        "- testAcc\n",
        "- testF1\n",
        "- testloss\n",
        "\n",
        "\n",
        "## エポックごと\n",
        "- trainAcc\n",
        "- trainF1\n",
        "- trainloss\n",
        "- valAcc\n",
        "- valF1\n",
        "- valoss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHeo6y028rRe"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "import datetime\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0FmH8BGY5Zw"
      },
      "outputs": [],
      "source": [
        "class  Result_Manager():\n",
        "    def __init__(self, base_time, batch_size, model_config, set_type):\n",
        "        self.model_config = model_config\n",
        "        self.batch_size = batch_size\n",
        "        self.set_type = set_type\n",
        "        self.epoch_size = None\n",
        "        self.dt_now = base_time\n",
        "        self.metrics_name_list = [\n",
        "            'trainAcc_batch',\n",
        "            'trainF1_batch',\n",
        "            'trainLoss_batch',\n",
        "            'valAcc_batch',\n",
        "            'valF1_batch',\n",
        "            'valLoss_batch',\n",
        "            'testAcc_batch',\n",
        "            'testF1_batch',\n",
        "            'testLoss_batch',\n",
        "            'trainAcc_epoch',\n",
        "            'trainF1_epoch',\n",
        "            'trainLoss_epoch',\n",
        "            'valAcc_epoch',\n",
        "            'valF1_epoch',\n",
        "            'valLoss_epoch',\n",
        "        ]\n",
        "\n",
        "        self.trainAcc_batch = []\n",
        "        self.trainF1_batch = []\n",
        "        self.trainLoss_batch = []\n",
        "        self.valAcc_batch = []\n",
        "        self.valF1_batch = []\n",
        "        self.valLoss_batch = []\n",
        "        self.testAcc_batch = []\n",
        "        self.testF1_batch = []\n",
        "        self.testLoss_batch = []\n",
        "\n",
        "        self.trainAcc_epoch = []\n",
        "        self.trainF1_epoch = []\n",
        "        self.trainLoss_epoch = []\n",
        "        self.valAcc_epoch = []\n",
        "        self.valF1_epoch = []\n",
        "        self.valLoss_epoch = []\n",
        "\n",
        "        self.metrics_list = [\n",
        "            self.trainAcc_batch,\n",
        "            self.trainF1_batch,\n",
        "            self.trainLoss_batch,\n",
        "            self.valAcc_batch,\n",
        "            self.valF1_batch,\n",
        "            self.valLoss_batch,\n",
        "            self.testAcc_batch,\n",
        "            self.testF1_batch,\n",
        "            self.testLoss_batch,\n",
        "            self.trainAcc_epoch,\n",
        "            self.trainF1_epoch,\n",
        "            self.trainLoss_epoch,\n",
        "            self.valAcc_epoch,\n",
        "            self.valF1_epoch,\n",
        "            self.valLoss_epoch,\n",
        "        ]\n",
        "\n",
        "    def _build_folder(self, base_path: str):\n",
        "\n",
        "        for folder in self.model_config['MODEL_FOLDER']:\n",
        "            base_path = f'{base_path}/{folder}'\n",
        "            if not os.path.exists(base_path):\n",
        "                os.mkdir(base_path)\n",
        "\n",
        "        return base_path\n",
        "\n",
        "\n",
        "    def save_result_log(self, base_path: str):\n",
        "\n",
        "        base_path = f'{base_path}/result_log/{self.dt_now}'\n",
        "        if not os.path.exists(base_path):\n",
        "            os.mkdir(base_path)\n",
        "\n",
        "        model_path = self._build_folder(base_path)\n",
        "\n",
        "        folder = f'{model_path}/{self.set_type}_batch_size:{self.batch_size}'\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "\n",
        "        for idx in range(len(self.metrics_list)):\n",
        "\n",
        "            if len(self.metrics_list[idx]) != 0:\n",
        "                file_name = self.metrics_name_list[idx]+'.csv'\n",
        "                # print(f'file_name : {file_name}')\n",
        "                # np_metric = [item.detach().cpu().numpy().astype(float) for item in self.metrics_list[idx]]\n",
        "                np_metric = self.metrics_list[idx]\n",
        "                print(self.metrics_name_list[idx])\n",
        "                print(f'len(np_metric) : {len(np_metric)}')\n",
        "                data = pd.DataFrame({self.metrics_name_list[idx]:np_metric})\n",
        "                data.to_csv(f'{folder}/{file_name}', header=True, index=False)\n",
        "\n",
        "\n",
        "        # pd.Series(self.trainAcc_batch).to_csv(folder+'/trainAcc_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainF1_batch).to_csv(folder+'/trainF1_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainLoss_batch).to_csv(folder+'/trainLoss_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valAcc_batch).to_csv(folder+'/valAcc_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valF1_batch).to_csv(folder+'/valF1_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valLoss_batch).to_csv(folder+'/valLoss_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.testAcc_batch).to_csv(folder+'/testAcc_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.testF1_batch).to_csv(folder+'/testF1_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.testLoss_batch).to_csv(folder+'/testLoss_batch.csv', header=False, index=False)\n",
        "\n",
        "        # pd.Series(self.trainAcc_epoch).to_csv(folder+'/trainAcc_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainF1_epoch).to_csv(folder+'/trainF1_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainLoss_epoch).to_csv(folder+'/trainLoss_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valAcc_epoch).to_csv(folder+'/valAcc_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valF1_epoch).to_csv(folder+'/valF1_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valLoss_epoch).to_csv(folder+'/valLoss_epoch.csv', header=False, index=False)\n",
        "\n",
        "\n",
        "    def make_single_result_img(self, base_path:str):\n",
        "\n",
        "        base_path = f'{base_path}/plot_figures/{self.dt_now}'\n",
        "        if not os.path.exists(base_path):\n",
        "            os.mkdir(base_path)\n",
        "\n",
        "        model_path = self._build_folder(base_path)\n",
        "\n",
        "        folder = f'{model_path}/{self.set_type}_batch_size:{self.batch_size}'\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "\n",
        "        for idx in range(len(self.metrics_name_list)):\n",
        "\n",
        "            save_fig_path = f'/{folder}/' + self.metrics_name_list[idx] + '.png'\n",
        "\n",
        "            if len(self.metrics_list[idx]) == 0:\n",
        "                continue\n",
        "            else:\n",
        "                plt.figure()\n",
        "                np_metric = self.metrics_list[idx]\n",
        "                # print(f'type(np_metric) : {type(np_metric)}')\n",
        "                # print(self.metrics_name_list[idx])\n",
        "                # print(f'np_metric : {np_metric}')\n",
        "                data = pd.DataFrame({self.metrics_name_list[idx]:np_metric})\n",
        "                # print(f'type(data) : {type(data)}')\n",
        "                data = data.astype(float)\n",
        "                # print(f'type(data) : {type(data)}')\n",
        "                data.plot(\n",
        "                title=self.metrics_name_list[idx],\n",
        "                legend=True\n",
        "                )\n",
        "                plt.savefig(save_fig_path)\n",
        "                # plt.close('all')\n",
        "\n",
        "\n",
        "            # plt.figure()\n",
        "            # if len(self.metrics_list[idx]) != 0:\n",
        "            #     np_metric = self.metrics_list[idx]\n",
        "            #     print(f'type(np_metric) : {type(np_metric)}')\n",
        "            #     print(self.metrics_name_list[idx])\n",
        "            #     print(f'np_metric : {np_metric}')\n",
        "            #     data = pd.Series(np_metric)\n",
        "            #     print(f'type(data) : {type(data)}')\n",
        "            #     data = data.astype(float)\n",
        "            #     print(f'type(data) : {type(data)}')\n",
        "            #     data.plot(\n",
        "            #     title=self.metrics_name_list[idx],\n",
        "            #     legend=True\n",
        "            #     )\n",
        "            # plt.savefig(save_fig_path)\n",
        "            # plt.close('all')\n",
        "\n",
        "    # def make_imgs_into_one(self, base_path:str, metrics: list, figure_name:str):\n",
        "\n",
        "    #     base_path = base_path + '/plot_figures/' + self.dt_now + '/'\n",
        "    #     imgs = []\n",
        "\n",
        "    #     for i in range(len(metrics)):\n",
        "    #         j = base_path + metrics[i] + '.png'\n",
        "    #         imgs.append(j)\n",
        "    #         j = ''\n",
        "\n",
        "    #     save_fig_path = base_path + figure_name +'.png'\n",
        "    #     fig, axs = plt.subplots(1, len(metrics), figsize=(10, 5))\n",
        "\n",
        "    #     for i, im in zip(range(len(axs)), imgs):\n",
        "    #         img = Image.open(im)\n",
        "    #         axs[i].imshow(img)\n",
        "    #         axs[i].set_title(metrics[i])\n",
        "    #         axs[i].axis('off')\n",
        "\n",
        "    #     plt.show()\n",
        "    #     plt.savefig(save_fig_path)\n",
        "\n",
        "    # def make_data_into_one(self, base_path:str, metrics: list, figure_name:str):\n",
        "\n",
        "    #     base_path = base_path + '/result_log/' + self.dt_now + '/'\n",
        "    #     data = []\n",
        "\n",
        "    #     for i in range(len(metrics)):\n",
        "    #         data_path = base_path + i + '.csv'\n",
        "    #         data.append(pd.read_csv(data_path))\n",
        "    #         data_path = ''\n",
        "\n",
        "    #     save_fig_path = base_path + figure_name +'.png'\n",
        "    #     fig, axs = plt.subplots(1, len(metrics), figsize=(10, 5))\n",
        "\n",
        "    #     for i, im in zip(range(len(axs)), imgs):\n",
        "    #         img = Image.open(im)\n",
        "    #         axs[i].imshow(img) # cmap=カラーマップの選択\n",
        "    #         axs[i].set_title(metrics[i])\n",
        "    #         axs[i].axis('off')\n",
        "\n",
        "    #     plt.show()\n",
        "    #     plt.savefig(save_fig_path)\n",
        "\n",
        "    # def get_result_img(self, base_path:str, image_name:str):\n",
        "    #     imgs = base_path + '/plot_figures/' + self.dt_now + '/' + image_name\n",
        "    #     img = imread(fname=imgs, format='png')\n",
        "    #     plt.imshow(\n",
        "    #         img,\n",
        "    #     )\n",
        "    #     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zx_rIKKtDDqm"
      },
      "outputs": [],
      "source": [
        "# import random\n",
        "# base_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning'\n",
        "\n",
        "# my_Result_Manager = Result_Manager()\n",
        "# my_Result_Manager.save_result_log(base_path)\n",
        "# my_Result_Manager.make_single_result_img(base_path)\n",
        "# my_Result_Manager.make_imgs_into_one(base_path, ['trainLoss_batch', 'valLoss_batch','trainLoss_batch', 'valLoss_batch', 'trainLoss_batch', 'valLoss_batch'], 'example_image')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3lf9p7sLyYO"
      },
      "outputs": [],
      "source": [
        "# class myclass():\n",
        "#     def __init__(self, num:int):\n",
        "#         self.num = num\n",
        "\n",
        "#     def _add_one(self):\n",
        "#         return self.num +1\n",
        "\n",
        "#     def add_one(self):\n",
        "#         return self._add_one()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdXGjnkbMm9s"
      },
      "outputs": [],
      "source": [
        "# mc = myclass(99)\n",
        "# print(mc.add_one())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJThITDcX55U"
      },
      "source": [
        "# train_and_test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBM_FaGPz8Bm"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDufpKSVE3ng"
      },
      "source": [
        "## train_and_test_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwiMog4M1qv9"
      },
      "outputs": [],
      "source": [
        "train_and_test_config =  {\n",
        "    'MODEL_NAME' : ['gpt2_small_config', 'gpt2_medium_config', 't5_small_config', 't5_base_config', 'bart_base_config'],\n",
        "    'BATCH_SIZE' : [1, 2, 4, 8], # train\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'TRAINING_DATA_PATH' : path_dict['main_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['main_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['main_input_for_lightning_model_test_data_true'],\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asT-m5I2Zb8c"
      },
      "source": [
        "## save_pretrained_model_as_transformers_CEG.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rhdfYSVZhjv"
      },
      "outputs": [],
      "source": [
        "def save_pretrained_model_as_transformers_CEG(saved_model_path:str):\n",
        "\n",
        "    file_type=''\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    for i in range(len(saved_model_path)):\n",
        "        # print(saved_model_path[-i])\n",
        "        if saved_model_path[-i-1] == '/':\n",
        "            counter += 1\n",
        "        if counter == 4:\n",
        "            path_for_model_name = saved_model_path[-i:]\n",
        "            continue\n",
        "        if counter == 6:\n",
        "            print(saved_model_path[-i:])\n",
        "            file_type = saved_model_path[-i:]\n",
        "            print(f'file_type: {file_type}')\n",
        "            break\n",
        "\n",
        "    for j in range(len(path_for_model_name)):\n",
        "        if path_for_model_name[j] == '/':\n",
        "            path_for_model_name = path_for_model_name[j+1:]\n",
        "            print(path_for_model_name)\n",
        "            break\n",
        "\n",
        "    file_list = os.listdir(saved_model_path)\n",
        "    ckpt_name = file_list[0]\n",
        "    ckpt_path = f'{file_type}/{ckpt_name}'\n",
        "    print(f'ckpt_name : {ckpt_name}')\n",
        "    print(f'ckpt_path : {ckpt_path}')\n",
        "\n",
        "    tr_model_path = f'./model_transformers/{path_for_model_name}'\n",
        "    model = CEG.load_from_checkpoint(ckpt_path)\n",
        "    model.model.save_pretrained(tr_model_path)\n",
        "\n",
        "    # if model_name in ['gpt2', 'gpt2-medium']:\n",
        "    #     model_from_pretrained = GPT2LMHeadModel.from_pretrained(tr_model_path)\n",
        "    # elif model_name in ['t5-small','t5-base']:\n",
        "    #     model_from_pretrained = T5ForConditionalGeneration.from_pretrained(tr_model_path)\n",
        "    # elif model_name in ['facebook/bart-base']:\n",
        "    #     model_from_pretrained = BartForConditionalGeneration.from_pretrained(tr_model_path)\n",
        "    # else:\n",
        "    #     raise ValueError('Wrong Model Type')\n",
        "\n",
        "    # print(f'pretrained model was saved in {tr_model_path}')\n",
        "\n",
        "    # return model_from_pretrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8DU2vwmi62H"
      },
      "source": [
        "## def train_and_test_CEG\n",
        "- バッチをいろいろ変えて実験を行う"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYTebhHDjLvC"
      },
      "outputs": [],
      "source": [
        "def train_and_test_CEG(model_config, train_and_test_config, batch_size:int, base_time, set_type):\n",
        "\n",
        "    print(f'----------- training and testing has just started (batch_size : {batch_size}) -----------')\n",
        "    pl.seed_everything(train_and_test_config['RANDOM_SEED'])\n",
        "\n",
        "    train_data = read_tsv(train_and_test_config[\"TRAINING_DATA_PATH\"])\n",
        "    test_data = read_tsv(train_and_test_config[\"TEST_DATA_PATH\"])\n",
        "    val_data = read_tsv(train_and_test_config[\"VALIDATION_DATA_PATH\"])\n",
        "    model_config['TRAINING_BATCH_SIZE'] = batch_size\n",
        "    result_manager = Result_Manager(base_time, batch_size, model_config, set_type)\n",
        "    base_path = path_dict['TensorBoardLogger'] + f'/{base_time}'\n",
        "    folder = model_config['TensorBoardLogger_NAME'] + f'/batch_size:{batch_size}'\n",
        "\n",
        "    data_module =  LIAR_PLUS_DataModule_For_CEG(\n",
        "                    model_config,\n",
        "                    train_data,\n",
        "                    test_data,\n",
        "                    val_data\n",
        "                    )\n",
        "\n",
        "    total_training_steps, warmup_steps = calculate_warmup_steps(train_data, model_config[\"TRAINING_EPOCHS\"],\n",
        "                                                        model_config[\"TRAINING_BATCH_SIZE\"])\n",
        "    print(f\"total_training_steps : {total_training_steps} , warmup_steps : {warmup_steps} \")\n",
        "    model = CEG(\n",
        "            config_dict=model_config,\n",
        "            n_warmup_steps=warmup_steps,\n",
        "            n_training_steps=total_training_steps,\n",
        "            n_classes=2,\n",
        "            result_manager=result_manager)\n",
        "\n",
        "    checkpoint_callback, saved_model_path = build_checkpoint_callback(model_config, base_time, batch_size, set_type)\n",
        "    logger = TensorBoardLogger(base_path, name=folder)\n",
        "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        logger=logger,\n",
        "        callbacks=[checkpoint_callback, early_stopping_callback],\n",
        "        max_epochs=model_config['TRAINING_EPOCHS'])\n",
        "\n",
        "    trainer.fit(model, datamodule=data_module)\n",
        "    trainer.test(datamodule=data_module)\n",
        "\n",
        "    model.result_manager.save_result_log('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning')\n",
        "    model.result_manager.make_single_result_img('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning')\n",
        "\n",
        "    save_pretrained_model_as_transformers_CEG(saved_model_path)\n",
        "    # print(f'model_from_pretrained : {model_from_pretrained}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gN4z8_DGBp0"
      },
      "source": [
        "## def get_base_time\n",
        "- ベースタイムを取得する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g97VCxNfGZFk"
      },
      "outputs": [],
      "source": [
        "def get_base_time():\n",
        "\n",
        "    now_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "    base_time = str(now_time)[:-16]\n",
        "\n",
        "    print(f'you got base_time : {base_time}')\n",
        "\n",
        "    return base_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uW-MK4zqqvJ"
      },
      "outputs": [],
      "source": [
        "# base_time = get_base_time()\n",
        "# train_and_test_CEG(\n",
        "#     model_config=gpt2_small_config,\n",
        "#     train_and_test_config=train_and_test_config,\n",
        "#     batch_size=1,\n",
        "#     base_time=base_time,\n",
        "#     set_type='true')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60u4GTAT0Cvw"
      },
      "source": [
        "# train_and_test_CEGの実行による各モデルの訓練"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-HuZdnZ0Nrs"
      },
      "source": [
        "## True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCkruQmdooH7"
      },
      "source": [
        "### train_and_test_config_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MgNzNB603K0"
      },
      "outputs": [],
      "source": [
        "train_and_test_config_true =  {\n",
        "    'MODEL_NAME' : ['gpt2_small_config', 'gpt2_medium_config', 't5_small_config', 't5_base_config', 'bart_base_config'],\n",
        "    'BATCH_SIZE' : [1, 2, 4, 8], # train\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'TRAINING_DATA_PATH' : path_dict['main_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['main_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['main_input_for_lightning_model_test_data_true'],\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XD5RHS2dXOuU"
      },
      "outputs": [],
      "source": [
        "# main_train_data_true = read_tsv(train_and_test_config_true['TRAINING_DATA_PATH'])\n",
        "# main_test_data_true = read_tsv(train_and_test_config_true['TEST_DATA_PATH'])\n",
        "# main_val_data_true = read_tsv(train_and_test_config_true['VALIDATION_DATA_PATH'])\n",
        "\n",
        "# print(len(main_train_data_true))\n",
        "# print(len(main_test_data_true))\n",
        "# print(len(main_val_data_true))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N3AvG7lydzu"
      },
      "source": [
        "### gpt2-small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LoaFzU1zb37"
      },
      "outputs": [],
      "source": [
        "# # # gpt2-small\n",
        "# base_time = get_base_time()\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_small_config,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h46osNIZygU_"
      },
      "source": [
        "### gpt2-medium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbWO94Io0RLO"
      },
      "outputs": [],
      "source": [
        "# # # gpt2-medium\n",
        "# base_time = get_base_time()\n",
        "# # for batch_size in [1, 2]:\n",
        "# #     train_and_test_CEG(\n",
        "# #         model_config=gpt2_medium_config,\n",
        "# #         train_and_test_config=train_and_test_config_true,\n",
        "# #         batch_size=batch_size,\n",
        "# #         base_time=base_time,\n",
        "# #         set_type=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La7hJpKuyvWz"
      },
      "source": [
        "### T5-small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHL8J0lI0Rlx"
      },
      "outputs": [],
      "source": [
        "# # t5-small\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=t5_small_config,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAizUat5yyqT"
      },
      "source": [
        "### T5-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9ZSBLd60Rs8"
      },
      "outputs": [],
      "source": [
        "# # t5-base\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=t5_base_config,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqx7eVMOy1Fc"
      },
      "source": [
        "### BART-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVbTeLvm0Rwf"
      },
      "outputs": [],
      "source": [
        "# # bart-base\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=bart_base_config,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfJfTUNn0X11"
      },
      "source": [
        "## False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpAontdzo0Le"
      },
      "source": [
        "### train_and_test_config_false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FusPgqAy0_Kf"
      },
      "outputs": [],
      "source": [
        "train_and_test_config_false =  {\n",
        "    'MODEL_NAME' : ['gpt2_small_config', 'gpt2_medium_config', 't5_small_config', 't5_base_config', 'bart_base_config'],\n",
        "    'BATCH_SIZE' : [1, 2, 4, 8], # train\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'TRAINING_DATA_PATH' : path_dict['main_input_for_lightning_model_train_data_false'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['main_input_for_lightning_model_val_data_false'],\n",
        "    'TEST_DATA_PATH' : path_dict['main_input_for_lightning_model_test_data_false'],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyoEbFt1ZH-A"
      },
      "outputs": [],
      "source": [
        "# main_train_data_false = read_tsv(train_and_test_config_false['TRAINING_DATA_PATH'])\n",
        "# main_test_data_false = read_tsv(train_and_test_config_false['TEST_DATA_PATH'])\n",
        "# main_val_data_false = read_tsv(train_and_test_config_false['VALIDATION_DATA_PATH'])\n",
        "\n",
        "# print(len(main_train_data_false))\n",
        "# print(len(main_test_data_false))\n",
        "# print(len(main_val_data_false))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka9DoL870X12"
      },
      "source": [
        "### gpt2-small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY0r-H1w0X13"
      },
      "outputs": [],
      "source": [
        "# # # gpt2-small\n",
        "# base_time = get_base_time()\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_small_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=false)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWmiAotx0X13"
      },
      "source": [
        "### gpt2-medium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8AC7YXA0X13"
      },
      "outputs": [],
      "source": [
        "# # gpt2-medium\n",
        "# base_time = get_base_time()\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_medium_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=false)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r4MVsod0X14"
      },
      "source": [
        "### T5-small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VGsyVZb0X15"
      },
      "outputs": [],
      "source": [
        "# # t5-small\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=t5_small_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=false)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okDGftAa0X15"
      },
      "source": [
        "### T5-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2TcfQFz0X16"
      },
      "outputs": [],
      "source": [
        "# # t5-base\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=t5_base_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=false)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU5Bspoz0X16"
      },
      "source": [
        "### BART-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLz4ionR0X16"
      },
      "outputs": [],
      "source": [
        "# # bart-base\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=bart_base_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=false)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6eFhN6gYCIY"
      },
      "source": [
        "# predict_CEG.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FClHb_Z7QKDM"
      },
      "source": [
        "## prediction_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6yLpINWQdYu"
      },
      "source": [
        "### GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcS52jgHQdZJ"
      },
      "outputs": [],
      "source": [
        "# GPT2-small\n",
        "gpt2_small_config_for_prediction = {\n",
        "    'MODEL_NAME' : 'gpt2',\n",
        "    'TOKENIZER_NAME' : 'gpt2',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/small',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'small'],\n",
        "    'INPUT_FLIP' : 1\n",
        "}\n",
        "\n",
        "# GPT2-medium\n",
        "gpt2_medium_config_for_prediction = {\n",
        "    'MODEL_NAME' : 'gpt2-medium',\n",
        "    'TOKENIZER_NAME' : 'gpt2-medium',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/medium',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'medium'],\n",
        "    'INPUT_FLIP' : 1\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDWJumDfQdZK"
      },
      "source": [
        "### T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktq3nYX8QdZK"
      },
      "outputs": [],
      "source": [
        "# T5-small\n",
        "t5_small_config_for_prediction = {\n",
        "    'MODEL_NAME' : 't5-small',\n",
        "    'TOKENIZER_NAME' : 't5-small',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata'],\n",
        "    'TensorBoardLogger_NAME' : 't5/small',\n",
        "    'MODEL_FOLDER' : ['t5', 'small'],\n",
        "    'INPUT_FLIP' : 0\n",
        "}\n",
        "\n",
        "# T5-base\n",
        "t5_base_config_for_prediction = {\n",
        "    'MODEL_NAME' : 't5-base',\n",
        "    'TOKENIZER_NAME' : 't5-base',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata'],\n",
        "    'TensorBoardLogger_NAME' : 't5/base',\n",
        "    'MODEL_FOLDER' : ['t5', 'base'],\n",
        "    'INPUT_FLIP' : 0\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9471NurQdZL"
      },
      "source": [
        "### BART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ7l-9zUQdZM"
      },
      "outputs": [],
      "source": [
        "# BART_base\n",
        "bart_base_config_for_prediction = {\n",
        "    'MODEL_NAME' : 'facebook/bart-base',\n",
        "    'TOKENIZER_NAME' : 'facebook/bart-base',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata'],\n",
        "    'TensorBoardLogger_NAME' : 'bart/base',\n",
        "    'MODEL_FOLDER' : ['bart', 'base'],\n",
        "    'INPUT_FLIP' : 0\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7obqzggSvFZN"
      },
      "source": [
        "## def load_transformers_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LKKGgntvKDM"
      },
      "outputs": [],
      "source": [
        "def load_transformers_model(model_name:str, tr_model_path:str):\n",
        "\n",
        "    if model_name in ['gpt2', 'gpt2-medium']:\n",
        "        model_from_pretrained = GPT2LMHeadModel.from_pretrained(tr_model_path)\n",
        "    elif model_name in ['t5-small','t5-base']:\n",
        "        model_from_pretrained = T5ForConditionalGeneration.from_pretrained(tr_model_path)\n",
        "    elif model_name in ['facebook/bart-base']:\n",
        "        model_from_pretrained = BartForConditionalGeneration.from_pretrained(tr_model_path)\n",
        "    else:\n",
        "        raise ValueError('Wrong Model Type')\n",
        "\n",
        "    return model_from_pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUJzynDQvyXs"
      },
      "outputs": [],
      "source": [
        "# tr_model_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/gpt2/small/2024-01-16 18:36/batch_size=1'\n",
        "# model_from_pretrained = load_transformers_model('gpt2', tr_model_path)\n",
        "# train_data = read_tsv(train_and_test_config[\"TRAINING_DATA_PATH\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNhDiYAaPCTe"
      },
      "source": [
        "## def get_textualized_outputs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9HjMNUUPIOx"
      },
      "outputs": [],
      "source": [
        "def get_textualized_outputs(outputs, input_length, tokenizer):\n",
        "\n",
        "    # print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "    generated_tokens = outputs.sequences[:, input_length:]\n",
        "    # print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "    tokens_list = outputs.sequences[0]\n",
        "    # print(tokens_list)\n",
        "    full_text = tokenizer.decode(tokens_list)\n",
        "    generated_text = tokenizer.decode(generated_tokens[0])\n",
        "\n",
        "    # print(f\"full text : {full_text}\\n\")\n",
        "    # print(f\"generated text: {generated_text}\")\n",
        "    # print(f\"len(generated) : {len(tokenizer.tokenize(generated_text))}\\n\")\n",
        "\n",
        "    return full_text, generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8m33Kj6LRTY"
      },
      "source": [
        "## def extend_data_with_generated_justification()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRABoNMGLnt0"
      },
      "outputs": [],
      "source": [
        "def extend_data_with_generated_justification(data, data_row, generated_text):\n",
        "\n",
        "    # df_data_row = pd.DataFrame(data_row)\n",
        "    df = pd.DataFrame([generated_text])\n",
        "\n",
        "    concatinated_data_row = pd.concat([data_row, df], ignore_index=True, axis=0).T\n",
        "    data = pd.concat([data, concatinated_data_row], axis=0).reset_index(drop=True)\n",
        "\n",
        "# data = pd.DataFrame()\n",
        "# data_row = train_data.iloc[0]\n",
        "# j = pd.DataFrame(['1111'])\n",
        "# concatinated_data_row = pd.concat([data_row, j], axis=0).T\n",
        "# # print(len(concatinated_data_row))\n",
        "# # concatinated_data_row\n",
        "# data_1 = pd.concat([data, concatinated_data_row], axis=0).reset_index(drop=True)\n",
        "# data_2 = pd.concat([data_1, concatinated_data_row], axis=0).reset_index(drop=True)\n",
        "# data_3 = pd.concat([data_2, concatinated_data_row], axis=0).reset_index(drop=True)\n",
        "# data_3\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_AtPhqnvb-q"
      },
      "source": [
        "## def predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCQsJDlcYObg"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def predict(tokenizer, prepare_tokenizer, model_from_pretrained, model_config, data:pd.DataFrame):\n",
        "\n",
        "    extended_data = pd.DataFrame()\n",
        "\n",
        "    for idx in tqdm(range(len(data))):\n",
        "\n",
        "        # ilocでデータ列を取得\n",
        "        data_row = data.iloc[idx]\n",
        "        # print(data_row)\n",
        "\n",
        "        # get_concatinated_input\n",
        "        prompt, full_input, concatinated_input_length, full_input_length = \\\n",
        "        concatinated_inputs_generator(data_row, model_config, tokenizer, prepare_tokenizer, model_config[\"INPUT_FLIP\"])\n",
        "\n",
        "        # モデルに入力\n",
        "        inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
        "        prompt_length = len(inputs[\"input_ids\"][0])\n",
        "        # print(f\"inputs_len : {prompt_length}\")\n",
        "\n",
        "        # greedy_search\n",
        "        # outputs = model.generate(**inputs, max_new_tokens=15, return_dict_in_generate=True, output_scores=True)\n",
        "        # beam_search\n",
        "        outputs = model_from_pretrained.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            num_beams=4,\n",
        "            num_return_sequences=1,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "        )\n",
        "\n",
        "        # get_textualized_outputs()\n",
        "        full_text, generated_text = get_textualized_outputs(outputs, prompt_length, tokenizer)\n",
        "\n",
        "        # print(f'generated text : {generated_text}')\n",
        "\n",
        "        # extend_data_with_generated_justification()\n",
        "        extended_data = extend_data_with_generated_justification(extended_data, data_row, generated_text)\n",
        "\n",
        "        del data_row\n",
        "\n",
        "    return extended_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMye9Hsw2RDt"
      },
      "source": [
        "## def predict_CEG.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnPtr-4jrobd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def predict_CEG(model_config, model_name:str, data:pd.DataFrame, ckpt_path, stage:str, binary_type:str, is_toy, base_time):\n",
        "    replaced_ckpt_path = ckpt_path.replace('/', '_')\n",
        "    replaced_model_name = model_name.replace('/', '-')\n",
        "    tr_model_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/' + model_config['TensorBoardLogger_NAME'] +f'/{ckpt_path}'\n",
        "\n",
        "    if is_toy == 0:\n",
        "        saved_data_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/' + model_config['TensorBoardLogger_NAME'] + f'/{stage}/{binary_type}_{replaced_ckpt_path}.tsv'\n",
        "    elif is_toy == 1:\n",
        "        base_path = f'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/{base_time}'\n",
        "        if not os.path.exists(base_path):\n",
        "            os.mkdir(base_path)\n",
        "\n",
        "        saved_data_path = f'{base_path}/{replaced_model_name}_{replaced_ckpt_path}_{binary_type}.tsv'\n",
        "    else:\n",
        "        ValueError('Wrong is_toy type')\n",
        "\n",
        "\n",
        "    model_from_pretrained = load_transformers_model(model_name, tr_model_path)\n",
        "\n",
        "    prepare_tokenizer = Prepare_Tokenizer(model_config)\n",
        "    tokenizer = prepare_tokenizer.get_tokenizer()\n",
        "\n",
        "    extended_data = predict(tokenizer, prepare_tokenizer, model_from_pretrained, model_config, data)\n",
        "    print(f'saved_model_path : {saved_data_path}')\n",
        "    write_dataframe_in_tsv(extended_data, saved_data_path)\n",
        "\n",
        "    print(f'the extended data was saved in {saved_data_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSPUHtxpKskA"
      },
      "source": [
        "## def jsut_predict_CEG()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p11tfx25tR73"
      },
      "outputs": [],
      "source": [
        "def just_predict_CEG(model_config, model_name:str, data:pd.DataFrame, ckpt_path, stage:str, binary_type:str):\n",
        "    replaced_ckpt_path = ckpt_path.replace('/', '_')\n",
        "    tr_model_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/' + model_config['TensorBoardLogger_NAME'] +f'/{ckpt_path}'\n",
        "    saved_data_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/' + model_config['TensorBoardLogger_NAME'] + f'/{stage}/{binary_type}_{replaced_ckpt_path}.tsv'\n",
        "    model_from_pretrained = load_transformers_model(model_name, tr_model_path)\n",
        "\n",
        "    prepare_tokenizer = Prepare_Tokenizer(model_config)\n",
        "    tokenizer = prepare_tokenizer.get_tokenizer()\n",
        "\n",
        "    extended_data = predict(tokenizer, prepare_tokenizer, model_from_pretrained, model_config, data)\n",
        "    # write_dataframe_in_tsv(extended_data, saved_data_path)\n",
        "\n",
        "    # print(f'the extended data was saved in {saved_data_path}')\n",
        "    return extended_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzoRlC6-m_Bh"
      },
      "source": [
        "## def predict_and_extend_data_for_EP()\n",
        "- *ファインチューニング済みのモデルを用いて推論を行った後、推論で得たt_trueとt_falseを新たに含むデータを作成し保存する*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjNmy_rJpMy_",
        "outputId": "6bdd47b4-e81c-4711-f7b9-34415a6b9587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10240 entries, 0 to 10239\n",
            "Data columns (total 7 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       10240 non-null  float64\n",
            " 1   1       10240 non-null  object \n",
            " 2   2       10240 non-null  object \n",
            " 3   3       10240 non-null  object \n",
            " 4   4       10240 non-null  object \n",
            " 5   5       10240 non-null  float64\n",
            " 6   6       10240 non-null  object \n",
            "dtypes: float64(2), object(5)\n",
            "memory usage: 560.1+ KB\n"
          ]
        }
      ],
      "source": [
        "ckpt_config = {\n",
        "    'true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/bart/base/2024-02-01 17:37/true_batch_size=4',\n",
        "    'false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/bart/base/2024-02-01 17:37/false_batch_size=2',\n",
        "}\n",
        "\n",
        "data = read_tsv('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_train2.tsv')\n",
        "\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWsoBU4jn14W"
      },
      "outputs": [],
      "source": [
        "# # extended_data_frameを外部から受け取り、推論結果を追加する\n",
        "\n",
        "# def predict_and_extend_data_for_EP(ckpt_config:dict, model_config:dict, data:pd.DataFrame, extended_data:pd.DataFrame, file_name:str):\n",
        "\n",
        "#     # print(f'data_size : {len(data)}')\n",
        "\n",
        "#     tmp_df = pd.DataFrame()\n",
        "\n",
        "#     # transformer対応のモデルを呼び出す\n",
        "#     model_true = load_transformers_model(model_config['MODEL_NAME'], ckpt_config['true'])\n",
        "#     model_false = load_transformers_model(model_config['MODEL_NAME'], ckpt_config['false'])\n",
        "#     prepare_tokenizer = Prepare_Tokenizer(model_config)\n",
        "#     tokenizer = prepare_tokenizer.get_tokenizer()\n",
        "\n",
        "#     # モデルを用いて説明文を生成する\n",
        "#     print('the generation of jusitification labeled on true is going on ...')\n",
        "#     extended_data_true = predict(tokenizer, prepare_tokenizer, model_true, model_config, data)\n",
        "#     # print(extended_data_true)\n",
        "#     print('the generation of jusitification labeled on false is going on ...')\n",
        "#     extended_data_false = predict(tokenizer, prepare_tokenizer, model_false, model_config, data)\n",
        "#     # print(extended_data_false)\n",
        "\n",
        "#     extended_data = pd.concat([data[:5], extended_data_true[7], extended_data_false[7]], axis=1, ignore_index=True)\n",
        "\n",
        "\n",
        "#     # 生成した説明文を持つデータを新たに作成する\n",
        "#     # true->[7] , false -> [8]\n",
        "\n",
        "#     # print(f'input_data : {data}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     # if len(extended_data) == 0:\n",
        "#     #     concatinated_justification = pd.concat([data, extended_data_true[7], extended_data_false[7]], ignore_index=True, axis=1)\n",
        "#     #     print(f'concatinated_justification.shape : {concatinated_justification.shape}')\n",
        "#     #     extended_data = pd.concat([extended_data, concatinated_justification], axis=0)\n",
        "#     #     print(f'extended_data.shape : {extended_data.shape}')\n",
        "#     # else :\n",
        "#     #     # i_date = df['date']\n",
        "#     #     # i_pred = pd.Series(gr.predict(X_test), index=df.index)\n",
        "#     #     # df2 = pd.concat([i_date,i_pred], axis=1)\n",
        "\n",
        "#     #     # concatinated_justification = pd.concat([data.reset_index().T[1:8], pd.DataFrame(extended_data_true[7]), pd.DataFrame(extended_data_false[7])], ignore_index=True, axis=0).T\n",
        "#     #     concatinated_justification = pd.concat([data.reset_index()[1:8], pd.DataFrame(extended_data_true[7]), pd.DataFrame(extended_data_false[7])], ignore_index=True, axis=0)\n",
        "\n",
        "#     #     print(f'concatinated_justification.shape : {concatinated_justification.shape}')\n",
        "#     #     extended_data = pd.concat([extended_data, concatinated_justification], axis=0)\n",
        "#         # print(f'extended_data.shape : {extended_data.shape}')\n",
        "\n",
        "#     # 拡張されたデータを保存する\n",
        "#     saved_data_path = f'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/{file_name}.tsv'\n",
        "#     write_dataframe_in_tsv(extended_data, saved_data_path)\n",
        "\n",
        "#     print(f'the extended data was saved in {saved_data_path}.')\n",
        "\n",
        "#     return extended_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjh0mYgIj8z4"
      },
      "outputs": [],
      "source": [
        "def predict_and_extend_data_for_EP(ckpt_config:dict, model_config:dict, data:pd.DataFrame, file_name:str):\n",
        "\n",
        "    extended_data = pd.DataFrame()\n",
        "\n",
        "    # transformer対応のモデルを呼び出す\n",
        "    model_true = load_transformers_model(model_config['MODEL_NAME'], ckpt_config['true'])\n",
        "    model_false = load_transformers_model(model_config['MODEL_NAME'], ckpt_config['false'])\n",
        "    prepare_tokenizer = Prepare_Tokenizer(model_config)\n",
        "    tokenizer = prepare_tokenizer.get_tokenizer()\n",
        "\n",
        "    # モデルを用いて説明文を生成する\n",
        "    print('the justification generation on true is going on ...')\n",
        "    extended_data_true = predict(tokenizer, prepare_tokenizer, model_true, model_config, data)\n",
        "    print('the justification generation on false is going on ...')\n",
        "    extended_data_false = predict(tokenizer, prepare_tokenizer, model_false, model_config, data)\n",
        "\n",
        "    # 生成した説明文を持つデータを新たに作成する\n",
        "    # true->[7] , false -> [8]\n",
        "    extended_data = pd.concat([data, extended_data_true[7], extended_data_false[7]], axis=1, ignore_index=True)\n",
        "\n",
        "    # 拡張されたデータを保存する\n",
        "    saved_data_path = f'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/{file_name}.tsv'\n",
        "    write_dataframe_in_tsv(extended_data, saved_data_path)\n",
        "\n",
        "    print(f'the extended data was saved in {saved_data_path}.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "header_names = [\n",
        "    'id', 'statement', 'metadata', 'gold_justification', 'credit_score', 'json_id', 'true_justification', 'false_justification'\n",
        "]"
      ],
      "metadata": {
        "id": "sSnVgaR91-qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(f'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/train[0:500].tsv', sep=\"\\t\",header=0, names=header_names)\n",
        "# df2 = pd.read_csv(f'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/train[500:1000].tsv', sep=\"\\t\",header=0, names=header_names)\n",
        "# df1 = pd.read_csv(f'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/train[0:500].tsv', sep=\"\\t\", header=None)\n",
        "# df2 = pd.read_csv(f'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/train[500:1000].tsv', sep=\"\\t\",  header=None)\n",
        "\n",
        "df1[8]\n",
        "# df2.info()\n",
        "# df = pd.read_csv('test.csv', header=None, skiprows=[0])\n",
        "# pd.concat([df1, df2], axis=1, ignore_index=True).reset_index().info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "RzL6YCbhxepz",
        "outputId": "77f3e80f-0e7c-4a84-c908-ab300908aee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "8",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 8",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-128-cedf73820b67>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# df2 = pd.read_csv(f'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/train[500:1000].tsv', sep=\"\\t\",  header=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# df2.info()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# df = pd.read_csv('test.csv', header=None, skiprows=[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 8"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi1CSgjSr7nS",
        "outputId": "bcf2bf2d-2ebf-433f-a2b1-b42d38baa63a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train[0:500]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       500 non-null    object \n",
            " 8   8       500 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 35.3+ KB\n",
            "train[500:1000]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       500 non-null    object \n",
            " 8   8       500 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n",
            "train[1000:1500]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       499 non-null    object \n",
            " 8   8       499 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n",
            "train[1500:2000]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       500 non-null    object \n",
            " 8   8       500 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n",
            "train[2000:2500]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       500 non-null    object \n",
            " 8   8       500 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n",
            "train[2500:3000]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       500 non-null    object \n",
            " 8   8       500 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n",
            "train[3000:3500]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       500 non-null    object \n",
            " 8   8       500 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n",
            "train[3500:4000]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       500 non-null    object \n",
            " 8   8       500 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n",
            "train[4000:4500]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       500 non-null    object \n",
            " 8   8       500 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n",
            "train[4500:5000]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       500 non-null    object \n",
            " 8   8       500 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n",
            "train[5000:5500]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       500 non-null    object \n",
            " 8   8       500 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n",
            "train[5500:6000]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       500 non-null    object \n",
            " 8   8       500 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n",
            "train[6000:6500]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       499 non-null    object \n",
            " 8   8       499 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n",
            "train[6500:7000]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       500 non-null    object \n",
            " 8   8       500 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n",
            "train[7000:7500]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       500 non-null    object \n",
            " 8   8       500 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n",
            "train[7500:8000]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       499 non-null    object \n",
            " 8   8       499 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n",
            "train[8000:8500]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       500 non-null    object \n",
            " 8   8       500 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n",
            "train[8500:9000]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       500 non-null    object \n",
            " 8   8       500 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n",
            "train[9000:9500]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       500 non-null    object \n",
            " 8   8       500 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n",
            "train[9500:10000]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       500 non-null    float64\n",
            " 1   1       500 non-null    object \n",
            " 2   2       500 non-null    object \n",
            " 3   3       500 non-null    object \n",
            " 4   4       500 non-null    object \n",
            " 5   5       500 non-null    float64\n",
            " 6   6       500 non-null    object \n",
            " 7   7       500 non-null    object \n",
            " 8   8       500 non-null    object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 70.4+ KB\n"
          ]
        }
      ],
      "source": [
        "start_idx = 0\n",
        "end_idx = 0\n",
        "blocks_num = 20\n",
        "block_size = 500\n",
        "\n",
        "df_list = []\n",
        "\n",
        "for block_idx in range(blocks_num):\n",
        "\n",
        "    file_name = f'train[{start_idx}:{start_idx+block_size}]'\n",
        "    print(file_name)\n",
        "    df_list.append(read_tsv(f'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/{file_name}.tsv'))\n",
        "    start_idx += block_size\n",
        "\n",
        "print(len(df_list))\n",
        "\n",
        "for idx in range(len(df_list)):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keKS1-3H0Dv2"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "runtime.unassign()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377,
          "referenced_widgets": [
            "ae267a1e90374fccb2334d5338b9bda2",
            "87855838eb6b496daf6f41d2c4380e1b",
            "b99d8597139643a9b8a14357035c30b2",
            "84ae34ba04eb47efb001444d17d9208c",
            "f481daaa5d404c16aba82dde2a1ea010",
            "07be893e207a4cc8935b919535103f1a",
            "8168e0225eab40a5864b2cd7ee15c04f",
            "203542b28df0430aaafc3d9c6fa91a7a",
            "5547641ba8b8456b96d4892386856b07",
            "f12086296f554cb883d96f8d5c53848e",
            "336c311af77247128d763bed2bf78644",
            "7c43fa28ad4c4049baca903f4616ce40",
            "543646e3f7e140e6908fbdf35be07c36",
            "827c189ccfb64369b4cab8521604b7de",
            "343f0b28433e48f6b4892cbaab4713c3",
            "424da0eedeee4d0e958badb08d2e29a9",
            "be8d440415d14fb39c1662ca326aad7d",
            "0fc0c943783742239ea1a50719e33c7c",
            "4698435907b44a73a2466e0164ae4816",
            "320ecc172af3413c8fc1bf515c0fabd5",
            "402cc7fc63ee46e1916ef318b4eb9afa",
            "bb02c559503d442384b9e8060767f34b",
            "e9119dfa49724cc7887aeda27b87419b",
            "3b970558292542e4ac14edb19fa09d76",
            "ead79d672be445e19b65d5da41c38dde",
            "2e0216464ed5408b88d27df3f26b40a0",
            "ce5f6bbe3d5f4892a8c8ed89741fd8ac",
            "f9a1a293849d48ae97f48b0854e0b9a4",
            "abe6b4e3eedf41d0a83fb8c8f14d00da",
            "31ac9735384d4b47aa6d0eac5efe8b92",
            "a322309165cc4615b7e6cdb1a49bbead",
            "6b3609b4352d4171b782adc440b8b918",
            "079ee46d09504009912320d8bc91bd42",
            "5ccb6ac714fd4e8992cef811ed152c86",
            "e26e3143345f4c2ca6c14c67c6096a7d",
            "4853414ef87740ea9595d2f882513f7d",
            "64d0076183204cf096517704c58e0fdc",
            "8c637f84ca0144a4b74867b712c836cc",
            "9819fea12769418a94e43ceff1c34315",
            "1cdfd7e4c7dc4da486a0a10f08adf1f8",
            "335647d8c4194dd3ad3ed3537d1b0cad",
            "827eeeb190fb48268325eff24d0c0777",
            "d7ef9fc3af3c433695b2943410320177",
            "1ba4ebabe00142e9ab4b6919b8b0abff",
            "69413b37ed4941d0a04946135ba2bfe8",
            "fef7b87d12bb4cfaae1e9c14966df5b7",
            "4b35b046cde1483ea297165c018a0ef4",
            "b5f8550db5c74639a41ca06b78a7d183",
            "cca837a0a9ce433b99c14da4804055fc",
            "48290b59993e4c1ba86b9f08c7eee690",
            "a28cfc6e6d4b408daff391bafb0d12bc",
            "01e88a5b3db74010a5fd150e64051fa1",
            "efeedef7100546febc51aa04c47a2bd3",
            "697192d6142d4de4b904928e9e748dd6",
            "d766eae7cae1403796fe67ec79871d20",
            "81bb1b122c014481be5885b95a05ca74"
          ]
        },
        "id": "VcR53Ff0i-ML",
        "outputId": "f11a249f-538e-4fcc-bf3a-39a5b0082b98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae267a1e90374fccb2334d5338b9bda2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c43fa28ad4c4049baca903f4616ce40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9119dfa49724cc7887aeda27b87419b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ccb6ac714fd4e8992cef811ed152c86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the justification generation on true is going on ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69413b37ed4941d0a04946135ba2bfe8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1284 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the justification generation on false is going on ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81bb1b122c014481be5885b95a05ca74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1284 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/val.tsv.\n"
          ]
        }
      ],
      "source": [
        "predict_and_extend_data_for_EP(ckpt_config, bart_base_config_for_prediction, data,'val')\n",
        "# import math\n",
        "# read_tsv('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/923.tsv')\n",
        "# e = read_tsv('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/test.tsv')\n",
        "\n",
        "# renwed = pd.concat([data, e[7], e[8]], axis=1, ignore_index=True)\n",
        "# saved_data_path = f'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/test.tsv'\n",
        "# write_dataframe_in_tsv(renwed, saved_data_path)\n",
        "# r = read_tsv('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/test.tsv')\n",
        "# r.info()\n",
        "# print(r.iloc[923])\n",
        "# write_dataframe_in_tsv(e.iloc[923], '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/923.tsv')\n",
        "# d = read_tsv('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/923.tsv').T\n",
        "\n",
        "# predict_and_extend_data_for_EP(ckpt_config, bart_base_config_for_prediction, d,'923')\n",
        "# print(r.iloc[703])\n",
        "# read_tsv('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/923.tsv')\n",
        "# for idx in range(len(r)):\n",
        "    # print(f'{idx} : {type(r.iloc[idx][7])}')\n",
        "    # if len(r.iloc[idx][7]) == 0:\n",
        "    #     print(f'[7] : {idx}')\n",
        "\n",
        "    # if len(r.iloc[idx][8]) == 0:\n",
        "    #     print(f'[8] : {idx}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toAk_1qGZ5Ea",
        "outputId": "d335aa9b-0827-49a3-c710-f6119bd1f508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1284 entries, 0 to 1283\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       1284 non-null   int64  \n",
            " 1   1       1284 non-null   object \n",
            " 2   2       1284 non-null   object \n",
            " 3   3       1284 non-null   object \n",
            " 4   4       1284 non-null   object \n",
            " 5   5       1284 non-null   float64\n",
            " 6   6       1284 non-null   object \n",
            " 7   7       1284 non-null   object \n",
            " 8   8       1284 non-null   object \n",
            "dtypes: float64(1), int64(1), object(7)\n",
            "memory usage: 90.4+ KB\n"
          ]
        }
      ],
      "source": [
        "read_tsv('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/val.tsv').info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "61185d80491b498381adef3ad89643f9",
            "42cec859b5134df5a42d2aa943d1df8a",
            "2f963ef10cb649f8b92133c9c39d298c",
            "f43e0b75ac8b40e6919f7caa21d70ebd",
            "3a36e8349425485797581c45903f843a",
            "bb26090f1f694de698538c3bb1b851ba",
            "76b02720217a4c55bdc4d36c3bf228e8",
            "47b9df4fdf44407086a938cb58553a16",
            "d952235a771e4eefbdb4d617b64123f9",
            "24abfe23f6f842869c85c46591ee209b",
            "94c8a110a9ff4a7d92c8d7605612569d",
            "e36228ec34d04d9a867b0ca9580a7b60",
            "b09e5caf7ee244fa8d6f6ddff906183e",
            "0d5896edc4424643b48e4f3b05e0fdf6",
            "3ed4c6a7319a438fbd0fa49c4c5e0107",
            "c1fdd0ee575e429195cbc70ad049e4e8",
            "42f30a52d0644cbd8562b1b8dc7e2f8e",
            "c36df41737f94021aa7d88a0742a4b05",
            "7e4d1354dfe242dc961c26ce7beec1d2",
            "740d322889fe4c4e9d9b7fa933aac879",
            "45c9f350b4504650a9b70e46ce6fa04f",
            "6a3dbc4e7d5249f2adfa15fad5bf1917",
            "2174a3c69d2a4ce29c4e6e85a2774659",
            "1b08b516fa0246288c994a18ec8f05ac",
            "abc50890c49c429fb46674e8d99043e8",
            "2406e250ac914a659cb02b4e3f22ba3e",
            "a5e29470ff2d4774b70a71523e7f8d49",
            "ddb87dd3af8743bdbe3db049bb122bb4",
            "3111ed49c6484415b4bd1ca3487b9b41",
            "80dbb99875f641b78bf12b775d657520",
            "9d135fc2e195444cbda257f153a686e1",
            "14bac1ea51aa4576b2a0309ddb17f2f7",
            "f18b8df6abb94cebaa87b233b49b8e3f",
            "8ace2a8668404f129c1e0b071040446d",
            "3c013906e62a4a059eca6403385fcaa2",
            "0b5235061b3b4493bfe59231bf066e2e",
            "56e65705ece94837b896e345c377a49a",
            "3a4d3f0bb23846daae423d3421c14b6d",
            "112461129b934fb3b66ab01137572d97",
            "3d4bba1a9ccb4f0494568c799674e95e",
            "9317c19186d9494ba5455a4d447fbf9c",
            "2f19258de8c84ec89ce50374def183b0",
            "09b4cb8291d840e0a24eebcb31da1f27",
            "668296031bf54b9ab351d7c9b22107a0"
          ]
        },
        "id": "rAabVgKwvxSP",
        "outputId": "2a8c7853-5868-4718-ac5f-483f96b61f30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generation 0~2 is going on ...\n",
            "the generation of jusitification labeled on true is going on ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61185d80491b498381adef3ad89643f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the generation of jusitification labeled on false is going on ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e36228ec34d04d9a867b0ca9580a7b60",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "concatinated_justification.shape : (2, 9)\n",
            "extended_data.shape : (2, 9)\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/delete this.tsv.\n",
            "generation 2~4 is going on ...\n",
            "the generation of jusitification labeled on true is going on ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2174a3c69d2a4ce29c4e6e85a2774659",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the generation of jusitification labeled on false is going on ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ace2a8668404f129c1e0b071040446d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "concatinated_justification.shape : (5, 9)\n",
            "extended_data.shape : (7, 10)\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/delete this.tsv.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7c322f1c-bf8e-488d-b01a-1fc67209a2ce\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>true</td>\n",
              "      <td>Building a wall on the U.S.-Mexico border will...</td>\n",
              "      <td>immigration rick-perry Governor Texas republic...</td>\n",
              "      <td>Meantime, engineering experts agree the wall w...</td>\n",
              "      <td>0.651049</td>\n",
              "      <td>11972.json</td>\n",
              "      <td>\"literally years. \"The border wall is a long ...</td>\n",
              "      <td>he wants to build the wall, and he has said h...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>false</td>\n",
              "      <td>Wisconsin is on pace to double the number of l...</td>\n",
              "      <td>jobs katrina-shankland State representative Wi...</td>\n",
              "      <td>She cited layoff notices received by the state...</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>11685.json</td>\n",
              "      <td>than 1,000 layoffs in the last three years, a...</td>\n",
              "      <td>more layoffs than any other state in the coun...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>half-true</td>\n",
              "      <td>Suzanne Bonamici supports a plan that will cut...</td>\n",
              "      <td>medicare,message-machine-2012,campaign-adverti...</td>\n",
              "      <td>But spending still goes up. In addition, many ...</td>\n",
              "      <td>0.621429</td>\n",
              "      <td>5209.json</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>McCain has said he has \"nothing to do\" with th...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>not have the power to regulate them. But ther...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>nothing to do with the vets, and he hasnt sai...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>choice for seniors, but it would eliminate th...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c322f1c-bf8e-488d-b01a-1fc67209a2ce')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7c322f1c-bf8e-488d-b01a-1fc67209a2ce button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7c322f1c-bf8e-488d-b01a-1fc67209a2ce');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3b648c92-3871-46aa-b247-5306fff26ed1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3b648c92-3871-46aa-b247-5306fff26ed1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3b648c92-3871-46aa-b247-5306fff26ed1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     0          1                                                  2  \\\n",
              "0  0.0       true  Building a wall on the U.S.-Mexico border will...   \n",
              "1  1.0      false  Wisconsin is on pace to double the number of l...   \n",
              "2  3.0  half-true  Suzanne Bonamici supports a plan that will cut...   \n",
              "3  NaN        NaN                                                NaN   \n",
              "4  NaN        NaN                                                NaN   \n",
              "5  NaN        NaN                                                NaN   \n",
              "6  NaN        NaN                                                NaN   \n",
              "\n",
              "                                                   3  \\\n",
              "0  immigration rick-perry Governor Texas republic...   \n",
              "1  jobs katrina-shankland State representative Wi...   \n",
              "2  medicare,message-machine-2012,campaign-adverti...   \n",
              "3                                                NaN   \n",
              "4                                                NaN   \n",
              "5                                                NaN   \n",
              "6                                                NaN   \n",
              "\n",
              "                                                   4         5           6  \\\n",
              "0  Meantime, engineering experts agree the wall w...  0.651049  11972.json   \n",
              "1  She cited layoff notices received by the state...  0.800000  11685.json   \n",
              "2  But spending still goes up. In addition, many ...  0.621429   5209.json   \n",
              "3                                                NaN       NaN         NaN   \n",
              "4                                                NaN       NaN         NaN   \n",
              "5                                                NaN       NaN         NaN   \n",
              "6                                                NaN       NaN         NaN   \n",
              "\n",
              "                                                   7  \\\n",
              "0   \"literally years. \"The border wall is a long ...   \n",
              "1   than 1,000 layoffs in the last three years, a...   \n",
              "2                                                NaN   \n",
              "3  McCain has said he has \"nothing to do\" with th...   \n",
              "4   not have the power to regulate them. But ther...   \n",
              "5   nothing to do with the vets, and he hasnt sai...   \n",
              "6   choice for seniors, but it would eliminate th...   \n",
              "\n",
              "                                                   8    9  \n",
              "0   he wants to build the wall, and he has said h...  NaN  \n",
              "1   more layoffs than any other state in the coun...  NaN  \n",
              "2                                                NaN  3.0  \n",
              "3                                                NaN  NaN  \n",
              "4                                                NaN  NaN  \n",
              "5                                                NaN  NaN  \n",
              "6                                                NaN  NaN  "
            ]
          },
          "execution_count": 506,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "extended_data = pd.DataFrame()\n",
        "\n",
        "start_idx = 0\n",
        "end_idx = 0\n",
        "blocks_num = 2\n",
        "block_size = 2\n",
        "\n",
        "for df_block_idx in range(blocks_num) :\n",
        "    # print(f'start_idx : {start_idx}')\n",
        "    if df_block_idx == blocks_num :\n",
        "        print(f\"inside if\")\n",
        "        break\n",
        "    else :\n",
        "        end_idx = block_size * (df_block_idx+1)\n",
        "\n",
        "\n",
        "    # if df_block_idx == 0 :\n",
        "    #     end_idx = block_size\n",
        "    # elif df_block\n",
        "    # else ;\n",
        "    #     end_idx = block_size * df_block_idx\n",
        "    print(f'generation {start_idx}~{end_idx} is going on ...')\n",
        "\n",
        "    extended_data = predict_and_extend_data_for_EP(ckpt_config, bart_base_config_for_prediction, data[start_idx:end_idx], extended_data, 'delete this')\n",
        "    start_idx += block_size\n",
        "\n",
        "\n",
        "read_tsv('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/delete this.tsv')\n",
        "\n",
        "\n",
        "# predict_and_extend_data_for_EP(ckpt_config, bart_base_config_for_prediction, data[:3], extended_data, 'delete this')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "v5h1PNI0Zr7v",
        "outputId": "dd90f26b-e737-4685-b24a-06f8db409dfe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2e6a5041-d51e-46f4-b17c-4bfbc7f69e0f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>than 1,000 layoffs in the last three years, a...</td>\n",
              "      <td>more layoffs than any other state in the coun...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e6a5041-d51e-46f4-b17c-4bfbc7f69e0f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2e6a5041-d51e-46f4-b17c-4bfbc7f69e0f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2e6a5041-d51e-46f4-b17c-4bfbc7f69e0f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    0   1   2   3   4   5   6  \\\n",
              "0 NaN NaN NaN NaN NaN NaN NaN   \n",
              "\n",
              "                                                   7  \\\n",
              "0   than 1,000 layoffs in the last three years, a...   \n",
              "\n",
              "                                                   8  \n",
              "0   more layoffs than any other state in the coun...  "
            ]
          },
          "execution_count": 486,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "read_tsv('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/delete this.tsv')\n",
        "\n",
        "# block[:3]は0~2の3つ\n",
        "# block[1:3]は1~2の2つ\n",
        "# block[j:k]はj~k-1のk-1個"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caHiWCn20yEI"
      },
      "outputs": [],
      "source": [
        "# pd.concat([data[:5], true_data[7], false_data[7]], axis=1, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gusISOBh6z6_"
      },
      "outputs": [],
      "source": [
        "# read_tsv('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/delete this.tsv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN7vcmrFlXkJ"
      },
      "source": [
        "# 学習・推論を行うコード"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAg8ZtHUbfBc"
      },
      "source": [
        "## 通しで学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIjbxdtgt-9A"
      },
      "outputs": [],
      "source": [
        "main_train_data_true = read_tsv(train_and_test_config_true['TRAINING_DATA_PATH'])\n",
        "main_test_data_true = read_tsv(train_and_test_config_true['TEST_DATA_PATH'])\n",
        "main_val_data_true = read_tsv(train_and_test_config_true['VALIDATION_DATA_PATH'])\n",
        "main_train_data_false = read_tsv(train_and_test_config_false['TRAINING_DATA_PATH'])\n",
        "main_test_data_false = read_tsv(train_and_test_config_false['TEST_DATA_PATH'])\n",
        "main_val_data_false = read_tsv(train_and_test_config_false['VALIDATION_DATA_PATH'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mcrYbAbaE0J"
      },
      "outputs": [],
      "source": [
        "# base_time = get_base_time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6V8_2bLbnzg"
      },
      "outputs": [],
      "source": [
        "base_time = '2024-02-01 17:37'\n",
        "\n",
        "# # gpt2-small\n",
        "# # true\n",
        "# for batch_size in [2, 4]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_small_config,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='true')\n",
        "# # false\n",
        "# for batch_size in [2, 4]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_small_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='false')\n",
        "\n",
        "# # gpt2-medium\n",
        "# # true\n",
        "# for batch_size in [2, 4]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_medium_config,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='true')\n",
        "# # false\n",
        "# for batch_size in [2, 4]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_medium_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='false')\n",
        "\n",
        "# t5-small\n",
        "# # true\n",
        "# for batch_size in [2, 4]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=t5_small_config,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='true')\n",
        "# # false\n",
        "# for batch_size in [2, 4]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=t5_small_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='false')\n",
        "\n",
        "# t5-base\n",
        "# true\n",
        "# for batch_size in [2, 4]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=t5_base_config,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='true')\n",
        "# # false\n",
        "# for batch_size in [2, 4]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=t5_base_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='false')\n",
        "\n",
        "# # bart-base\n",
        "\n",
        "# for batch_size in [2, 4]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=bart_base_config,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='true')\n",
        "\n",
        "# for batch_size in [2, 4]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=bart_base_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='false')\n",
        "\n",
        "# from time import sleep\n",
        "# sleep(120)\n",
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV9QBB8xZUVd"
      },
      "source": [
        "## ちまちま学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bd46fWbrxCq"
      },
      "outputs": [],
      "source": [
        "# base_time = get_base_time()\n",
        "# # print(base_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBv4FL5Zlp09"
      },
      "outputs": [],
      "source": [
        "# # flip\n",
        "# base_time = 'delete this flip'\n",
        "# gpt2_small_config1 = {\n",
        "#     'MODEL_NAME' : 'gpt2',\n",
        "#     'TOKENIZER_NAME' : 'gpt2',\n",
        "#     'MAX_LENGTH' : 512,\n",
        "#     'TRAINING_BATCH_SIZE' : 16,\n",
        "#     'VALIDATION_BATCH_SIZE' : 16,\n",
        "#     'TEST_BATCH_SIZE' : 1,\n",
        "#     'TRAINING_EPOCHS' : 2,\n",
        "#     'SAVE_TOP_K' : 1,\n",
        "#     'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "#     'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "#     'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "#     'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "#     'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "#     'LR' : 7e-5,\n",
        "#     'RANDOM_SEED' : 42,\n",
        "#     'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "#     'COMPONENT_NAME' : 'CEG',\n",
        "#     'PROMPT_COMPONENTS' : ['statement','metadata','justification'],\n",
        "#     'TensorBoardLogger_NAME' : 'gpt2/small',\n",
        "#     'MODEL_FOLDER' : ['gpt2', 'small'],\n",
        "#     'INPUT_FLIP' : 1\n",
        "\n",
        "# }\n",
        "\n",
        "# # GPT2-medium\n",
        "# gpt2_medium_config1 = {\n",
        "#     'MODEL_NAME' : 'gpt2-medium',\n",
        "#     'TOKENIZER_NAME' : 'gpt2-medium',\n",
        "#     'MAX_LENGTH' : 512,\n",
        "#     'TRAINING_BATCH_SIZE' : 16,\n",
        "#     'VALIDATION_BATCH_SIZE' : 16,\n",
        "#     'TEST_BATCH_SIZE' : 1,\n",
        "#     'TRAINING_EPOCHS' : 2,\n",
        "#     'SAVE_TOP_K' : 1,\n",
        "#     'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "#     'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "#     'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "#     'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "#     'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "#     'LR' : 7e-5,\n",
        "#     'RANDOM_SEED' : 42,\n",
        "#     'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "#     'COMPONENT_NAME' : 'CEG',\n",
        "#     'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "#     'TensorBoardLogger_NAME' : 'gpt2/medium',\n",
        "#     'MODEL_FOLDER' : ['gpt2', 'medium'],\n",
        "#     'INPUT_FLIP' : 1\n",
        "# }\n",
        "# # gpt2-small\n",
        "# # true\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_small_config1,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='true')\n",
        "# # # false\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_small_config1,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='false')\n",
        "\n",
        "# # gpt2-medium\n",
        "# # true\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_medium_config1,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='true')\n",
        "# # # false\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_medium_config1,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='false')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onVArZXnoOtX"
      },
      "outputs": [],
      "source": [
        "# statementのみ\n",
        "# GPT2-small\n",
        "# base_time = 'delete this s only'\n",
        "\n",
        "# gpt2_small_config2 = {\n",
        "#     'MODEL_NAME' : 'gpt2',\n",
        "#     'TOKENIZER_NAME' : 'gpt2',\n",
        "#     'MAX_LENGTH' : 512,\n",
        "#     'TRAINING_BATCH_SIZE' : 16,\n",
        "#     'VALIDATION_BATCH_SIZE' : 16,\n",
        "#     'TEST_BATCH_SIZE' : 1,\n",
        "#     'TRAINING_EPOCHS' : 2,\n",
        "#     'SAVE_TOP_K' : 1,\n",
        "#     'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "#     'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "#     'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "#     'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "#     'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "#     'LR' : 7e-5,\n",
        "#     'RANDOM_SEED' : 42,\n",
        "#     'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "#     'COMPONENT_NAME' : 'CEG',\n",
        "#     'PROMPT_COMPONENTS' : ['statement', 'justification'],\n",
        "#     'TensorBoardLogger_NAME' : 'gpt2/small',\n",
        "#     'MODEL_FOLDER' : ['gpt2', 'small'],\n",
        "#     'INPUT_FLIP' : 0\n",
        "\n",
        "# }\n",
        "\n",
        "# # GPT2-medium\n",
        "# gpt2_medium_config2 = {\n",
        "#     'MODEL_NAME' : 'gpt2-medium',\n",
        "#     'TOKENIZER_NAME' : 'gpt2-medium',\n",
        "#     'MAX_LENGTH' : 512,\n",
        "#     'TRAINING_BATCH_SIZE' : 16,\n",
        "#     'VALIDATION_BATCH_SIZE' : 16,\n",
        "#     'TEST_BATCH_SIZE' : 1,\n",
        "#     'TRAINING_EPOCHS' : 2,\n",
        "#     'SAVE_TOP_K' : 1,\n",
        "#     'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "#     'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "#     'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "#     'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "#     'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "#     'LR' : 7e-5,\n",
        "#     'RANDOM_SEED' : 42,\n",
        "#     'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "#     'COMPONENT_NAME' : 'CEG',\n",
        "#     'PROMPT_COMPONENTS' : ['statement', 'justification'],\n",
        "#     'TensorBoardLogger_NAME' : 'gpt2/medium',\n",
        "#     'MODEL_FOLDER' : ['gpt2', 'medium'],\n",
        "#     'INPUT_FLIP' : 0\n",
        "# }\n",
        "\n",
        "# gpt2-small\n",
        "# true\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_small_config2,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='true')\n",
        "# # # false\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_small_config2,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='false')\n",
        "\n",
        "# gpt2-medium\n",
        "# # # true\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_medium_config2,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='true')\n",
        "# # # false\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_medium_config2,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='false')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrkUASPId8Yp"
      },
      "source": [
        "## 通しで推論"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_nePVdlkeOz"
      },
      "outputs": [],
      "source": [
        "# データのロード\n",
        "train_data_true = read_tsv(train_and_test_config_true[\"TRAINING_DATA_PATH\"])\n",
        "train_data_false = read_tsv(train_and_test_config_false[\"TRAINING_DATA_PATH\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlGa761SkEK5"
      },
      "outputs": [],
      "source": [
        "# config_dict\n",
        "time = '2024-02-01 17:37'\n",
        "ckpt_dict_true = {\n",
        "    'gpt2-small-bs2' : f'{time}/true_batch_size=2',\n",
        "    'gpt2-small-bs4' : f'{time}/true_batch_size=4',\n",
        "    'gpt2-medium-bs2' : f'{time}/true_batch_size=2',\n",
        "    'gpt2-medium-bs4' : f'{time}/true_batch_size=4',\n",
        "    't5-small-bs2' : f'{time}/true_batch_size=2',\n",
        "    't5-small-bs4' : f'{time}/true_batch_size=4',\n",
        "    't5-base-bs2' : f'{time}/true_batch_size=2',\n",
        "    't5-base-bs4' : f'{time}/true_batch_size=4',\n",
        "    'bart-base-bs2' : f'{time}/true_batch_size=2',\n",
        "    'bart-base-bs4' : f'{time}/true_batch_size=4',\n",
        "}\n",
        "\n",
        "ckpt_dict_false = {\n",
        "    'gpt2-small-bs2' : f'{time}/false_batch_size=2',\n",
        "    'gpt2-small-bs4' : f'{time}/false_batch_size=4',\n",
        "    'gpt2-medium-bs2' : f'{time}/false_batch_size=2',\n",
        "    'gpt2-medium-bs4' : f'{time}/false_batch_size=4',\n",
        "    't5-small-bs2' : f'{time}/false_batch_size=2',\n",
        "    't5-small-bs4' : f'{time}/false_batch_size=4',\n",
        "    't5-base-bs2' : f'{time}/false_batch_size=2',\n",
        "    't5-base-bs4' : f'{time}/false_batch_size=4',\n",
        "    'bart-base-bs2' : f'{time}/false_batch_size=2',\n",
        "    'bart-base-bs4' : f'{time}/false_batch_size=4',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPOmjg-RhqIl"
      },
      "outputs": [],
      "source": [
        "time = '2024-02-01 17:37'\n",
        "\n",
        "# # 推論の実行\n",
        "# true_gpt2_bs2        = predict_CEG(gpt2_small_config_for_prediction, 'gpt2', train_data_true[:100], ckpt_dict_true['gpt2-small-bs2'], 'train', 'true', 1, time)\n",
        "# true_gpt2_medium_bs2 = predict_CEG(gpt2_medium_config_for_prediction, 'gpt2-medium', train_data_true[:100], ckpt_dict_true['gpt2-medium-bs2'], 'train', 'true', 1, time)\n",
        "# true_t5_small_bs2    = predict_CEG(t5_small_config_for_prediction, 't5-small', train_data_true[:100], ckpt_dict_true['t5-small-bs2'], 'train', 'true', 1, time)\n",
        "# true_t5_base_bs2     = predict_CEG(t5_base_config_for_prediction, 't5-base', train_data_true[:100], ckpt_dict_true['t5-base-bs2'], 'train', 'true', 1, time)\n",
        "# true_bart_base_bs2   = predict_CEG(bart_base_config_for_prediction, 'facebook/bart-base', train_data_true[:100], ckpt_dict_true['bart-base-bs2'], 'train', 'true', 1, time)\n",
        "\n",
        "# true_gpt2_bs4        = predict_CEG(gpt2_small_config_for_prediction, 'gpt2', train_data_true[:100], ckpt_dict_true['gpt2-small-bs4'], 'train', 'true', 1, time)\n",
        "# true_gpt2_medium_bs4 = predict_CEG(gpt2_medium_config_for_prediction, 'gpt2-medium', train_data_true[:100], ckpt_dict_true['gpt2-medium-bs4'], 'train', 'true', 1, time)\n",
        "# true_t5_small_bs4    = predict_CEG(t5_small_config_for_prediction, 't5-small', train_data_true[:100], ckpt_dict_true['t5-small-bs4'], 'train', 'true', 1, time)\n",
        "# true_t5_base_bs4     = predict_CEG(t5_base_config_for_prediction, 't5-base', train_data_true[:100], ckpt_dict_true['t5-base-bs4'], 'train', 'true', 1, time)\n",
        "# true_bart_base_bs4   = predict_CEG(bart_base_config_for_prediction, 'facebook/bart-base', train_data_true[:100], ckpt_dict_true['bart-base-bs4'], 'train', 'true', 1, time)\n",
        "\n",
        "# false_gpt2_bs2        = predict_CEG(gpt2_small_config_for_prediction, 'gpt2', train_data_false[:100], ckpt_dict_false['gpt2-small-bs2'], 'train', 'false', 1, time)\n",
        "# false_gpt2_medium_bs2 = predict_CEG(gpt2_medium_config_for_prediction, 'gpt2-medium', train_data_false[:100], ckpt_dict_false['gpt2-medium-bs2'], 'train', 'false', 1, time)\n",
        "# false_t5_small_bs2    = predict_CEG(t5_small_config_for_prediction, 't5-small', train_data_false[:100], ckpt_dict_false['t5-small-bs2'], 'train', 'false', 1, time)\n",
        "# false_t5_base_bs2     = predict_CEG(t5_base_config_for_prediction, 't5-base', train_data_false[:100], ckpt_dict_false['t5-base-bs2'], 'train', 'false', 1, time)\n",
        "# false_bart_base_bs2   = predict_CEG(bart_base_config_for_prediction, 'facebook/bart-base', train_data_false[:100], ckpt_dict_false['bart-base-bs2'], 'train', 'false', 1, time)\n",
        "\n",
        "# false_gpt2_bs4        = predict_CEG(gpt2_small_config_for_prediction, 'gpt2', train_data_false[:100], ckpt_dict_false['gpt2-small-bs4'], 'train', 'false', 1, time)\n",
        "# false_gpt2_medium_bs4 = predict_CEG(gpt2_medium_config_for_prediction, 'gpt2-medium', train_data_false[:100], ckpt_dict_false['gpt2-medium-bs4'], 'train', 'false', 1, time)\n",
        "# false_t5_small_bs4    = predict_CEG(t5_small_config_for_prediction, 't5-small', train_data_false[:100], ckpt_dict_false['t5-small-bs4'], 'train', 'false', 1, time)\n",
        "# false_t5_base_bs4     = predict_CEG(t5_base_config_for_prediction, 't5-base', train_data_false[:100], ckpt_dict_false['t5-base-bs4'], 'train', 'false', 1, time)\n",
        "# false_bart_base_bs4   = predict_CEG(bart_base_config_for_prediction, 'facebook/bart-base', train_data_false[:100], ckpt_dict_false['bart-base-bs4'], 'train', 'false', 1, time)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "702E7sGAkbyH"
      },
      "source": [
        "## ちまちま推論\n",
        "- 23: gpt2 入力入れ替え, gpt2 statementのみ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2XbM_aDFj-9"
      },
      "source": [
        "# BERTScoreの計算"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtOkoGpvwfpQ"
      },
      "outputs": [],
      "source": [
        "# # データのロード\n",
        "# from google.colab import runtime\n",
        "\n",
        "# runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsWOWpiSwH7H",
        "outputId": "c758f4c4-bbcf-476f-dc70-c7edb509b097"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gpt2_true_batch_size=2_true\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "gpt2_true_batch_size=4_true\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "gpt2_false_batch_size=2_false\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "gpt2_false_batch_size=4_false\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "gpt2-medium_true_batch_size=2_true\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "gpt2-medium_true_batch_size=4_true\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "gpt2-medium_false_batch_size=2_false\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "gpt2-medium_false_batch_size=4_false\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "t5-small_true_batch_size=2_true\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "t5-small_true_batch_size=4_true\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "t5-small_false_batch_size=2_false\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "t5-small_false_batch_size=4_false\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "t5-base_true_batch_size=2_true\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "t5-base_true_batch_size=4_true\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "t5-base_false_batch_size=2_false\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "t5-base_false_batch_size=4_false\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "facebook-bart-base_true_batch_size=2_true\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "facebook-bart-base_true_batch_size=4_true\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "facebook-bart-base_false_batch_size=2_false\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n",
            "facebook-bart-base_false_batch_size=4_false\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       100 non-null    float64\n",
            " 1   1       100 non-null    object \n",
            " 2   2       100 non-null    object \n",
            " 3   3       100 non-null    object \n",
            " 4   4       100 non-null    object \n",
            " 5   5       100 non-null    float64\n",
            " 6   6       100 non-null    object \n",
            " 7   7       100 non-null    object \n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 6.4+ KB\n"
          ]
        }
      ],
      "source": [
        "time = '2024-02-01 17:37'\n",
        "base_path = f'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/{time}'\n",
        "\n",
        "model_names_list = ['gpt2', 'gpt2-medium', 't5-small', 't5-base', 'facebook-bart-base']\n",
        "train_types_list = ['true_batch_size=2_true', 'true_batch_size=4_true', 'false_batch_size=2_false', 'false_batch_size=4_false']\n",
        "\n",
        "# data_dictを作る\n",
        "data_dict = {}\n",
        "\n",
        "for model_name in range(len(model_names_list)):\n",
        "    for train_type in range(len(train_types_list)):\n",
        "\n",
        "        dict_key = f\"{model_names_list[model_name]}_{train_types_list[train_type]}\"\n",
        "        print(f\"{base_path}/{model_names_list[model_name]}_{time}_{train_types_list[train_type]}.tsv\")\n",
        "        # data_dict[dict_key] = read_tsv(f\"{base_path}/{model_names_list[model_name]}_{time}_{train_types_list[train_type]}.tsv\").fillna('')\n",
        "        print(dict_key)\n",
        "        data_dict[dict_key].info()\n",
        "# p, r, f = get_bertscore_rank(time, BERTScore_config, data_dict)\n",
        "\n",
        "# data_dict = {'gpt2_true_batch_size=2': 'ok',\n",
        "#  'gpt2_true_batch_size=4': 'ok',\n",
        "#  'gpt2_false_batch_size=2': 'ok',\n",
        "#  'gpt2_false_batch_size=4': 'ok',\n",
        "#  'gpt2-medium_true_batch_size=2': 'ok',\n",
        "#  'gpt2-medium_true_batch_size=4': 'ok',\n",
        "#  'gpt2-medium_false_batch_size=2': 'ok',\n",
        "#  'gpt2-medium_false_batch_size=4': 'ok',\n",
        "#  't5-small_true_batch_size=2': 'ok',\n",
        "#  't5-small_true_batch_size=4': 'ok',\n",
        "#  't5-small_false_batch_size=2': 'ok',\n",
        "#  't5-small_false_batch_size=4': 'ok',\n",
        "#  't5-base_true_batch_size=2': 'ok',\n",
        "#  't5-base_true_batch_size=4': 'ok',\n",
        "#  't5-base_false_batch_size=2': 'ok',\n",
        "#  't5-base_false_batch_size=4': 'ok',\n",
        "#  'facebook-bart-base_true_batch_size=2': 'ok',\n",
        "#  'facebook-bart-base_true_batch_size=4': 'ok',\n",
        "#  'facebook-bart-base_false_batch_size=2': 'ok',\n",
        "#  'facebook-bart-base_false_batch_size=4': 'ok'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBvK-XnrwyGM",
        "outputId": "a737a410-3b42-4b13-a0d5-a64eed5eee0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : gpt2_true_batch_size=2_true\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:45<00:00,  1.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : gpt2_true_batch_size=4_true\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:45<00:00,  1.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : gpt2_false_batch_size=2_false\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:45<00:00,  1.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : gpt2_false_batch_size=4_false\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:45<00:00,  1.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : gpt2-medium_true_batch_size=2_true\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:44<00:00,  1.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : gpt2-medium_true_batch_size=4_true\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:44<00:00,  1.04s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : gpt2-medium_false_batch_size=2_false\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:44<00:00,  1.04s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : gpt2-medium_false_batch_size=4_false\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:44<00:00,  1.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : t5-small_true_batch_size=2_true\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 1/100 [00:02<03:39,  2.22s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            "  3%|▎         | 3/100 [00:04<02:05,  1.30s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            "  6%|▌         | 6/100 [00:07<01:43,  1.10s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            "  8%|▊         | 8/100 [00:09<01:37,  1.06s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 10%|█         | 10/100 [00:11<01:34,  1.05s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 11%|█         | 11/100 [00:12<01:32,  1.04s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 12%|█▏        | 12/100 [00:13<01:31,  1.04s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 20%|██        | 20/100 [00:21<01:22,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 21%|██        | 21/100 [00:22<01:21,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 23%|██▎       | 23/100 [00:24<01:19,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 24%|██▍       | 24/100 [00:25<01:18,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 28%|██▊       | 28/100 [00:30<01:13,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 29%|██▉       | 29/100 [00:31<01:13,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 31%|███       | 31/100 [00:33<01:10,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 35%|███▌      | 35/100 [00:37<01:06,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 36%|███▌      | 36/100 [00:38<01:05,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 42%|████▏     | 42/100 [00:44<00:59,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 44%|████▍     | 44/100 [00:46<00:57,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 46%|████▌     | 46/100 [00:48<00:55,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 48%|████▊     | 48/100 [00:50<00:53,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 56%|█████▌    | 56/100 [00:58<00:45,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 58%|█████▊    | 58/100 [01:00<00:43,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 59%|█████▉    | 59/100 [01:01<00:42,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 64%|██████▍   | 64/100 [01:07<00:37,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 65%|██████▌   | 65/100 [01:08<00:36,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 67%|██████▋   | 67/100 [01:10<00:33,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 76%|███████▌  | 76/100 [01:19<00:24,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 78%|███████▊  | 78/100 [01:21<00:22,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 84%|████████▍ | 84/100 [01:27<00:16,  1.04s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 85%|████████▌ | 85/100 [01:28<00:15,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 86%|████████▌ | 86/100 [01:29<00:14,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 87%|████████▋ | 87/100 [01:30<00:13,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 88%|████████▊ | 88/100 [01:31<00:12,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 90%|█████████ | 90/100 [01:33<00:10,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 91%|█████████ | 91/100 [01:34<00:09,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 96%|█████████▌| 96/100 [01:40<00:04,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 98%|█████████▊| 98/100 [01:42<00:02,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 99%|█████████▉| 99/100 [01:43<00:01,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            "100%|██████████| 100/100 [01:44<00:00,  1.04s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : t5-small_true_batch_size=4_true\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 8/100 [00:09<01:37,  1.06s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 10%|█         | 10/100 [00:11<01:34,  1.05s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 11%|█         | 11/100 [00:12<01:32,  1.04s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 16%|█▌        | 16/100 [00:17<01:26,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 25%|██▌       | 25/100 [00:26<01:17,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 27%|██▋       | 27/100 [00:28<01:15,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 28%|██▊       | 28/100 [00:30<01:14,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 29%|██▉       | 29/100 [00:31<01:13,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 30%|███       | 30/100 [00:32<01:11,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 35%|███▌      | 35/100 [00:37<01:06,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 36%|███▌      | 36/100 [00:38<01:05,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 39%|███▉      | 39/100 [00:41<01:02,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 42%|████▏     | 42/100 [00:44<00:59,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 52%|█████▏    | 52/100 [00:54<00:49,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 59%|█████▉    | 59/100 [01:01<00:42,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 63%|██████▎   | 63/100 [01:06<00:38,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 65%|██████▌   | 65/100 [01:08<00:36,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 66%|██████▌   | 66/100 [01:09<00:35,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 67%|██████▋   | 67/100 [01:10<00:33,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 78%|███████▊  | 78/100 [01:21<00:22,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 85%|████████▌ | 85/100 [01:28<00:15,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 86%|████████▌ | 86/100 [01:29<00:14,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 96%|█████████▌| 96/100 [01:40<00:04,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 97%|█████████▋| 97/100 [01:41<00:03,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 99%|█████████▉| 99/100 [01:43<00:01,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            "100%|██████████| 100/100 [01:44<00:00,  1.04s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : t5-small_false_batch_size=2_false\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 2/100 [00:03<02:28,  1.52s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            "  3%|▎         | 3/100 [00:04<02:05,  1.29s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 11%|█         | 11/100 [00:12<01:32,  1.04s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 16%|█▌        | 16/100 [00:17<01:26,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 17%|█▋        | 17/100 [00:18<01:25,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 18%|█▊        | 18/100 [00:19<01:24,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 19%|█▉        | 19/100 [00:20<01:23,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 22%|██▏       | 22/100 [00:23<01:20,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 24%|██▍       | 24/100 [00:25<01:18,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 25%|██▌       | 25/100 [00:26<01:17,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 27%|██▋       | 27/100 [00:28<01:15,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 32%|███▏      | 32/100 [00:34<01:11,  1.05s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 33%|███▎      | 33/100 [00:35<01:10,  1.05s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 34%|███▍      | 34/100 [00:36<01:08,  1.04s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 36%|███▌      | 36/100 [00:38<01:06,  1.04s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 38%|███▊      | 38/100 [00:40<01:04,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 39%|███▉      | 39/100 [00:41<01:02,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 43%|████▎     | 43/100 [00:45<00:58,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 45%|████▌     | 45/100 [00:47<00:56,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 52%|█████▏    | 52/100 [00:54<00:49,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 53%|█████▎    | 53/100 [00:55<00:48,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 55%|█████▌    | 55/100 [00:57<00:46,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 57%|█████▋    | 57/100 [01:00<00:44,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 59%|█████▉    | 59/100 [01:02<00:42,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 60%|██████    | 60/100 [01:03<00:41,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 62%|██████▏   | 62/100 [01:05<00:39,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 63%|██████▎   | 63/100 [01:06<00:38,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 65%|██████▌   | 65/100 [01:08<00:36,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 70%|███████   | 70/100 [01:13<00:30,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 74%|███████▍  | 74/100 [01:17<00:26,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 80%|████████  | 80/100 [01:23<00:20,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 84%|████████▍ | 84/100 [01:27<00:16,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 86%|████████▌ | 86/100 [01:29<00:14,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 87%|████████▋ | 87/100 [01:30<00:13,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 88%|████████▊ | 88/100 [01:31<00:12,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 89%|████████▉ | 89/100 [01:32<00:11,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 90%|█████████ | 90/100 [01:33<00:10,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 92%|█████████▏| 92/100 [01:36<00:08,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 94%|█████████▍| 94/100 [01:38<00:06,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 95%|█████████▌| 95/100 [01:39<00:05,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 97%|█████████▋| 97/100 [01:41<00:03,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            "100%|██████████| 100/100 [01:44<00:00,  1.04s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : t5-small_false_batch_size=4_false\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 7/100 [00:08<01:40,  1.08s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 14%|█▍        | 14/100 [00:15<01:29,  1.04s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 16%|█▌        | 16/100 [00:17<01:26,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 18%|█▊        | 18/100 [00:19<01:24,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 24%|██▍       | 24/100 [00:25<01:18,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 31%|███       | 31/100 [00:33<01:11,  1.04s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 33%|███▎      | 33/100 [00:35<01:10,  1.05s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 36%|███▌      | 36/100 [00:38<01:06,  1.04s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 39%|███▉      | 39/100 [00:41<01:02,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 41%|████      | 41/100 [00:43<01:00,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 43%|████▎     | 43/100 [00:45<00:58,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 52%|█████▏    | 52/100 [00:54<00:49,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 53%|█████▎    | 53/100 [00:55<00:48,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 59%|█████▉    | 59/100 [01:02<00:42,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 60%|██████    | 60/100 [01:03<00:41,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 61%|██████    | 61/100 [01:04<00:40,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 63%|██████▎   | 63/100 [01:06<00:38,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 65%|██████▌   | 65/100 [01:08<00:36,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 70%|███████   | 70/100 [01:13<00:30,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 74%|███████▍  | 74/100 [01:17<00:26,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 76%|███████▌  | 76/100 [01:19<00:24,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 81%|████████  | 81/100 [01:24<00:19,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 83%|████████▎ | 83/100 [01:26<00:17,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 86%|████████▌ | 86/100 [01:29<00:14,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 88%|████████▊ | 88/100 [01:31<00:12,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 92%|█████████▏| 92/100 [01:36<00:08,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 95%|█████████▌| 95/100 [01:39<00:05,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            "100%|██████████| 100/100 [01:44<00:00,  1.04s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : t5-base_true_batch_size=2_true\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 6/100 [00:07<01:43,  1.10s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 10%|█         | 10/100 [00:11<01:34,  1.05s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 11%|█         | 11/100 [00:12<01:32,  1.04s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 26%|██▌       | 26/100 [00:27<01:16,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 41%|████      | 41/100 [00:43<01:00,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 46%|████▌     | 46/100 [00:48<00:55,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 61%|██████    | 61/100 [01:04<00:40,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 65%|██████▌   | 65/100 [01:08<00:36,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 67%|██████▋   | 67/100 [01:10<00:33,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 78%|███████▊  | 78/100 [01:21<00:22,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 84%|████████▍ | 84/100 [01:27<00:16,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 85%|████████▌ | 85/100 [01:28<00:15,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            "100%|██████████| 100/100 [01:44<00:00,  1.04s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : t5-base_true_batch_size=4_true\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 10/100 [00:11<01:34,  1.05s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 36%|███▌      | 36/100 [00:38<01:05,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 52%|█████▏    | 52/100 [00:54<00:49,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 59%|█████▉    | 59/100 [01:02<00:42,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 65%|██████▌   | 65/100 [01:08<00:36,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 78%|███████▊  | 78/100 [01:21<00:22,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 79%|███████▉  | 79/100 [01:22<00:21,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 83%|████████▎ | 83/100 [01:26<00:17,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 84%|████████▍ | 84/100 [01:27<00:16,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 86%|████████▌ | 86/100 [01:29<00:14,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 99%|█████████▉| 99/100 [01:43<00:01,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            "100%|██████████| 100/100 [01:44<00:00,  1.04s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : t5-base_false_batch_size=2_false\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 17/100 [00:18<01:25,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 29%|██▉       | 29/100 [00:31<01:13,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 37%|███▋      | 37/100 [00:39<01:05,  1.04s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 51%|█████     | 51/100 [00:53<00:50,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 52%|█████▏    | 52/100 [00:54<00:49,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 54%|█████▍    | 54/100 [00:56<00:47,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 56%|█████▌    | 56/100 [00:59<00:45,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 65%|██████▌   | 65/100 [01:08<00:36,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 74%|███████▍  | 74/100 [01:17<00:26,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 87%|████████▋ | 87/100 [01:30<00:13,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 91%|█████████ | 91/100 [01:35<00:09,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 95%|█████████▌| 95/100 [01:39<00:05,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 96%|█████████▌| 96/100 [01:40<00:04,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 97%|█████████▋| 97/100 [01:41<00:03,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            "100%|██████████| 100/100 [01:44<00:00,  1.04s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : t5-base_false_batch_size=4_false\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 18/100 [00:19<01:24,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 51%|█████     | 51/100 [00:53<00:50,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            " 67%|██████▋   | 67/100 [01:10<00:33,  1.03s/it]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
            "100%|██████████| 100/100 [01:44<00:00,  1.04s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : facebook-bart-base_true_batch_size=2_true\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:45<00:00,  1.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : facebook-bart-base_true_batch_size=4_true\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:45<00:00,  1.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : facebook-bart-base_false_batch_size=2_false\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:45<00:00,  1.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name : facebook-bart-base_false_batch_size=4_false\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:45<00:00,  1.06s/it]\n"
          ]
        }
      ],
      "source": [
        "# p, r, f = get_bertscore_rank(time, BERTScore_config, data_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtS3pqdooRkS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytM25NO0wIf2"
      },
      "outputs": [],
      "source": [
        "p, r, f = get_bertscore_rank('delete this', BERTScore_config, data_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIXqI-eiksB8"
      },
      "outputs": [],
      "source": [
        "# # データのロード\n",
        "# train_data_true = read_tsv(train_and_test_config_true[\"TRAINING_DATA_PATH\"])\n",
        "# train_data_false = read_tsv(train_and_test_config_false[\"TRAINING_DATA_PATH\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEDlN_9SksCZ"
      },
      "outputs": [],
      "source": [
        "# base_time = get_base_time()\n",
        "# print(base_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6d9fc398f7294169914868322dc32f47",
            "b27de78c01a342b3b8725b709579e605",
            "8cff50a7fd434bc19aef755138d44945",
            "795e77db32c74a9da820a0675bec2c6d",
            "e10e410e9189448096cd62fc7c247aa5",
            "51f982844a3d4b6397a039941d6a054e",
            "f9bb6dae3d604c378b355a49edd628d9",
            "3f46711e2165438fa51ce5b68adea86d",
            "356902efe1294e78831a8e52e5010d49",
            "578ef8349a7148e6ba9cd337159167fd",
            "c7ab92038df64177b69ed09cf12d9fff",
            "f322c82941cf416fbca58ef3f9081876",
            "9383b399472d4672a9f0fc571a905647",
            "403d9cb7cb0343169f4438737b7b2faa",
            "b661c05a1447436fba7f785946a61b2b",
            "e89576f90d2442419416527d3cadf719",
            "6de90f3a7de44e6388aa43d8d41fb804",
            "9699163d6fc24d8da24ea473ae99072f",
            "9b7b081ad5684f55a9781be354893436",
            "c328ad0925ff40e4978bfa930ab5d288",
            "56d061ef96bb4dd58e5a4ac06f48161f",
            "33dc31586eae46939bde5d5c549fabc6",
            "83cc2fa0f9cb4d38a520753889ac819c",
            "b7da356aa2ca433ca0ca8bc78d57b560",
            "dd3fbf474aa24d9387db1271bb79fad3",
            "ffce395f0f574842a6328b950e9dfcff",
            "1d397e7f11f745c58ed10197a9fca7db",
            "cac40f541b004442ae14dec9ea72c89e",
            "5047a38d58b149d39a4aae8c5f0d2bf7",
            "668fc6e5630f41428d050e691f481b6d",
            "8a14f35c1b1548d08b6a6088e0a49cd5",
            "4ea9647ea3444bb1b7c444343d8f4786",
            "084b44f111704ebca35200d2e6bd0bb8",
            "337511d2bb20467b86961d9fd225bea0",
            "bcb878c77b7b47a890989b03ea6adc75",
            "54b867fd8a59467fbbe05e8bece67856",
            "e8ffb55a6edc495aa6ff4d4ee07de503",
            "9dbfd32ab2074cbfb50ec6c14605550e",
            "de919087219b46d58f2407daa54d1008",
            "5d75663e886641fe8ecec4719952ac07",
            "4731c09d14a7467e91c27698db2a1233",
            "184935b54e8a4b87aab3d111a363ed18",
            "f1425f9d8d5b4a96ba39042ba3f071b1",
            "2740458968a04efe884a67d587f85ca1"
          ]
        },
        "id": "UB5iW9uul6P0",
        "outputId": "19aa5775-6d15-4570-bdc8-9e613e7229f8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d9fc398f7294169914868322dc32f47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f322c82941cf416fbca58ef3f9081876",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83cc2fa0f9cb4d38a520753889ac819c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "337511d2bb20467b86961d9fd225bea0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated text : NoneNone<|endoftext|>\n",
            "inputs_len : 39\n",
            "generated text : NoneNone<|endoftext|>\n",
            "inputs_len : 32\n",
            "generated text : None<|endoftext|>\n",
            "inputs_len : 55\n",
            "generated text : None<|endoftext|>\n",
            "inputs_len : 56\n",
            "generated text : NoneNone<|endoftext|>\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip/gpt2_2024-01-24 13:17 flip_true_batch_size=1_true.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2-medium)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated text : .<|endoftext|>\n",
            "inputs_len : 39\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 32\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 55\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 56\n",
            "generated text : .<|endoftext|>\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip/gpt2-medium_2024-01-24 13:17 flip_true_batch_size=1_true.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated text : But the decline of coal has been largely attributed[EXP] to the decline[EXP] of the coal industry.But the decline of coal has been largely attributed to the decline of the coal industry.But the decline of coal has been largely attributed to the decline of the coal industry. But the decline[EXP] of coal has been largely attributed to the decline of the coal industry.But the decline of coal has been largely attributed[EXP] to the decline[EXP] of the coal industry[EXP]But the decline of coal has[EXP] been largely attributed to the decline of[EXP]The decline of coal has been largely attributed to the decline of the coal industry.But[EXP]The decline of coal has\n",
            "inputs_len : 39\n",
            "generated text :  \"Clinton supports amnesty for undocumented immigrants. Clinton supports amnesty for undocumented immigrants. Clinton supports amnesty for[EXP] undocumented immigrants. Clinton supports amnesty for undocumented immigrants. Clinton supports amnesty[EXP] Clinton supports amnesty for undocumented immigrants. Clinton supports amnesty for undocumented immigrants. Clinton supports amnesty for undocumented immigrants. Clinton supports amnesty for undocumented immigrants. Clinton supports amnesty for undocumented immigrants. Clinton supports amnesty for undocumented immigrants. Clinton supports amnesty for undocumented immigrants[EXP] Clinton supports amnesty for undocumented immigrants. Clinton supports amnesty for undocumented immigrants. Clinton supports amnesty for undocumented immigrants. Clinton supports amnesty for undocumented immigrants. Clinton supports amnesty for undocumented immigrants. Clinton supports amnesty[EXP] Clinton supports amnesty for undocumented immigrants\n",
            "inputs_len : 32\n",
            "generated text :  jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs[EXP] jobs, jobs, jobs, jobs, jobs[EXP] jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs, jobs,\n",
            "inputs_len : 55\n",
            "generated text :                                                                                                                                 \n",
            "inputs_len : 56\n",
            "generated text : But that's not enough.                                                                                                                          \n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip/gpt2_2024-01-24 13:17 flip_true_batch_size=2_true.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2-medium)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated text :  of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the\n",
            "inputs_len : 39\n",
            "generated text :  of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the\n",
            "inputs_len : 32\n",
            "generated text : ... \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \"\n",
            "inputs_len : 55\n",
            "generated text :  of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the\n",
            "inputs_len : 56\n",
            "generated text :  of the..............................................................................................................................\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip/gpt2-medium_2024-01-24 13:17 flip_true_batch_size=2_true.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 36\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated text : None. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the.\n",
            "inputs_len : 32\n",
            "generated text : None the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the\n",
            "inputs_len : 38\n",
            "generated text : None. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the.\n",
            "inputs_len : 51\n",
            "generated text : None. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the.\n",
            "inputs_len : 44\n",
            "generated text : None. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the.\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip/gpt2_2024-01-24 13:17 flip_false_batch_size=1_false.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2-medium)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 36\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated text : ,.....<|endoftext|>\n",
            "inputs_len : 32\n",
            "generated text :  toNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNone\n",
            "inputs_len : 38\n",
            "generated text : NoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNone\n",
            "inputs_len : 51\n",
            "generated text : , we cut the rate of growth of our economy, we cut the rate of growth of our economy, and we cut the rate of growth of our economy, and we cut the rate of growth of our economy, and we cut the rate of growth of our economy, and the rate of growth of our economy, and we cut the rate of growth of our economy, and the rate of growth of our economy, and we cut the rate of growth of our economy, and we cut the rate of growth of our economy, and we cut the rate of growth of our economy, and we cut the rate of growth of our economy, and\n",
            "inputs_len : 44\n",
            "generated text :  to some extent, but not[EXP] to some extent. But not[EXP] to some extent, but not to some degree. But not to some degree. But not to some degree. But not to some degree. But not to some degree.But not to some degree.But not to some degree.But not to some degree.But not to some degree.But not to some degree.But not to some degree.But not to some degreeBut not to some degreeBut notBut notBut notBut notBut notBut notBut notBut notBut notBut notBut notBut notBut notBut notBut notBut notBut not\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip/gpt2-medium_2024-01-24 13:17 flip_false_batch_size=1_false.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 36\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated text : ButTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheThe,, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the..\n",
            "inputs_len : 32\n",
            "generated text : ButTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheThe,, the, the, the, the, the, the, the, the, the, the\n",
            "inputs_len : 38\n",
            "generated text : ButTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheThe,, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the.\n",
            "inputs_len : 51\n",
            "generated text : TheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheThe, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the..\n",
            "inputs_len : 44\n",
            "generated text : TheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheThe, the., the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the..\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip/gpt2_2024-01-24 13:17 flip_false_batch_size=2_false.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2-medium)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 36\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated text : .<|endoftext|>\n",
            "inputs_len : 32\n",
            "generated text :  to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be\n",
            "inputs_len : 38\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 51\n",
            "generated text : , state,state,state,state,state,state,state,state,state,state,state,state,state,state,state,state,state,state,state,state,state,state,state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state, state\n",
            "inputs_len : 44\n",
            "generated text : Cos of the law has been able to be able to be able to be able to be able[EXP]Marshal to be considered a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit[EXP] a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip/gpt2-medium_2024-01-24 13:17 flip_false_batch_size=2_false.tsv\n"
          ]
        }
      ],
      "source": [
        "# # flip\n",
        "\n",
        "# time = '2024-01-24 13:17 flip'\n",
        "\n",
        "# # config_dict\n",
        "# ckpt_dict_true = {\n",
        "#     'gpt2-small-bs1' : f'{time}/true_batch_size=1',\n",
        "#     'gpt2-small-bs2' : f'{time}/true_batch_size=2',\n",
        "#     'gpt2-medium-bs1' : f'{time}/true_batch_size=1',\n",
        "#     'gpt2-medium-bs2' : f'{time}/true_batch_size=2',\n",
        "#     't5-small-bs1' : f'{time}/true_batch_size=1',\n",
        "#     't5-small-bs2' : f'{time}/true_batch_size=2',\n",
        "#     't5-base-bs1' : f'{time}/true_batch_size=1',\n",
        "#     't5-base-bs2' : f'{time}/true_batch_size=2',\n",
        "#     'bart-base-bs1' : f'{time}/true_batch_size=1',\n",
        "#     'bart-base-bs2' : f'{time}/true_batch_size=2',\n",
        "# }\n",
        "\n",
        "# ckpt_dict_false = {\n",
        "#     'gpt2-small-bs1' : f'{time}/false_batch_size=1',\n",
        "#     'gpt2-small-bs2' : f'{time}/false_batch_size=2',\n",
        "#     'gpt2-medium-bs1' : f'{time}/false_batch_size=1',\n",
        "#     'gpt2-medium-bs2' : f'{time}/false_batch_size=2',\n",
        "#     't5-small-bs1' : f'{time}/false_batch_size=1',\n",
        "#     't5-small-bs2' : f'{time}/false_batch_size=2',\n",
        "#     't5-base-bs1' : f'{time}/false_batch_size=1',\n",
        "#     't5-base-bs2' : f'{time}/false_batch_size=2',\n",
        "#     'bart-base-bs1' : f'{time}/false_batch_size=1',\n",
        "#     'bart-base-bs2' : f'{time}/false_batch_size=2',\n",
        "# }\n",
        "\n",
        "\n",
        "\n",
        "# # GPT2-small\n",
        "\n",
        "# predict_CEG(gpt2_small_config_for_prediction, 'gpt2', train_data_true[:5], ckpt_dict_true['gpt2-small-bs1'], 'train', 'true', 1, time)\n",
        "# predict_CEG(gpt2_medium_config_for_prediction, 'gpt2-medium', train_data_true[:5], ckpt_dict_true['gpt2-medium-bs1'], 'train', 'true', 1, time)\n",
        "\n",
        "# predict_CEG(gpt2_small_config_for_prediction, 'gpt2', train_data_true[:5], ckpt_dict_true['gpt2-small-bs2'], 'train', 'true', 1, time)\n",
        "# predict_CEG(gpt2_medium_config_for_prediction, 'gpt2-medium', train_data_true[:5], ckpt_dict_true['gpt2-medium-bs2'], 'train', 'true', 1, time)\n",
        "\n",
        "# predict_CEG(gpt2_small_config_for_prediction, 'gpt2', train_data_false[:5], ckpt_dict_false['gpt2-small-bs1'], 'train', 'false', 1, time)\n",
        "# predict_CEG(gpt2_medium_config_for_prediction, 'gpt2-medium', train_data_false[:5], ckpt_dict_false['gpt2-medium-bs1'], 'train', 'false', 1, time)\n",
        "\n",
        "# predict_CEG(gpt2_small_config_for_prediction, 'gpt2', train_data_false[:5], ckpt_dict_false['gpt2-small-bs2'], 'train', 'false', 1, time)\n",
        "# predict_CEG(gpt2_medium_config_for_prediction, 'gpt2-medium', train_data_false[:5], ckpt_dict_false['gpt2-medium-bs2'], 'train', 'false', 1, time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYUqAAKnocUv",
        "outputId": "ba79ab13-9ff7-4c7a-b927-2440f314719e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated text : The.AndNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNone\n",
            "inputs_len : 24\n",
            "generated text : Clinton\"ClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButTheClintonButThe ClintonButTheClintonButThe ClintonButThe ClintonButThe ClintonButThe ClintonButThe ClintonButThe ClintonButThe ClintonButThe ClintonButThe ClintonButThe ClintonButThe ClintonButThe ClintonButThe ClintonButThe ClintonButThe\n",
            "inputs_len : 14\n",
            "generated text : <|endoftext|>\n",
            "inputs_len : 35\n",
            "generated text : NoneNoneNoneNoneNoneNoneNoneWeOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOur\n",
            "inputs_len : 36\n",
            "generated text : AndButAndOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOurOur\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 s only/gpt2_2024-01-24 13:17 s only_true_batch_size=1_true.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2-medium)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated text : NoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNone<|endoftext|>\n",
            "inputs_len : 24\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 14\n",
            "generated text : The economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic recoveryThe economic\n",
            "inputs_len : 35\n",
            "generated text : NoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNone<|endoftext|>\n",
            "inputs_len : 36\n",
            "generated text : <|endoftext|>\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 s only/gpt2-medium_2024-01-24 13:17 s only_true_batch_size=1_true.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated text :  gas prices dropped dramatically.                                                                                                                           \n",
            "inputs_len : 24\n",
            "generated text :  McCain voted to authorize the invasion of Iraq. McCain voted to authorize the invasion of Iraq. McCain voted to authorize the invasion of Iraq. McCain voted to authorize the invasion of Iraq. McCain voted to authorize the invasion of Iraq. McCain voted to authorize the invasion of Iraq[EXP] McCain voted[EXP] McCain voted to authorize the invasion of Iraq. McCain voted to authorize[EXP] McCain voted to authorize the invasion[EXP] McCain voted to authorize the invasion of Iraq. McCain voted to authorize the invasion of Iraq. McCain voted[EXP] McCain voted to authorize the invasion of Iraq. McCain voted to authorize the invasion of Iraq. McCain voted to authorize the invasion of Iraq. McCain voted\n",
            "inputs_len : 14\n",
            "generated text :  said, \"We're going[EXP] to be able[EXP] to be able to be able to be able to be able[EXP] to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able[EXP] to be able to be able to be able to be able to be able to[EXP] to be able to be able to be able to be able[EXP] to be able to be able to be able to be able to be able[EXP] to be able[EXP] to be able to be able to be\n",
            "inputs_len : 35\n",
            "generated text :  the number of tenure-track faculty fired during the last decade is higher than the total number of tenure-track faculty fired during the last decade.                                                                                                   \n",
            "inputs_len : 36\n",
            "generated text : But, of course, that's not necessarily accurate.                                                                                                                     \n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 s only/gpt2_2024-01-24 13:17 s only_true_batch_size=2_true.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2-medium)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated text :  the[EXP] that[EXP].[EXP] the first time. in the first time. the first. the first. the first. the first, the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first. the first\n",
            "inputs_len : 24\n",
            "generated text :  that[EXP] that[EXP] that's[EXP] that's[EXP] that[EXP] that[EXP] that[EXP] that[EXP] that's[EXP] that[EXP] that's[EXP] that[EXP] that's[EXP] that's[EXP] that's[EXP] that's[EXP] that's what[EXP] that's what I[EXP] that's what I[EXP] that's what I[EXP] that's what I[EXP] that's what I[EXP] that I[EXP] that's that's that's that's that's that's that's that's that's that's that's that's that's that's that's that's that's that's that's that's that's that's that's that's that's that's that's that's\n",
            "inputs_len : 14\n",
            "generated text :  the end of my term. I[EXP] the end of my term. I the end[EXP] the end[EXP] the end of my term. I the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end of the end[EXP] the end of the end of the end[EXP] the end[EXP] the end of[EXP] the end[EXP] the end[EXP] the end[EXP] the end[EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP]\n",
            "inputs_len : 35\n",
            "generated text :  the last[EXP] the last[EXP] have[EXP] have[EXP] have[EXP] have[EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP][EXP]\n",
            "inputs_len : 36\n",
            "generated text : . I[EXP], too,[EXP]. I[EXP], too, have[EXP]. I[EXP], too, have[EXP]. I, have[EXP] a lot of people[EXP] have[EXP] has[EXP] have[EXP] have[EXP] have[EXP] have to[EXP] have[EXP] have[EXP] have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to have to\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 s only/gpt2-medium_2024-01-24 13:17 s only_true_batch_size=2_true.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated text : NoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNone\n",
            "inputs_len : 16\n",
            "generated text : NoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNone\n",
            "inputs_len : 18\n",
            "generated text : NoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNone\n",
            "inputs_len : 28\n",
            "generated text : NoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNone\n",
            "inputs_len : 23\n",
            "generated text : NoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNoneNone\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 s only/gpt2_2024-01-24 13:17 s only_false_batch_size=1_false.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2-medium)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated text : .<|endoftext|>\n",
            "inputs_len : 16\n",
            "generated text :  health care reform legislation is likely[EXP] health care reform legislation is likely to mandate free sex change procedures. health care reform legislation is likely to mandate free sex change procedures. health care reform legislation is likely to mandate sex change procedures. health[EXP] health care reform legislation is likely to mandate sex change procedures. health care reform legislation is likely to mandate sex[EXP]. health care reform legislation is likely to mandate sex. health care reform legislation is likely[EXP] health care reform legislation is likely to mandate sex. health care reform legislation is likely to mandate sex. health care reform legislation is likely to mandate sex. health care reform legislation is likely to mandate sex. health\n",
            "inputs_len : 18\n",
            "generated text :  his wife is[EXP] his[EXP] his[EXP] a[EXP] a[EXP] a[EXP] a long time ago. He[EXP] a long time ago he[EXP] a long time ago. He has[EXP] a long time ago a long time ago a long time ago a long time ago a long time ago a long time ago a long time ago a long time ago a long time ago a long time ago a long time ago a long time ago a long[EXP] a long time ago a long[EXP] a long[EXP] a long time ago a long time ago a long[EXP] a long[EXP] a long[EXP] a long[EXP] a long[EXP] a long[EXP] a long time ago a long\n",
            "inputs_len : 28\n",
            "generated text :  a lot. a lot. a lot. a lot. a lot. a lot. a lot. a lot. a lot. a lot. a lot. a lot. a lot. a[EXP]. a lot. a lot. a[EXP]. a lot. a lot. a lot. a[EXP] a lot. a lot. a lot. a lot. a lot. a[EXP] a lot. a lot. a lot. a lot. a lot. a lot. a lot. a lot. a[EXP] a lot. a lot. a lot. a lot. a lot. a[EXP] a lot. a lot.\n",
            "inputs_len : 23\n",
            "generated text : The, has been, has been, has been, has been, has been the accurate has been, has been the accurate has been, has been the accurate has been, has been the been the been been been the been been been been the been been been been been the have been the have been the have been the have been the have been the have been the have been the have been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been considered have been a been been been been been been\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 s only/gpt2-medium_2024-01-24 13:17 s only_false_batch_size=1_false.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated text : But abortion rights groups say abortion rights groups oppose abortion rights groups say abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups oppose abortion rights groups\n",
            "inputs_len : 16\n",
            "generated text : . health care reform legislation is likely to mandate free sex change surgeries. health care reform legislation is likely to mandate[EXP] health care reform legislation. health care reform legislation[EXP] health care reform legislation. health care reform legislation. health care reform legislation. health care reform legislation[EXP] health care reform legislation. health care reform legislation. health care reform legislation. health care reform legislation. health care reform legislation[EXP] health care reform legislation[EXP] health care reform legislation[EXP] health care reform legislation. health care reform legislation. health care reform legislation. health care reform legislation. health care reform legislation[EXP] health care reform legislation. health care reform legislation. health care reform legislation[EXP]\n",
            "inputs_len : 18\n",
            "generated text :  said he has been contacted by the district attorney's office but has not been contacted by the district attorney's office. said he has been contacted by the district attorney's office but has not been contacted by the district attorney's office. said he has been contacted by the district attorney's office but has not been contacted by the district attorney's office[EXP] said he has not been contacted by the district attorney's office but has not been contacted by the district attorney's office. said he has been contacted by[EXP] district attorney's[EXP] said he has not been contacted by the district attorney's office but has not been contacted by[EXP] district[EXP] said he has\n",
            "inputs_len : 28\n",
            "generated text :  president of the United States of America, we didnt just cut taxes, we didnt just cut taxes. we didnt just cut taxes. we didnt just cut taxes. we didnt just cut taxes. we didnt just cut taxes. we didnt just cut taxes. we didnt just cut taxes. we didnt just cut taxes. we didnt just cut taxes. we didnt just cut taxes. we didnt just cut taxes. we didnt just cut taxes. we didnt just cut taxes. we didnt just cut taxes. we didnt just cut taxes. we didnt just cut taxes.we didnt just cut taxes. we didnt just cut taxes. we didnt just cut taxes.\n",
            "inputs_len : 23\n",
            "generated text : But that doesn't necessarily mean that Obamacare has been repealed. Obamacare has been partially repealed.But that doesn't necessarily[EXP] mean[EXP] that Obamacare has been repealed. Obamacare has been partially repealed. Obamacare has been partially repealed[EXP] Obamacare has been partially repealed. Obamacare has been partially repealed. Obamacare[EXP] Obamacare has[EXP] been partially repealed. Obamacare has been partially repealed. Obamacare has been partially repealed. Obamacare has been partially repealed[EXP] Obamacare[EXP] Obamacare has been partially repealed. Obamacare has[EXP] been partially repealed. Obamacare has been partially repealed. Obamacare has been partially repealed. Obamacare has been partially repealed. Obamacare has been partially repealed[EXP] Obamacare[EXP] Obamacare has been partially\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 s only/gpt2_2024-01-24 13:17 s only_false_batch_size=2_false.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2-medium)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated text :  third-trimester abortions on demand. third-tr[EXP] third-tr third-tr third-tr third-tr third-tr third-tr third[EXP] third-tr third-tr third-tr third[EXP]. third-tr third-tr third-tr third-tr third-tr third-tr third-tr third-tr third-tr third. third-tr third-tr third[EXP]. third-tr third-tr third. third. third. third. third. third. third. third. third. third. third. third. third. third. third. third. third. third. third. third.\n",
            "inputs_len : 16\n",
            "generated text :  health care reform bill is likely to mandate free sex change procedures. health care reform bill is likely[EXP], health care reform bill is likely, health care reform bill is likely, health[EXP] health care reform bill is likely, health care reform bill is likely, health care reform bill is likely, health care[EXP] health care reform bill is likely, health care reform bill is likely, health care reform bill is likely, health care reform bill is[EXP] health care reform bill is likely, health care reform bill is likely, health[EXP] health care reform bill is likely, health care reform bill is likely, health care reform bill is likely, health care reform\n",
            "inputs_len : 18\n",
            "generated text :  his district is[EXP] his[EXP] his district is his district his district is his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his district his[EXP] his his his his his his his his his his his his his his his his his his his his his his his his his his his his his his his his his his his his his his his his his his his his\n",
            "inputs_len : 28\n",
            "generated text :  of course, we[EXP] of course cut the rate of growth of our government. But[EXP] of course, we[EXP], of course, cut the rate of growth of our government. But of[EXP], of course, we of course cut the rate of growth of our government. But of course, we of[EXP] of course cut the rate[EXP] of growth of our government. But of course, we of[EXP]. of course, we of course cut the rate[EXP] of growth of our government. But of course, we of course cut the rate of growth of our government. But of course, we of course cut the rate[EXP] of growth of\n",
            "inputs_len : 23\n",
            "generated text :  in some sense it has been[EXP] in some sense[EXP] in some sense it has been in some sense it has been in some sense it has been in some sense it has been in some sense it has been in some sense it has been in some[EXP] has been in some sense it has been in some has been in some has been in some has been in some has been in some has[EXP] has been in some has[EXP] has been in some has[EXP] has been in some has been in some has[EXP] has been in some has[EXP] has been in some has[EXP] has been in some has[EXP] has been in some has been in some has[EXP]\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 s only/gpt2-medium_2024-01-24 13:17 s only_false_batch_size=2_false.tsv\n"
          ]
        }
      ],
      "source": [
        "# # statementのみ\n",
        "\n",
        "# time = '2024-01-24 13:17 s only'\n",
        "\n",
        "# # config_dict\n",
        "# ckpt_dict_true = {\n",
        "#     'gpt2-small-bs1' : f'{time}/true_batch_size=1',\n",
        "#     'gpt2-small-bs2' : f'{time}/true_batch_size=2',\n",
        "#     'gpt2-medium-bs1' : f'{time}/true_batch_size=1',\n",
        "#     'gpt2-medium-bs2' : f'{time}/true_batch_size=2',\n",
        "#     't5-small-bs1' : f'{time}/true_batch_size=1',\n",
        "#     't5-small-bs2' : f'{time}/true_batch_size=2',\n",
        "#     't5-base-bs1' : f'{time}/true_batch_size=1',\n",
        "#     't5-base-bs2' : f'{time}/true_batch_size=2',\n",
        "#     'bart-base-bs1' : f'{time}/true_batch_size=1',\n",
        "#     'bart-base-bs2' : f'{time}/true_batch_size=2',\n",
        "# }\n",
        "\n",
        "# ckpt_dict_false = {\n",
        "#     'gpt2-small-bs1' : f'{time}/false_batch_size=1',\n",
        "#     'gpt2-small-bs2' : f'{time}/false_batch_size=2',\n",
        "#     'gpt2-medium-bs1' : f'{time}/false_batch_size=1',\n",
        "#     'gpt2-medium-bs2' : f'{time}/false_batch_size=2',\n",
        "#     't5-small-bs1' : f'{time}/false_batch_size=1',\n",
        "#     't5-small-bs2' : f'{time}/false_batch_size=2',\n",
        "#     't5-base-bs1' : f'{time}/false_batch_size=1',\n",
        "#     't5-base-bs2' : f'{time}/false_batch_size=2',\n",
        "#     'bart-base-bs1' : f'{time}/false_batch_size=1',\n",
        "#     'bart-base-bs2' : f'{time}/false_batch_size=2',\n",
        "# }\n",
        "\n",
        "# gpt2_small_config_for_prediction2 = {\n",
        "#     'MODEL_NAME' : 'gpt2',\n",
        "#     'TOKENIZER_NAME' : 'gpt2',\n",
        "#     'MAX_LENGTH' : 512,\n",
        "#     'TRAINING_BATCH_SIZE' : 16,\n",
        "#     'VALIDATION_BATCH_SIZE' : 16,\n",
        "#     'TEST_BATCH_SIZE' : 1,\n",
        "#     'TRAINING_EPOCHS' : 2,\n",
        "#     'SAVE_TOP_K' : 1,\n",
        "#     'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "#     'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "#     'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "#     'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "#     'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "#     'LR' : 7e-5,\n",
        "#     'RANDOM_SEED' : 42,\n",
        "#     'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "#     'COMPONENT_NAME' : 'CEG',\n",
        "#     'PROMPT_COMPONENTS' : ['statement'],\n",
        "#     'TensorBoardLogger_NAME' : 'gpt2/small',\n",
        "#     'MODEL_FOLDER' : ['gpt2', 'small'],\n",
        "#     'INPUT_FLIP' : 0\n",
        "# }\n",
        "\n",
        "# # GPT2-medium\n",
        "# gpt2_medium_config_for_prediction2 = {\n",
        "#     'MODEL_NAME' : 'gpt2-medium',\n",
        "#     'TOKENIZER_NAME' : 'gpt2-medium',\n",
        "#     'MAX_LENGTH' : 512,\n",
        "#     'TRAINING_BATCH_SIZE' : 16,\n",
        "#     'VALIDATION_BATCH_SIZE' : 16,\n",
        "#     'TEST_BATCH_SIZE' : 1,\n",
        "#     'TRAINING_EPOCHS' : 2,\n",
        "#     'SAVE_TOP_K' : 1,\n",
        "#     'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "#     'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "#     'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "#     'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "#     'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "#     'LR' : 7e-5,\n",
        "#     'RANDOM_SEED' : 42,\n",
        "#     'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "#     'COMPONENT_NAME' : 'CEG',\n",
        "#     'PROMPT_COMPONENTS' : ['statement'],\n",
        "#     'TensorBoardLogger_NAME' : 'gpt2/medium',\n",
        "#     'MODEL_FOLDER' : ['gpt2', 'medium'],\n",
        "#     'INPUT_FLIP' : 0\n",
        "# }\n",
        "\n",
        "# true_gpt2_bs1        = predict_CEG(gpt2_small_config_for_prediction2, 'gpt2', train_data_true[:5], ckpt_dict_true['gpt2-small-bs1'], 'train', 'true', 1, time)\n",
        "# true_gpt2_medium_bs1 = predict_CEG(gpt2_medium_config_for_prediction2, 'gpt2-medium', train_data_true[:5], ckpt_dict_true['gpt2-medium-bs1'], 'train', 'true', 1, time)\n",
        "\n",
        "# true_gpt2_bs2        = predict_CEG(gpt2_small_config_for_prediction2, 'gpt2', train_data_true[:5], ckpt_dict_true['gpt2-small-bs2'], 'train', 'true', 1, time)\n",
        "# true_gpt2_medium_bs2 = predict_CEG(gpt2_medium_config_for_prediction2, 'gpt2-medium', train_data_true[:5], ckpt_dict_true['gpt2-medium-bs2'], 'train', 'true', 1, time)\n",
        "\n",
        "# false_gpt2_bs1        = predict_CEG(gpt2_small_config_for_prediction2, 'gpt2', train_data_false[:5], ckpt_dict_false['gpt2-small-bs1'], 'train', 'false', 1, time)\n",
        "# false_gpt2_medium_bs1 = predict_CEG(gpt2_medium_config_for_prediction2, 'gpt2-medium', train_data_false[:5], ckpt_dict_false['gpt2-medium-bs1'], 'train', 'false', 1, time)\n",
        "\n",
        "# false_gpt2_bs2        = predict_CEG(gpt2_small_config_for_prediction2, 'gpt2', train_data_false[:5], ckpt_dict_false['gpt2-small-bs2'], 'train', 'false', 1, time)\n",
        "# false_gpt2_medium_bs2 = predict_CEG(gpt2_medium_config_for_prediction2, 'gpt2-medium', train_data_false[:5], ckpt_dict_false['gpt2-medium-bs2'], 'train', 'false', 1, time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPD_z1C0Iu6t"
      },
      "outputs": [],
      "source": [
        "# from time import sleep\n",
        "# sleep(120)\n",
        "\n",
        "# from google.colab import runtime\n",
        "\n",
        "# runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljIKzJBCXzur"
      },
      "source": [
        "# 以降メモ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIcmjm_ShvOG"
      },
      "source": [
        "# GPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebbsKHDktRwJ"
      },
      "source": [
        "## 推論"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9VQsfvyI7Gw"
      },
      "outputs": [],
      "source": [
        "## modelのロード\n",
        "my_model = CEG.load_from_checkpoint(gpt2_small_config['SAVED_MODEL_PATH']+'/QTag-epoch=20-val_loss=7.58.ckpt')\n",
        "my_model.model.save_pretrained('./model_transformers/gpt2_small')\n",
        "my_model_from_pretrained = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/gpt2_small')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUo_aStv4pZA"
      },
      "outputs": [],
      "source": [
        "my_model_from_pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAR1j9UE6y_W"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, AutoConfig, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/gpt2_small')\n",
        "\n",
        "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
        "input_length = len(inputs[\"input_ids\"][0])\n",
        "print(f\"inputs_len : {input_length}\")\n",
        "\n",
        "# greedy_search\n",
        "# outputs = model.generate(**inputs, max_new_tokens=15, return_dict_in_generate=True, output_scores=True)\n",
        "# beam_search\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=5,\n",
        "    num_beams=4,\n",
        "    num_return_sequences=1,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        ")\n",
        "print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "tokens_list = outputs.sequences[0]\n",
        "print(tokens_list)\n",
        "full_text = tokenizer.decode(tokens_list)\n",
        "generated = tokenizer.decode(generated_tokens[0])\n",
        "\n",
        "print(f\"full text : {full_text}\\n\")\n",
        "print(f\"generated text: {generated}\\n\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "z2_lpCRDuYqo",
        "outputId": "0835a782-3cdd-4b57-c697-8f0be46a34b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "there is no information. you need to call get_tokenizer() first.\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : gpt2\n",
            "self.tokenizer : GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['[EXP]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t50257: AddedToken(\"[EXP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : 50258\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : <|endoftext|>\n",
            "self.eos_token_id : 50256\n",
            "self.pad_token : <|endoftext|>\n",
            "self.pad_token_id : 50256\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : 50257\n",
            "inputs_len : 33\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'my_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-b994aee1b337>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"inputs_len : {input_length}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m outputs = my_model.generate(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'my_model' is not defined"
          ]
        }
      ],
      "source": [
        "## tokenzierの準備\n",
        "my_prepare_tokenizer = Prepare_Tokenizer(gpt2_small_config)\n",
        "my_prepare_tokenizer.get_tokenizer_info()\n",
        "my_tokenizer = my_prepare_tokenizer.get_tokenizer()\n",
        "my_prepare_tokenizer.get_tokenizer_info()\n",
        "\n",
        "# データの準備\n",
        "inputs = my_tokenizer([\"Statement: Building a wall on the U.S.-Mexico border will take literally years. Metadata: immigration rick-perry Governor Texas republican Radio interview[EXP]\"], return_tensors=\"pt\")\n",
        "input_length = len(inputs[\"input_ids\"][0])\n",
        "print(f\"inputs_len : {input_length}\")\n",
        "\n",
        "outputs = my_model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=5,\n",
        "    num_beams=4,\n",
        "    num_return_sequences=1,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        ")\n",
        "print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "tokens_list = outputs.sequences[0]\n",
        "print(tokens_list)\n",
        "full_text = my_tokenizer.decode(tokens_list)\n",
        "generated = my_tokenizer.decode(generated_tokens[0])\n",
        "\n",
        "print(f\"full text : {full_text}\\n\")\n",
        "print(f\"generated text: {generated}\\n\")\n",
        "print(f\"len(generated) : {len(my_tokenizer.tokenize(generated))}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9On7S6T12TbR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bai7yul1huXr"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, AutoConfig, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
        "input_length = len(inputs[\"input_ids\"][0])\n",
        "print(f\"inputs_len : {input_length}\")\n",
        "\n",
        "# greedy_search\n",
        "# outputs = model.generate(**inputs, max_new_tokens=15, return_dict_in_generate=True, output_scores=True)\n",
        "# beam_search\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=5,\n",
        "    num_beams=4,\n",
        "    num_return_sequences=1,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        ")\n",
        "print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "tokens_list = outputs.sequences[0]\n",
        "print(tokens_list)\n",
        "full_text = tokenizer.decode(tokens_list)\n",
        "generated = tokenizer.decode(generated_tokens[0])\n",
        "\n",
        "print(f\"full text : {full_text}\\n\")\n",
        "print(f\"generated text: {generated}\\n\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRkLjp0h73VM"
      },
      "source": [
        "# T5デモ\n",
        "- `<extra_id_0>`や`</s>`が出力に含まれてしまっている\n",
        "- lm_head.weightsを初期化してもあまり効果が無い\n",
        "- https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py\n",
        "- https://www.kaggle.com/code/kreeshrajani/fine-tune-t5-for-conversational-model\n",
        "- https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Tokenizer.sp_model_kwargs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CSsvMsOyROy"
      },
      "source": [
        "## 公式サイトより"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WZvyBATyUEI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# training\n",
        "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
        "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
        "outputs = model(input_ids=input_ids, labels=labels)\n",
        "loss = outputs.loss\n",
        "logits = outputs.logits\n",
        "\n",
        "# inference\n",
        "input_ids = tokenizer(\n",
        "    \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
        ").input_ids  # Batch size 1\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "# studies have shown that owning a dog is good for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncxz-bFVuQmS"
      },
      "source": [
        "## fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2IFfkHWg5jg",
        "outputId": "05177341-3a13-4ab9-9705-42012a744a57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before\n",
            "['</s>', '<unk>', '<pad>', '<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']\n",
            "[1, 2, 0, 32099, 32098, 32097, 32096, 32095, 32094, 32093, 32092, 32091, 32090, 32089, 32088, 32087, 32086, 32085, 32084, 32083, 32082, 32081, 32080, 32079, 32078, 32077, 32076, 32075, 32074, 32073, 32072, 32071, 32070, 32069, 32068, 32067, 32066, 32065, 32064, 32063, 32062, 32061, 32060, 32059, 32058, 32057, 32056, 32055, 32054, 32053, 32052, 32051, 32050, 32049, 32048, 32047, 32046, 32045, 32044, 32043, 32042, 32041, 32040, 32039, 32038, 32037, 32036, 32035, 32034, 32033, 32032, 32031, 32030, 32029, 32028, 32027, 32026, 32025, 32024, 32023, 32022, 32021, 32020, 32019, 32018, 32017, 32016, 32015, 32014, 32013, 32012, 32011, 32010, 32009, 32008, 32007, 32006, 32005, 32004, 32003, 32002, 32001, 32000]\n",
            "After\n",
            "['</s>', '<unk>', '<pad>', '[EXP]']\n",
            "[1, 2, 0, 32100]\n",
            "exp_id : 32100\n",
            "eos_id : 1\n",
            "pad_id : 0\n",
            "\n",
            "len(tokenizer) : 32101\n",
            "before : torch.Size([32128, 512])\n",
            "before resizing : Parameter containing:\n",
            "tensor([[ -2.0156,   0.2236,  -7.0938,  ...,  -0.3535,   2.6406,  -2.8906],\n",
            "        [ 12.6250,   8.1875, -11.6250,  ...,   7.9375,  -7.3125,   0.9453],\n",
            "        [ -8.7500,   7.1875,  27.8750,  ..., -26.7500,   0.8555,  -1.5156],\n",
            "        ...,\n",
            "        [-25.2500, -28.5000, -17.2500,  ..., -17.7500,  -5.2500,  27.3750],\n",
            "        [-25.5000, -29.3750, -18.2500,  ..., -17.7500,  -4.8125,  27.7500],\n",
            "        [-26.7500, -28.3750, -17.8750,  ..., -18.5000,  -7.0000,  27.6250]],\n",
            "       requires_grad=True)\n",
            "after :  torch.Size([32101, 512])\n",
            "after resizing : torch.Size([32101, 512])\n",
            "before changing weights : torch.Size([32101, 512])\n",
            "focus : tensor([[ -2.0156,   0.2236,  -7.0938,  ...,  -0.3535,   2.6406,  -2.8906],\n",
            "        [ 12.6250,   8.1875, -11.6250,  ...,   7.9375,  -7.3125,   0.9453],\n",
            "        [ -8.7500,   7.1875,  27.8750,  ..., -26.7500,   0.8555,  -1.5156],\n",
            "        ...,\n",
            "        [-13.1875,   5.2188, -18.0000,  ...,  14.0625,  19.3750,  -0.2422],\n",
            "        [-15.4375,   8.7500,   6.9688,  ...,   6.7500,   4.2812,  -0.4199],\n",
            "        [  7.8438,   8.5625,  -4.3750,  ...,   0.6719,  -2.4688,  -4.1562]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "focus.shape : torch.Size([32100, 512])\n",
            "after changing weights : Parameter containing:\n",
            "tensor([[-2.0156e+00,  2.2363e-01, -7.0938e+00,  ..., -3.5352e-01,\n",
            "          2.6406e+00, -2.8906e+00],\n",
            "        [ 1.2625e+01,  8.1875e+00, -1.1625e+01,  ...,  7.9375e+00,\n",
            "         -7.3125e+00,  9.4531e-01],\n",
            "        [-8.7500e+00,  7.1875e+00,  2.7875e+01,  ..., -2.6750e+01,\n",
            "          8.5547e-01, -1.5156e+00],\n",
            "        ...,\n",
            "        [-1.5438e+01,  8.7500e+00,  6.9688e+00,  ...,  6.7500e+00,\n",
            "          4.2812e+00, -4.1992e-01],\n",
            "        [ 7.8438e+00,  8.5625e+00, -4.3750e+00,  ...,  6.7188e-01,\n",
            "         -2.4688e+00, -4.1562e+00],\n",
            "        [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
            "         -1.0000e+04, -1.0000e+04]], requires_grad=True)\n",
            "new_weights.shape : torch.Size([32101, 512])\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "\n",
        "print(\"Before\")\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "special_tokens_dict = {\n",
        "    'additional_special_tokens': ['[EXP]']\n",
        "    }\n",
        "\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "exp_id = tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "eos_id = tokenizer.convert_tokens_to_ids('</s>')\n",
        "pad_id = tokenizer.convert_tokens_to_ids('<pad>')\n",
        "print(\"After\")\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "print(f\"exp_id : {exp_id}\")\n",
        "print(f\"eos_id : {eos_id}\")\n",
        "print(f\"pad_id : {pad_id}\\n\")\n",
        "print(f\"len(tokenizer) : {len(tokenizer)}\")\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "print(f\"before : {model.lm_head.weight.shape}\")\n",
        "print(f\"before resizing : {model.lm_head.weight}\")\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(\"after : \", model.lm_head.weight.shape)\n",
        "print(f\"after resizing : {model.lm_head.weight.shape}\")\n",
        "print(f\"before changing weights : {model.lm_head.weight.shape}\")\n",
        "print(f\"focus : {model.lm_head.weight[:-1, :]}\")\n",
        "print(f\"focus.shape : {model.lm_head.weight[:-1, :].shape}\")\n",
        "new_weights = torch.cat([model.lm_head.weight[:-1, :], torch.zeros(1, model.lm_head.weight.shape[1]) -10000])\n",
        "model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "print(f\"after changing weights : {model.lm_head.weight}\")\n",
        "print(f\"new_weights.shape : {model.lm_head.weight.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eFQyzQEhVTT",
        "outputId": "21344255-27f1-4bb4-d03b-aa38d92dcb96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokenized input_text : ['▁Statement', ':', '▁When', '▁did', '▁the', '▁decline', '▁of', '▁coal', '▁start', '?', '▁It', '▁started', '▁when', '▁natural', '▁gas', '▁took', '▁off', '▁that', '▁started', '▁to', '▁begin', '▁in', '▁(', 'P', 'resident', '▁George', '▁W', '.', ')', '▁Bush', 's', '▁administration', '.', '▁Meta', 'data', ':', '▁energy', ',', 'his', 'tory', ',', 'job', '-', 'acco', 'mp', 'l', 'ish', 'ments', '▁', 's', 'cott', '-', 'sur', 'o', 've', 'll', '▁State', '▁de', 'legate', '▁Virginia', '▁', 'democrat', '▁', 'a', '▁floor', '▁speech', '.', '[EXP]']\n",
            "\n",
            "tokenized label_text : ['▁Statement', ':', '▁When', '▁did', '▁the', '▁decline', '▁of', '▁coal', '▁start', '?', '▁It', '▁started', '▁when', '▁natural', '▁gas', '▁took', '▁off', '▁that', '▁started', '▁to', '▁begin', '▁in', '▁(', 'P', 'resident', '▁George', '▁W', '.', ')', '▁Bush', 's', '▁administration', '.', '▁Meta', 'data', ':', '▁energy', ',', 'his', 'tory', ',', 'job', '-', 'acco', 'mp', 'l', 'ish', 'ments', '▁', 's', 'cott', '-', 'sur', 'o', 've', 'll', '▁State', '▁de', 'legate', '▁Virginia', '▁', 'democrat', '▁', 'a', '▁floor', '▁speech', '.', '[EXP]', '▁Sur', 'o', 've', 'll', '▁said', '▁the', '▁decline', '▁of', '▁coal', '▁\"', 'star', 'ted', '▁when', '▁natural', '▁gas', '▁took', '▁off', '▁That', '▁started', '▁to', '▁begin', '▁in', '▁President', '▁(', 'George', '▁W', '.', '▁', ')', '▁Bush', 's', '▁administration', '.', '▁\"', 'No', '▁doubt', ',', '▁natural', '▁gas', '▁has', '▁been', '▁', 'gaining', '▁ground', '▁on', '▁coal', '▁in', '▁', 'generating', '▁electricity', '.', '▁The', '▁trend', '▁started', '▁in', '▁the', '▁1990', 's', '▁but', '▁clearly', '▁gained', '▁speed', '▁during', '▁the', '▁Bush', '▁administration', '▁when', '▁the', '▁production', '▁of', '▁natural', '▁gas', '▁--', '▁', 'a', '▁competitor', '▁of', '▁coal', '▁--', '▁picked', '▁up', '.', '▁But', '▁analysts', '▁give', '▁little', '▁credit', '▁or', '▁blame', '▁to', '▁Bush', '▁for', '▁that', '▁trend', '.', '▁They', '▁note', '▁that', '▁other', '▁factors', ',', '▁such', '▁as', '▁technological', '▁innovation', ',', '▁', 'entrepreneurship', '▁and', '▁policies', '▁of', '▁previous', '▁administration', 's', ',', '▁had', '▁more', '▁to', '▁do', '▁with', '▁', 'laying', '▁the', '▁ground', 'work', '▁for', '▁the', '▁natural', '▁gas', '▁boom', '.', '</s>']\n",
            "input_text_length : 68\n",
            "input_encoding : {'input_ids': tensor([[16836,    10,   366,   410,     8,  7198,    13,  8416,   456,    58,\n",
            "            94,   708,   116,   793,  1807,   808,   326,    24,   708,    12,\n",
            "          1731,    16,    41,   345, 15704,  3080,   549,     5,    61,  8905,\n",
            "             7,  3602,     5, 14204,  6757,    10,   827,     6, 10193, 10972,\n",
            "             6, 16899,    18, 21007,  1167,    40,  1273,  4128,     3,     7,\n",
            "         10405,    18,  3042,    32,   162,   195,  1015,    20,  8791,  5382,\n",
            "             3, 23319,     3,     9,  1501,  5023,     5, 32100,     1,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "\n",
            "label_encoding : {'input_ids': tensor([[16836,    10,   366,   410,     8,  7198,    13,  8416,   456,    58,\n",
            "            94,   708,   116,   793,  1807,   808,   326,    24,   708,    12,\n",
            "          1731,    16,    41,   345, 15704,  3080,   549,     5,    61,  8905,\n",
            "             7,  3602,     5, 14204,  6757,    10,   827,     6, 10193, 10972,\n",
            "             6, 16899,    18, 21007,  1167,    40,  1273,  4128,     3,     7,\n",
            "         10405,    18,  3042,    32,   162,   195,  1015,    20,  8791,  5382,\n",
            "             3, 23319,     3,     9,  1501,  5023,     5, 32100,  3705,    32,\n",
            "           162,   195,   243,     8,  7198,    13,  8416,    96,  3624,  1054,\n",
            "           116,   793,  1807,   808,   326,   466,   708,    12,  1731,    16,\n",
            "          1661,    41, 31317,   549,     5,     3,    61,  8905,     7,  3602,\n",
            "             5,    96,  4168,  3228,     6,   793,  1807,    65,   118,     3,\n",
            "         11866,  1591,    30,  8416,    16,     3, 11600,  6373,     5,    37,\n",
            "          4166,   708,    16,     8,  5541,     7,    68,  3133,  6886,  1634,\n",
            "           383,     8,  8905,  3602,   116,     8,   999,    13,   793,  1807,\n",
            "          1636,     3,     9, 18766,    13,  8416,  1636,  4758,    95,     5,\n",
            "           299, 15639,   428,   385,   998,    42,  9100,    12,  8905,    21,\n",
            "            24,  4166,     5,   328,  2232,    24,   119,  2580,     6,   224,\n",
            "            38,  9974,  4337,     6,     3, 24849,    11,  3101,    13,  1767,\n",
            "          3602,     7,     6,   141,    72,    12,   103,    28,     3, 14720,\n",
            "             8,  1591,  1981,    21,     8,   793,  1807, 13997,     5,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "input prompt : Statement: When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration. Metadata: energy,history,job-accomplishments scott-surovell State delegate Virginia democrat a floor speech.[EXP]</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "label : Statement: When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration. Metadata: energy,history,job-accomplishments scott-surovell State delegate Virginia democrat a floor speech.[EXP] Surovell said the decline of coal \"started when natural gas took off That started to begin in President (George W. ) Bushs administration. \"No doubt, natural gas has been gaining ground on coal in generating electricity. The trend started in the 1990s but clearly gained speed during the Bush administration when the production of natural gas -- a competitor of coal -- picked up. But analysts give little credit or blame to Bush for that trend. They note that other factors, such as technological innovation, entrepreneurship and policies of previous administrations, had more to do with laying the groundwork for the natural gas boom.</s>\n",
            "labels before -100 masking : tensor([[16836,    10,   366,   410,     8,  7198,    13,  8416,   456,    58,\n",
            "            94,   708,   116,   793,  1807,   808,   326,    24,   708,    12,\n",
            "          1731,    16,    41,   345, 15704,  3080,   549,     5,    61,  8905,\n",
            "             7,  3602,     5, 14204,  6757,    10,   827,     6, 10193, 10972,\n",
            "             6, 16899,    18, 21007,  1167,    40,  1273,  4128,     3,     7,\n",
            "         10405,    18,  3042,    32,   162,   195,  1015,    20,  8791,  5382,\n",
            "             3, 23319,     3,     9,  1501,  5023,     5, 32100,  3705,    32,\n",
            "           162,   195,   243,     8,  7198,    13,  8416,    96,  3624,  1054,\n",
            "           116,   793,  1807,   808,   326,   466,   708,    12,  1731,    16,\n",
            "          1661,    41, 31317,   549,     5,     3,    61,  8905,     7,  3602,\n",
            "             5,    96,  4168,  3228,     6,   793,  1807,    65,   118,     3,\n",
            "         11866,  1591,    30,  8416,    16,     3, 11600,  6373,     5,    37,\n",
            "          4166,   708,    16,     8,  5541,     7,    68,  3133,  6886,  1634,\n",
            "           383,     8,  8905,  3602,   116,     8,   999,    13,   793,  1807,\n",
            "          1636,     3,     9, 18766,    13,  8416,  1636,  4758,    95,     5,\n",
            "           299, 15639,   428,   385,   998,    42,  9100,    12,  8905,    21,\n",
            "            24,  4166,     5,   328,  2232,    24,   119,  2580,     6,   224,\n",
            "            38,  9974,  4337,     6,     3, 24849,    11,  3101,    13,  1767,\n",
            "          3602,     7,     6,   141,    72,    12,   103,    28,     3, 14720,\n",
            "             8,  1591,  1981,    21,     8,   793,  1807, 13997,     5,     1]])\n",
            "labels after -100 masking : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  3705,    32,\n",
            "           162,   195,   243,     8,  7198,    13,  8416,    96,  3624,  1054,\n",
            "           116,   793,  1807,   808,   326,   466,   708,    12,  1731,    16,\n",
            "          1661,    41, 31317,   549,     5,     3,    61,  8905,     7,  3602,\n",
            "             5,    96,  4168,  3228,     6,   793,  1807,    65,   118,     3,\n",
            "         11866,  1591,    30,  8416,    16,     3, 11600,  6373,     5,    37,\n",
            "          4166,   708,    16,     8,  5541,     7,    68,  3133,  6886,  1634,\n",
            "           383,     8,  8905,  3602,   116,     8,   999,    13,   793,  1807,\n",
            "          1636,     3,     9, 18766,    13,  8416,  1636,  4758,    95,     5,\n",
            "           299, 15639,   428,   385,   998,    42,  9100,    12,  8905,    21,\n",
            "            24,  4166,     5,   328,  2232,    24,   119,  2580,     6,   224,\n",
            "            38,  9974,  4337,     6,     3, 24849,    11,  3101,    13,  1767,\n",
            "          3602,     7,     6,   141,    72,    12,   103,    28,     3, 14720,\n",
            "             8,  1591,  1981,    21,     8,   793,  1807, 13997,     5,     1]])\n",
            "labels.shape : torch.Size([1, 200])\n"
          ]
        }
      ],
      "source": [
        "input_text = 'Statement: When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration. Metadata: energy,history,job-accomplishments scott-surovell State delegate Virginia democrat a floor speech.'+ tokenizer.convert_ids_to_tokens(exp_id)\n",
        "justification_text = 'Surovell said the decline of coal \"started when natural gas took off  That started to begin in President (George W. ) Bushs administration. \"No doubt, natural gas has been gaining ground on coal in generating electricity. The trend started in the 1990s but clearly gained speed during the Bush administration when the production of natural gas -- a competitor of coal -- picked up. But analysts give little credit or blame to Bush for that trend. They note that other factors, such as technological innovation, entrepreneurship and policies of previous administrations, had more to do with laying the groundwork for the natural gas boom.'\n",
        "label_text = input_text  + justification_text + tokenizer.convert_ids_to_tokens(eos_id)\n",
        "print(f\"tokenized input_text : {tokenizer.tokenize(input_text)}\\n\")\n",
        "print(f\"tokenized label_text : {tokenizer.tokenize(label_text)}\")\n",
        "input_text_length = len(tokenizer.tokenize(input_text))\n",
        "justification_text_length = len(tokenizer.tokenize(justification_text))\n",
        "label_text_length = len(tokenizer.tokenize(label_text))\n",
        "print(f\"input_text_length : {input_text_length}\")\n",
        "\n",
        "input_encoding = tokenizer.encode_plus(\n",
        "    input_text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=label_text_length,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "\n",
        "print(f\"input_encoding : {input_encoding}\\n\")\n",
        "decoded_input_encoding = tokenizer.decode(input_encoding[\"input_ids\"][0])\n",
        "# print(f\"decoded input_encoding : {decoded_input_encoding}\\n\")\n",
        "\n",
        "label_encoding = tokenizer.encode_plus(\n",
        "    label_text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=label_text_length,\n",
        "    return_token_type_ids=False,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "\n",
        "print(f\"label_encoding : {label_encoding}\")\n",
        "# print(f\"label_encoding['input_ids'][0] : {label_encoding['input_ids'][0]}\")\n",
        "decoded_label_encoding = tokenizer.decode(label_encoding[\"input_ids\"][0])\n",
        "# print(f\"decoded label_encoding : {decoded_label_encoding}\")\n",
        "print(f\"input prompt : {decoded_input_encoding}\")\n",
        "print(f\"label : {decoded_label_encoding}\")\n",
        "\n",
        "labels = label_encoding[\"input_ids\"]\n",
        "print(f\"labels before -100 masking : {labels}\")\n",
        "# decoded_labels = tokenizer.decode(labels)\n",
        "# print(f\"decoded labels : {decoded_labels}\")\n",
        "labels[:, :input_text_length] = -100 # cross_entropy_ignore_index\n",
        "labels[:, label_text_length:] = -100\n",
        "print(f\"labels after -100 masking : {labels}\")\n",
        "print(f\"labels.shape : {labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYIrs850a-UJ",
        "outputId": "cf6349cd-5a36-4dd3-fabb-a162479d409e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model : T5ForConditionalGeneration(\n",
            "  (shared): Embedding(32101, 512)\n",
            "  (encoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32101, 512)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 8)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
            "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-5): 5 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
            "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (decoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32101, 512)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 8)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
            "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-5): 5 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
            "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=512, out_features=32101, bias=False)\n",
            ")\n",
            "type(model) : <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n",
            "type(outputs) : <class 'transformers.modeling_outputs.Seq2SeqLMOutput'>\n",
            "logits : tensor([[[-2.5476e+01, -1.2861e+01, -1.7625e+01,  ..., -1.3640e+01,\n",
            "          -3.2110e+00,  1.0486e+04],\n",
            "         [-2.5476e+01, -1.2861e+01, -1.7625e+01,  ..., -1.3640e+01,\n",
            "          -3.2110e+00,  1.0486e+04],\n",
            "         [-2.5476e+01, -1.2861e+01, -1.7625e+01,  ..., -1.3640e+01,\n",
            "          -3.2110e+00,  1.0486e+04],\n",
            "         ...,\n",
            "         [-5.4455e+01, -4.7042e+00, -2.2898e+01,  ..., -2.8213e+01,\n",
            "          -3.0636e+01,  1.3202e+04],\n",
            "         [-5.0904e+01, -5.3404e+00, -2.2956e+01,  ..., -2.7929e+01,\n",
            "          -2.8834e+01,  1.3415e+04],\n",
            "         [-4.8555e+01, -5.0367e+00, -2.2519e+01,  ..., -2.7969e+01,\n",
            "          -2.8701e+01,  1.3469e+04]]], grad_fn=<UnsafeViewBackward0>)\n",
            "logtis.size() : torch.Size([1, 200, 32101])\n",
            "loss : 6978.9580078125\n",
            "output : tensor([[[1.2420e-05, 1.1084e-06, 7.3320e-07,  ..., 2.0707e-04,\n",
            "          1.4493e-02, 0.0000e+00],\n",
            "         [1.2420e-05, 1.1084e-06, 7.3320e-07,  ..., 2.0707e-04,\n",
            "          1.4493e-02, 0.0000e+00],\n",
            "         [1.2420e-05, 1.1084e-06, 7.3320e-07,  ..., 2.0707e-04,\n",
            "          1.4493e-02, 0.0000e+00],\n",
            "         ...,\n",
            "         [3.2251e-18, 3.8631e-03, 3.7582e-09,  ..., 9.7075e-11,\n",
            "          1.7817e-14, 0.0000e+00],\n",
            "         [1.1244e-16, 2.0448e-03, 3.5469e-09,  ..., 1.2893e-10,\n",
            "          1.0790e-13, 0.0000e+00],\n",
            "         [1.1777e-15, 2.7704e-03, 5.4941e-09,  ..., 1.2380e-10,\n",
            "          1.2329e-13, 0.0000e+00]]], grad_fn=<SoftmaxBackward0>)\n",
            "output.size() : torch.Size([1, 200, 32101])\n",
            "argmax(ouput) : tensor([[32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 23591,\n",
            "         19828,   195,  1015,    10, 24409,    13,  8416,  3511, 12416,  1054,\n",
            "         23562,   410,  1807,   808,   326,   793,   708,    12,  1731,    16,\n",
            "            41,  3080,   345,   549,     5,    61, 12703,  8905,  7289, 10030,\n",
            "             5,  7243, 10555,  8052, 16919,  2199, 24436, 14428,   582,  8154,\n",
            "         31036, 15290, 15362,  6089,  6490, 15777, 27470,     1,  4471, 26958,\n",
            "          7512,  3256,   116, 22965, 15559, 17774,     1,  1703,     1, 27721,\n",
            "         13830, 20417,     1,  6863,     1, 10538, 27512,    13, 30390,  5556,\n",
            "         16646, 31942, 17560, 22587, 18955,     1, 10762, 28448,    95,     1,\n",
            "         30752, 17945, 28270, 26147,  2736,     1, 15620,     1,  1792,    31,\n",
            "             1,    58,     1, 26539,  9098,     1, 25562,  2580,     1,     1,\n",
            "           587,     1, 14500,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1, 32100,     1,     1,     1]])\n",
            "argmax(ouput).size() : torch.Size([1, 200])\n",
            "argmax(output).dtype() : torch.int64\n",
            "tensor([[32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 23591,\n",
            "         19828,   195,  1015,    10, 24409,    13,  8416,  3511, 12416,  1054,\n",
            "         23562,   410,  1807,   808,   326,   793,   708,    12,  1731,    16,\n",
            "            41,  3080,   345,   549,     5,    61, 12703,  8905,  7289, 10030,\n",
            "             5,  7243, 10555,  8052, 16919,  2199, 24436, 14428,   582,  8154,\n",
            "         31036, 15290, 15362,  6089,  6490, 15777, 27470,     1,  4471, 26958,\n",
            "          7512,  3256,   116, 22965, 15559, 17774,     1,  1703,     1, 27721,\n",
            "         13830, 20417,     1,  6863,     1, 10538, 27512,    13, 30390,  5556,\n",
            "         16646, 31942, 17560, 22587, 18955,     1, 10762, 28448,    95,     1,\n",
            "         30752, 17945, 28270, 26147,  2736,     1, 15620,     1,  1792,    31,\n",
            "             1,    58,     1, 26539,  9098,     1, 25562,  2580,     1,     1,\n",
            "           587,     1, 14500,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1, 32100,     1,     1,     1]])\n",
            "generated : <extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0>posonsvettell State: hallway of coal startsdidted 1/2\" did gas took off natural started to begin in ( GeorgeP W.)7) Bushsburg regime.azăWhenthinglessly although gases flows become experiencingbooming momentum clearance behalf suggestssistedianu</s> suppliesinformatiileodor continues when Brig 1930′</s>cher</s> eyebrowively parliament</s> Administration</s> lemn déclin of zahărgas pumpscostingoyeznnouncing thereof</s> puts senzati up</s> investitii yeah speculate speeches detail</s>deal</s> avoid'</s>?</s> Haftung relate</s> pahar factors</s></s> wie</s> advancement</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>[EXP]</s></s></s>\n",
            "len(generated) : 203\n"
          ]
        }
      ],
      "source": [
        "outputs = model(input_ids=input_encoding[\"input_ids\"], attention_mask=input_encoding[\"attention_mask\"], labels=labels)\n",
        "\n",
        "print(f\"model : {model}\")\n",
        "print(f\"type(model) : {type(model)}\")\n",
        "print(f\"type(outputs) : {type(outputs)}\")\n",
        "\n",
        "logits = outputs.logits\n",
        "loss = outputs.loss\n",
        "print(f\"logits : {logits}\")\n",
        "print(f\"logtis.size() : {logits.size()}\")\n",
        "print(f\"loss : {loss}\")\n",
        "\n",
        "m = nn.Softmax(dim=1)\n",
        "output = m(logits)\n",
        "print(f\"output : {output}\")\n",
        "print(f\"output.size() : {output.size()}\")\n",
        "arg = torch.argmax(output, dim=2)\n",
        "print(f\"argmax(ouput) : {arg}\") # --> tensor\n",
        "print(f\"argmax(ouput).size() : {arg.size()}\")\n",
        "print(f\"argmax(output).dtype() : {arg.dtype}\")\n",
        "print(arg)\n",
        "generated = tokenizer.decode(arg[0])\n",
        "print(f\"generated : {generated}\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNWyFtg7uWzL"
      },
      "source": [
        "## 推論"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k20XUiAPuPB9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "inputs = tokenizer([\"summarize: Today is\"], return_tensors=\"pt\")\n",
        "input_length = len(inputs[\"input_ids\"][0])\n",
        "print(f\"inputs_len : {input_length}\")\n",
        "\n",
        "# greedy_search\n",
        "outputs = model.generate(**inputs, max_new_tokens=15, return_dict_in_generate=True, output_scores=True)\n",
        "# beam_search\n",
        "# outputs = model.generate(\n",
        "#     **inputs,\n",
        "#     max_new_tokens=5,\n",
        "#     num_beams=4,\n",
        "#     num_return_sequences=1,\n",
        "#     return_dict_in_generate=True,\n",
        "#     output_scores=True,\n",
        "# )\n",
        "print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "tokens_list = outputs.sequences[0]\n",
        "print(tokens_list)\n",
        "full_text = tokenizer.decode(tokens_list)\n",
        "generated = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"full text : {full_text}\\n\")\n",
        "print(f\"generated text: {generated}\\n\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoDbJrtWKA8k"
      },
      "source": [
        "# BART\n",
        "- add_special_token=Trueにしたとき、先頭に`<s>`が追加されるため-100のマスキング時にinput_length+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H9d_ni0KDdO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "print(\"Before\")\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "special_tokens_dict = {\n",
        "    'additional_special_tokens': ['[EXP]']\n",
        "    }\n",
        "\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "exp_id = tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "eos_id = tokenizer.convert_tokens_to_ids('</s>')\n",
        "pad_id = tokenizer.convert_tokens_to_ids('<pad>')\n",
        "print(\"After\")\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "print(f\"exp_id : {exp_id}\")\n",
        "print(f\"eos_id : {eos_id}\")\n",
        "print(f\"pad_id : {pad_id}\\n\")\n",
        "print(f\"len(tokenizer) : {len(tokenizer)}\")\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
        "# print(f\"before : {model.lm_head.weight.shape}\")\n",
        "# print(f\"before resizing : {model.lm_head.weight}\")\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# print(\"after : \", model.lm_head.weight.shape)\n",
        "# print(f\"after resizing : {model.lm_head.weight.shape}\")\n",
        "# print(f\"before changing weights : {model.lm_head.weight.shape}\")\n",
        "# print(f\"focus : {model.lm_head.weight[:-1, :]}\")\n",
        "# print(f\"focus.shape : {model.lm_head.weight[:-1, :].shape}\")\n",
        "# new_weights = torch.cat([model.lm_head.weight[:-1, :], torch.zeros(1, model.lm_head.weight.shape[1]) -10000])\n",
        "# model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "# print(f\"after changing weights : {model.lm_head.weight}\")\n",
        "# print(f\"new_weights.shape : {model.lm_head.weight.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS-7CkYAKTI0"
      },
      "outputs": [],
      "source": [
        "input_text = 'Statement: When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration. Metadata: energy,history,job-accomplishments scott-surovell State delegate Virginia democrat a floor speech.'+ tokenizer.convert_ids_to_tokens(exp_id)\n",
        "justification_text = 'Surovell said the decline of coal \"started when natural gas took off  That started to begin in President (George W. ) Bushs administration. \"No doubt, natural gas has been gaining ground on coal in generating electricity. The trend started in the 1990s but clearly gained speed during the Bush administration when the production of natural gas -- a competitor of coal -- picked up. But analysts give little credit or blame to Bush for that trend. They note that other factors, such as technological innovation, entrepreneurship and policies of previous administrations, had more to do with laying the groundwork for the natural gas boom.'\n",
        "label_text = input_text  + justification_text + tokenizer.convert_ids_to_tokens(eos_id)\n",
        "print(f\"tokenized input_text : {tokenizer.tokenize(input_text)}\\n\")\n",
        "print(f\"tokenized label_text : {tokenizer.tokenize(label_text)}\")\n",
        "input_text_length = len(tokenizer.tokenize(input_text))\n",
        "justification_text_length = len(tokenizer.tokenize(justification_text))\n",
        "label_text_length = len(tokenizer.tokenize(label_text))\n",
        "print(f\"input_text_length : {input_text_length}\")\n",
        "\n",
        "input_encoding = tokenizer.encode_plus(\n",
        "    input_text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=label_text_length,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "\n",
        "print(f\"input_encoding : {input_encoding}\\n\")\n",
        "decoded_input_encoding = tokenizer.decode(input_encoding[\"input_ids\"][0])\n",
        "# print(f\"decoded input_encoding : {decoded_input_encoding}\\n\")\n",
        "\n",
        "label_encoding = tokenizer.encode_plus(\n",
        "    label_text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=label_text_length,\n",
        "    return_token_type_ids=False,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "\n",
        "print(f\"label_encoding : {label_encoding}\")\n",
        "# print(f\"label_encoding['input_ids'][0] : {label_encoding['input_ids'][0]}\")\n",
        "decoded_label_encoding = tokenizer.decode(label_encoding[\"input_ids\"][0])\n",
        "# print(f\"decoded label_encoding : {decoded_label_encoding}\")\n",
        "print(f\"input prompt : {decoded_input_encoding}\")\n",
        "print(f\"label : {decoded_label_encoding}\")\n",
        "\n",
        "labels = label_encoding[\"input_ids\"]\n",
        "print(f\"labels before -100 masking : {labels}\")\n",
        "# decoded_labels = tokenizer.decode(labels)\n",
        "# print(f\"decoded labels : {decoded_labels}\")\n",
        "labels[:, :input_text_length+1] = -100 # cross_entropy_ignore_index\n",
        "labels[:, label_text_length:] = -100\n",
        "print(f\"labels after -100 masking : {labels}\")\n",
        "print(f\"labels.shape : {labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fy4pRgFbKYRU"
      },
      "outputs": [],
      "source": [
        "outputs = model(input_ids=input_encoding[\"input_ids\"], attention_mask=input_encoding[\"attention_mask\"], labels=labels)\n",
        "\n",
        "print(f\"model : {model}\")\n",
        "print(f\"type(model) : {type(model)}\")\n",
        "print(f\"type(outputs) : {type(outputs)}\")\n",
        "\n",
        "logits = outputs.logits\n",
        "loss = outputs.loss\n",
        "print(f\"logits : {logits}\")\n",
        "print(f\"logtis.size() : {logits.size()}\")\n",
        "print(f\"loss : {loss}\")\n",
        "\n",
        "m = nn.Softmax(dim=1)\n",
        "output = m(logits)\n",
        "print(f\"output : {output}\")\n",
        "print(f\"output.size() : {output.size()}\")\n",
        "arg = torch.argmax(output, dim=2)\n",
        "print(f\"argmax(ouput) : {arg}\") # --> tensor\n",
        "print(f\"argmax(ouput).size() : {arg.size()}\")\n",
        "print(f\"argmax(output).dtype() : {arg.dtype}\")\n",
        "print(arg)\n",
        "generated = tokenizer.decode(arg[0])\n",
        "print(f\"generated : {generated}\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ_6sUIJtXs5"
      },
      "source": [
        "## 推論"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOaCTJIdtZ_R"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMpTxZ9aBJBrlY2KO272ol1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01e88a5b3db74010a5fd150e64051fa1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "079ee46d09504009912320d8bc91bd42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07be893e207a4cc8935b919535103f1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "084b44f111704ebca35200d2e6bd0bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09b4cb8291d840e0a24eebcb31da1f27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b5235061b3b4493bfe59231bf066e2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9317c19186d9494ba5455a4d447fbf9c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f19258de8c84ec89ce50374def183b0",
            "value": 2
          }
        },
        "0d5896edc4424643b48e4f3b05e0fdf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e4d1354dfe242dc961c26ce7beec1d2",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_740d322889fe4c4e9d9b7fa933aac879",
            "value": 2
          }
        },
        "0fc0c943783742239ea1a50719e33c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "112461129b934fb3b66ab01137572d97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14bac1ea51aa4576b2a0309ddb17f2f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "184935b54e8a4b87aab3d111a363ed18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b08b516fa0246288c994a18ec8f05ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddb87dd3af8743bdbe3db049bb122bb4",
            "placeholder": "​",
            "style": "IPY_MODEL_3111ed49c6484415b4bd1ca3487b9b41",
            "value": "100%"
          }
        },
        "1ba4ebabe00142e9ab4b6919b8b0abff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cdfd7e4c7dc4da486a0a10f08adf1f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d397e7f11f745c58ed10197a9fca7db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "203542b28df0430aaafc3d9c6fa91a7a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2174a3c69d2a4ce29c4e6e85a2774659": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b08b516fa0246288c994a18ec8f05ac",
              "IPY_MODEL_abc50890c49c429fb46674e8d99043e8",
              "IPY_MODEL_2406e250ac914a659cb02b4e3f22ba3e"
            ],
            "layout": "IPY_MODEL_a5e29470ff2d4774b70a71523e7f8d49"
          }
        },
        "2406e250ac914a659cb02b4e3f22ba3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14bac1ea51aa4576b2a0309ddb17f2f7",
            "placeholder": "​",
            "style": "IPY_MODEL_f18b8df6abb94cebaa87b233b49b8e3f",
            "value": " 2/2 [00:17&lt;00:00,  8.60s/it]"
          }
        },
        "24abfe23f6f842869c85c46591ee209b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2740458968a04efe884a67d587f85ca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e0216464ed5408b88d27df3f26b40a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b3609b4352d4171b782adc440b8b918",
            "placeholder": "​",
            "style": "IPY_MODEL_079ee46d09504009912320d8bc91bd42",
            "value": " 456k/456k [00:00&lt;00:00, 1.85MB/s]"
          }
        },
        "2f19258de8c84ec89ce50374def183b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f963ef10cb649f8b92133c9c39d298c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47b9df4fdf44407086a938cb58553a16",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d952235a771e4eefbdb4d617b64123f9",
            "value": 2
          }
        },
        "3111ed49c6484415b4bd1ca3487b9b41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31ac9735384d4b47aa6d0eac5efe8b92": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "320ecc172af3413c8fc1bf515c0fabd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "335647d8c4194dd3ad3ed3537d1b0cad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "336c311af77247128d763bed2bf78644": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "337511d2bb20467b86961d9fd225bea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bcb878c77b7b47a890989b03ea6adc75",
              "IPY_MODEL_54b867fd8a59467fbbe05e8bece67856",
              "IPY_MODEL_e8ffb55a6edc495aa6ff4d4ee07de503"
            ],
            "layout": "IPY_MODEL_9dbfd32ab2074cbfb50ec6c14605550e"
          }
        },
        "33dc31586eae46939bde5d5c549fabc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "343f0b28433e48f6b4892cbaab4713c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_402cc7fc63ee46e1916ef318b4eb9afa",
            "placeholder": "​",
            "style": "IPY_MODEL_bb02c559503d442384b9e8060767f34b",
            "value": " 899k/899k [00:00&lt;00:00, 2.74MB/s]"
          }
        },
        "356902efe1294e78831a8e52e5010d49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a36e8349425485797581c45903f843a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a4d3f0bb23846daae423d3421c14b6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b970558292542e4ac14edb19fa09d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9a1a293849d48ae97f48b0854e0b9a4",
            "placeholder": "​",
            "style": "IPY_MODEL_abe6b4e3eedf41d0a83fb8c8f14d00da",
            "value": "merges.txt: 100%"
          }
        },
        "3c013906e62a4a059eca6403385fcaa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_112461129b934fb3b66ab01137572d97",
            "placeholder": "​",
            "style": "IPY_MODEL_3d4bba1a9ccb4f0494568c799674e95e",
            "value": "100%"
          }
        },
        "3d4bba1a9ccb4f0494568c799674e95e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ed4c6a7319a438fbd0fa49c4c5e0107": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45c9f350b4504650a9b70e46ce6fa04f",
            "placeholder": "​",
            "style": "IPY_MODEL_6a3dbc4e7d5249f2adfa15fad5bf1917",
            "value": " 2/2 [00:16&lt;00:00,  8.30s/it]"
          }
        },
        "3f46711e2165438fa51ce5b68adea86d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "402cc7fc63ee46e1916ef318b4eb9afa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "403d9cb7cb0343169f4438737b7b2faa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b7b081ad5684f55a9781be354893436",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c328ad0925ff40e4978bfa930ab5d288",
            "value": 1042301
          }
        },
        "424da0eedeee4d0e958badb08d2e29a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42cec859b5134df5a42d2aa943d1df8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb26090f1f694de698538c3bb1b851ba",
            "placeholder": "​",
            "style": "IPY_MODEL_76b02720217a4c55bdc4d36c3bf228e8",
            "value": "100%"
          }
        },
        "42f30a52d0644cbd8562b1b8dc7e2f8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45c9f350b4504650a9b70e46ce6fa04f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4698435907b44a73a2466e0164ae4816": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4731c09d14a7467e91c27698db2a1233": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47b9df4fdf44407086a938cb58553a16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48290b59993e4c1ba86b9f08c7eee690": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4853414ef87740ea9595d2f882513f7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_335647d8c4194dd3ad3ed3537d1b0cad",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_827eeeb190fb48268325eff24d0c0777",
            "value": 1355863
          }
        },
        "4b35b046cde1483ea297165c018a0ef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01e88a5b3db74010a5fd150e64051fa1",
            "max": 1284,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efeedef7100546febc51aa04c47a2bd3",
            "value": 234
          }
        },
        "4ea9647ea3444bb1b7c444343d8f4786": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5047a38d58b149d39a4aae8c5f0d2bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51f982844a3d4b6397a039941d6a054e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "543646e3f7e140e6908fbdf35be07c36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be8d440415d14fb39c1662ca326aad7d",
            "placeholder": "​",
            "style": "IPY_MODEL_0fc0c943783742239ea1a50719e33c7c",
            "value": "vocab.json: 100%"
          }
        },
        "54b867fd8a59467fbbe05e8bece67856": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4731c09d14a7467e91c27698db2a1233",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_184935b54e8a4b87aab3d111a363ed18",
            "value": 1355256
          }
        },
        "5547641ba8b8456b96d4892386856b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56d061ef96bb4dd58e5a4ac06f48161f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56e65705ece94837b896e345c377a49a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09b4cb8291d840e0a24eebcb31da1f27",
            "placeholder": "​",
            "style": "IPY_MODEL_668296031bf54b9ab351d7c9b22107a0",
            "value": " 2/2 [00:16&lt;00:00,  8.09s/it]"
          }
        },
        "578ef8349a7148e6ba9cd337159167fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ccb6ac714fd4e8992cef811ed152c86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e26e3143345f4c2ca6c14c67c6096a7d",
              "IPY_MODEL_4853414ef87740ea9595d2f882513f7d",
              "IPY_MODEL_64d0076183204cf096517704c58e0fdc"
            ],
            "layout": "IPY_MODEL_8c637f84ca0144a4b74867b712c836cc"
          }
        },
        "5d75663e886641fe8ecec4719952ac07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61185d80491b498381adef3ad89643f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42cec859b5134df5a42d2aa943d1df8a",
              "IPY_MODEL_2f963ef10cb649f8b92133c9c39d298c",
              "IPY_MODEL_f43e0b75ac8b40e6919f7caa21d70ebd"
            ],
            "layout": "IPY_MODEL_3a36e8349425485797581c45903f843a"
          }
        },
        "64d0076183204cf096517704c58e0fdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7ef9fc3af3c433695b2943410320177",
            "placeholder": "​",
            "style": "IPY_MODEL_1ba4ebabe00142e9ab4b6919b8b0abff",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 3.29MB/s]"
          }
        },
        "668296031bf54b9ab351d7c9b22107a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "668fc6e5630f41428d050e691f481b6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69413b37ed4941d0a04946135ba2bfe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fef7b87d12bb4cfaae1e9c14966df5b7",
              "IPY_MODEL_4b35b046cde1483ea297165c018a0ef4",
              "IPY_MODEL_b5f8550db5c74639a41ca06b78a7d183"
            ],
            "layout": "IPY_MODEL_cca837a0a9ce433b99c14da4804055fc"
          }
        },
        "697192d6142d4de4b904928e9e748dd6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a3dbc4e7d5249f2adfa15fad5bf1917": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b3609b4352d4171b782adc440b8b918": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d9fc398f7294169914868322dc32f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b27de78c01a342b3b8725b709579e605",
              "IPY_MODEL_8cff50a7fd434bc19aef755138d44945",
              "IPY_MODEL_795e77db32c74a9da820a0675bec2c6d"
            ],
            "layout": "IPY_MODEL_e10e410e9189448096cd62fc7c247aa5"
          }
        },
        "6de90f3a7de44e6388aa43d8d41fb804": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "740d322889fe4c4e9d9b7fa933aac879": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76b02720217a4c55bdc4d36c3bf228e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "795e77db32c74a9da820a0675bec2c6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_578ef8349a7148e6ba9cd337159167fd",
            "placeholder": "​",
            "style": "IPY_MODEL_c7ab92038df64177b69ed09cf12d9fff",
            "value": " 665/665 [00:00&lt;00:00, 58.5kB/s]"
          }
        },
        "7c43fa28ad4c4049baca903f4616ce40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_543646e3f7e140e6908fbdf35be07c36",
              "IPY_MODEL_827c189ccfb64369b4cab8521604b7de",
              "IPY_MODEL_343f0b28433e48f6b4892cbaab4713c3"
            ],
            "layout": "IPY_MODEL_424da0eedeee4d0e958badb08d2e29a9"
          }
        },
        "7e4d1354dfe242dc961c26ce7beec1d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80dbb99875f641b78bf12b775d657520": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8168e0225eab40a5864b2cd7ee15c04f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "827c189ccfb64369b4cab8521604b7de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4698435907b44a73a2466e0164ae4816",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_320ecc172af3413c8fc1bf515c0fabd5",
            "value": 898823
          }
        },
        "827eeeb190fb48268325eff24d0c0777": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83cc2fa0f9cb4d38a520753889ac819c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7da356aa2ca433ca0ca8bc78d57b560",
              "IPY_MODEL_dd3fbf474aa24d9387db1271bb79fad3",
              "IPY_MODEL_ffce395f0f574842a6328b950e9dfcff"
            ],
            "layout": "IPY_MODEL_1d397e7f11f745c58ed10197a9fca7db"
          }
        },
        "84ae34ba04eb47efb001444d17d9208c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f12086296f554cb883d96f8d5c53848e",
            "placeholder": "​",
            "style": "IPY_MODEL_336c311af77247128d763bed2bf78644",
            "value": " 1.72k/1.72k [00:00&lt;00:00, 133kB/s]"
          }
        },
        "87855838eb6b496daf6f41d2c4380e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07be893e207a4cc8935b919535103f1a",
            "placeholder": "​",
            "style": "IPY_MODEL_8168e0225eab40a5864b2cd7ee15c04f",
            "value": "config.json: 100%"
          }
        },
        "8a14f35c1b1548d08b6a6088e0a49cd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ace2a8668404f129c1e0b071040446d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c013906e62a4a059eca6403385fcaa2",
              "IPY_MODEL_0b5235061b3b4493bfe59231bf066e2e",
              "IPY_MODEL_56e65705ece94837b896e345c377a49a"
            ],
            "layout": "IPY_MODEL_3a4d3f0bb23846daae423d3421c14b6d"
          }
        },
        "8c637f84ca0144a4b74867b712c836cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cff50a7fd434bc19aef755138d44945": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f46711e2165438fa51ce5b68adea86d",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_356902efe1294e78831a8e52e5010d49",
            "value": 665
          }
        },
        "9317c19186d9494ba5455a4d447fbf9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9383b399472d4672a9f0fc571a905647": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6de90f3a7de44e6388aa43d8d41fb804",
            "placeholder": "​",
            "style": "IPY_MODEL_9699163d6fc24d8da24ea473ae99072f",
            "value": "vocab.json: 100%"
          }
        },
        "94c8a110a9ff4a7d92c8d7605612569d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9699163d6fc24d8da24ea473ae99072f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9819fea12769418a94e43ceff1c34315": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b7b081ad5684f55a9781be354893436": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d135fc2e195444cbda257f153a686e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9dbfd32ab2074cbfb50ec6c14605550e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a28cfc6e6d4b408daff391bafb0d12bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a322309165cc4615b7e6cdb1a49bbead": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5e29470ff2d4774b70a71523e7f8d49": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abc50890c49c429fb46674e8d99043e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80dbb99875f641b78bf12b775d657520",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d135fc2e195444cbda257f153a686e1",
            "value": 2
          }
        },
        "abe6b4e3eedf41d0a83fb8c8f14d00da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae267a1e90374fccb2334d5338b9bda2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87855838eb6b496daf6f41d2c4380e1b",
              "IPY_MODEL_b99d8597139643a9b8a14357035c30b2",
              "IPY_MODEL_84ae34ba04eb47efb001444d17d9208c"
            ],
            "layout": "IPY_MODEL_f481daaa5d404c16aba82dde2a1ea010"
          }
        },
        "b09e5caf7ee244fa8d6f6ddff906183e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42f30a52d0644cbd8562b1b8dc7e2f8e",
            "placeholder": "​",
            "style": "IPY_MODEL_c36df41737f94021aa7d88a0742a4b05",
            "value": "100%"
          }
        },
        "b27de78c01a342b3b8725b709579e605": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51f982844a3d4b6397a039941d6a054e",
            "placeholder": "​",
            "style": "IPY_MODEL_f9bb6dae3d604c378b355a49edd628d9",
            "value": "config.json: 100%"
          }
        },
        "b5f8550db5c74639a41ca06b78a7d183": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_697192d6142d4de4b904928e9e748dd6",
            "placeholder": "​",
            "style": "IPY_MODEL_d766eae7cae1403796fe67ec79871d20",
            "value": " 234/1284 [32:46&lt;2:27:42,  8.44s/it]"
          }
        },
        "b661c05a1447436fba7f785946a61b2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56d061ef96bb4dd58e5a4ac06f48161f",
            "placeholder": "​",
            "style": "IPY_MODEL_33dc31586eae46939bde5d5c549fabc6",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 1.31MB/s]"
          }
        },
        "b7da356aa2ca433ca0ca8bc78d57b560": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cac40f541b004442ae14dec9ea72c89e",
            "placeholder": "​",
            "style": "IPY_MODEL_5047a38d58b149d39a4aae8c5f0d2bf7",
            "value": "merges.txt: 100%"
          }
        },
        "b99d8597139643a9b8a14357035c30b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_203542b28df0430aaafc3d9c6fa91a7a",
            "max": 1716,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5547641ba8b8456b96d4892386856b07",
            "value": 1716
          }
        },
        "bb02c559503d442384b9e8060767f34b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb26090f1f694de698538c3bb1b851ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcb878c77b7b47a890989b03ea6adc75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de919087219b46d58f2407daa54d1008",
            "placeholder": "​",
            "style": "IPY_MODEL_5d75663e886641fe8ecec4719952ac07",
            "value": "tokenizer.json: 100%"
          }
        },
        "be8d440415d14fb39c1662ca326aad7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1fdd0ee575e429195cbc70ad049e4e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c328ad0925ff40e4978bfa930ab5d288": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c36df41737f94021aa7d88a0742a4b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7ab92038df64177b69ed09cf12d9fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cac40f541b004442ae14dec9ea72c89e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cca837a0a9ce433b99c14da4804055fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce5f6bbe3d5f4892a8c8ed89741fd8ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d766eae7cae1403796fe67ec79871d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7ef9fc3af3c433695b2943410320177": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d952235a771e4eefbdb4d617b64123f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd3fbf474aa24d9387db1271bb79fad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_668fc6e5630f41428d050e691f481b6d",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a14f35c1b1548d08b6a6088e0a49cd5",
            "value": 456318
          }
        },
        "ddb87dd3af8743bdbe3db049bb122bb4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de919087219b46d58f2407daa54d1008": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e10e410e9189448096cd62fc7c247aa5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e26e3143345f4c2ca6c14c67c6096a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9819fea12769418a94e43ceff1c34315",
            "placeholder": "​",
            "style": "IPY_MODEL_1cdfd7e4c7dc4da486a0a10f08adf1f8",
            "value": "tokenizer.json: 100%"
          }
        },
        "e36228ec34d04d9a867b0ca9580a7b60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b09e5caf7ee244fa8d6f6ddff906183e",
              "IPY_MODEL_0d5896edc4424643b48e4f3b05e0fdf6",
              "IPY_MODEL_3ed4c6a7319a438fbd0fa49c4c5e0107"
            ],
            "layout": "IPY_MODEL_c1fdd0ee575e429195cbc70ad049e4e8"
          }
        },
        "e89576f90d2442419416527d3cadf719": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8ffb55a6edc495aa6ff4d4ee07de503": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1425f9d8d5b4a96ba39042ba3f071b1",
            "placeholder": "​",
            "style": "IPY_MODEL_2740458968a04efe884a67d587f85ca1",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 6.84MB/s]"
          }
        },
        "e9119dfa49724cc7887aeda27b87419b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b970558292542e4ac14edb19fa09d76",
              "IPY_MODEL_ead79d672be445e19b65d5da41c38dde",
              "IPY_MODEL_2e0216464ed5408b88d27df3f26b40a0"
            ],
            "layout": "IPY_MODEL_ce5f6bbe3d5f4892a8c8ed89741fd8ac"
          }
        },
        "ead79d672be445e19b65d5da41c38dde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31ac9735384d4b47aa6d0eac5efe8b92",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a322309165cc4615b7e6cdb1a49bbead",
            "value": 456318
          }
        },
        "efeedef7100546febc51aa04c47a2bd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f12086296f554cb883d96f8d5c53848e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1425f9d8d5b4a96ba39042ba3f071b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f18b8df6abb94cebaa87b233b49b8e3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f322c82941cf416fbca58ef3f9081876": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9383b399472d4672a9f0fc571a905647",
              "IPY_MODEL_403d9cb7cb0343169f4438737b7b2faa",
              "IPY_MODEL_b661c05a1447436fba7f785946a61b2b"
            ],
            "layout": "IPY_MODEL_e89576f90d2442419416527d3cadf719"
          }
        },
        "f43e0b75ac8b40e6919f7caa21d70ebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24abfe23f6f842869c85c46591ee209b",
            "placeholder": "​",
            "style": "IPY_MODEL_94c8a110a9ff4a7d92c8d7605612569d",
            "value": " 2/2 [00:16&lt;00:00,  8.36s/it]"
          }
        },
        "f481daaa5d404c16aba82dde2a1ea010": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9a1a293849d48ae97f48b0854e0b9a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9bb6dae3d604c378b355a49edd628d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fef7b87d12bb4cfaae1e9c14966df5b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48290b59993e4c1ba86b9f08c7eee690",
            "placeholder": "​",
            "style": "IPY_MODEL_a28cfc6e6d4b408daff391bafb0d12bc",
            "value": "  0%"
          }
        },
        "ffce395f0f574842a6328b950e9dfcff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ea9647ea3444bb1b7c444343d8f4786",
            "placeholder": "​",
            "style": "IPY_MODEL_084b44f111704ebca35200d2e6bd0bb8",
            "value": " 456k/456k [00:00&lt;00:00, 782kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}