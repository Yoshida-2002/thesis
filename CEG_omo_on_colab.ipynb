{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yoshida-2002/thesis/blob/main/CEG_omo_on_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "Lefh-Mu8KRcM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a5f88a-55cb-4c3d-b6d6-46f74e3a2122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "aSs33HuiK1M6",
        "outputId": "358cd6d0-9284-4056-f112-da5ad55ed9bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning\n",
            "' CEG omo on colab'           'data for CEG'     models                \u001b[0m\u001b[01;34mplot_figures\u001b[0m/\n",
            "' CEG omo on colab のコピー'   \u001b[01;34mlightning_logs\u001b[0m/  'models のコピー'      \u001b[01;34mresult_log\u001b[0m/\n",
            " \u001b[01;34mdata\u001b[0m/                         \u001b[01;34mmodel\u001b[0m/            \u001b[01;34mmodel_transformers\u001b[0m/   \u001b[01;34msrc\u001b[0m/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 194
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning\n",
        "%ls\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "jRrJ4uFkLMCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0814beca-c82a-4bc2-9d81-78445b52fdaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.3.0.post0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch-lightning) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch-lightning) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pathを辞書にまとめて管理する"
      ],
      "metadata": {
        "id": "_fm0w0jc01Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_dict = {\n",
        "    # 生データのパス\n",
        "    'raw_train_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/train2.tsv',\n",
        "    'raw_test_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/test2.tsv',\n",
        "    'raw_val_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/val2.tsv',\n",
        "\n",
        "    # 欠損値処理を行った後のデータのパス\n",
        "    'missing_value_processed_train_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_train2.tsv',\n",
        "    'missing_value_processed_test_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_test2.tsv',\n",
        "    'missing_value_processed_val_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_val2.tsv',\n",
        "\n",
        "    # 欠損値処理を行った後、さらなる整形を行ったデータ\n",
        "    'input_for_lightning_model_train_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_train2.tsv',\n",
        "    'input_for_lightning_model_test_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_test2.tsv',\n",
        "    'input_for_lightning_model_val_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_val2.tsv',\n",
        "\n",
        "    # 集合に分割した後のデータ (main)\n",
        "    'main_input_for_lightning_model_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/train/true_set2.tsv',\n",
        "    'main_input_for_lightning_model_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/train/false_set2.tsv',\n",
        "    'main_input_for_lightning_model_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/test/true_set2.tsv',\n",
        "    'main_input_for_lightning_model_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/test/false_set2.tsv',\n",
        "    'main_input_for_lightning_model_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/val/true_set2.tsv',\n",
        "    'main_input_for_lightning_model_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/val/false_set2.tsv',\n",
        "\n",
        "    # 集合に分割した後のデータ (toy)\n",
        "    'toy_input_for_lightning_model_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/train/true_set2.tsv',\n",
        "    'toy_input_for_lightning_model_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/train/false_set2.tsv',\n",
        "    'toy_input_for_lightning_model_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/test/true_set2.tsv',\n",
        "    'toy_input_for_lightning_model_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/test/false_set2.tsv',\n",
        "    'toy_input_for_lightning_model_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/val/true_set2.tsv',\n",
        "    'toy_input_for_lightning_model_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/val/false_set2.tsv',\n",
        "\n",
        "    # モデルを保存するPath\n",
        "    'saved_model_path' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/src/models',\n",
        "    'TensorBoardLogger': 'lightning_logs',\n",
        "\n",
        "    # 拡張したデータの保存場所(main)\n",
        "    'main_extended_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/train/true_set2.tsv',\n",
        "    'main_extended_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/train/false_set2.tsv',\n",
        "    'main_extended_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/test/true_set2.tsv',\n",
        "    'main_extended_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/test/false_set2.tsv',\n",
        "    'main_extended_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/val/true_set2.tsv',\n",
        "    'main_extended_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/val/false_set2.tsv',\n",
        "\n",
        "\n",
        "    # 拡張したデータの保存場所(main)\n",
        "    'toy_extended_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/train/true_set2.tsv',\n",
        "    'toy_extended_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/train/false_set2.tsv',\n",
        "    'toy_extended_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/test/true_set2.tsv',\n",
        "    'toy_extended_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/test/false_set2.tsv',\n",
        "    'toy_extended_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/val/true_set2.tsv',\n",
        "    'toy_extended_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/val/false_set2.tsv',\n",
        "\n",
        "  }\n"
      ],
      "metadata": {
        "id": "VrElytlN06OI"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-H8IgiRLeom"
      },
      "source": [
        "# data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2W3EWwDNM-t"
      },
      "source": [
        "## data_reader.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "xDRiQFePNLVS"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"data_reader module is written for read files\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def read_csv(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "\n",
        "    :param path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "def read_tsv(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "\n",
        "    :param path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return pd.read_csv(path, sep=\"\\t\", header=None)\n",
        "\n",
        "\n",
        "def read_npy(path: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    load a list of numpy elements into memory\n",
        "    :param path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return np.load(path, allow_pickle=True)\n",
        "\n",
        "\n",
        "def read_excel(path:str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    :param path: where to load data from\n",
        "    :return: pd.DataFrame\n",
        "    \"\"\"\n",
        "    return pd.read_excel(path, engine=\"openpyxl\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sdEJztwNSgt"
      },
      "source": [
        "## data_writer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "od80SyiCNVaE"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"data_writer module is written for write data in files\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def write_npy(path: str, data: list) -> None:\n",
        "    \"\"\"\n",
        "    save a list of numpy elements into disk\n",
        "    :param path:\n",
        "    :param data:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return np.save(path, data, allow_pickle=True)\n",
        "\n",
        "def write_dataframe_in_tsv(data: pd.DataFrame, path: str) -> None:\n",
        "\n",
        "  \"\"\"\n",
        "  save pd.Dataframe in tsv file\n",
        "  :param path:\n",
        "  :param data:\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  data.to_csv(path, sep='\\t', index=False, header=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lROVJGpnfD8U"
      },
      "source": [
        "## data_missing_value_processor\n",
        "- 欠損値の処理を行う\n",
        "- やること\n",
        "  1. 全ての値がNaNの列を削除する\n",
        "  2. NaNを0で補完する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "mFLX1BVOfi1v"
      },
      "outputs": [],
      "source": [
        "def data_missing_value_processor(data: pd.DataFrame, path: str) -> pd.DataFrame:\n",
        "\n",
        "  print('input data size : ' + str(len(data)))\n",
        "\n",
        "  data_without_invalid_row = data.dropna(how=\"all\")\n",
        "  print(str(len(data) - len(data_without_invalid_row)) + \" data was dropped because all of the values were NaN. \")\n",
        "\n",
        "  processed_data = data_without_invalid_row.fillna(0)\n",
        "  print(\"NaN in dataframe was filled with 0. \")\n",
        "  print(\"processed input data size : \" + str(len(processed_data)))\n",
        "  print(\"removed data size : \" + str(len(data) - len(processed_data)) )\n",
        "  print('\\n')\n",
        "\n",
        "  write_dataframe_in_tsv(processed_data, path)\n",
        "\n",
        "  return processed_data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phuOrNG7eh6i"
      },
      "source": [
        "##data_pre_processor_for_LIAR_PLUS_Dataset\n",
        "- LIAR_PLUS_Datasetクラスの入力に用いるdataframeを作成する\n",
        "- 作成したdataframeを保存する\n",
        "- statement, metadata, justification, credit_scoreの四つの列からなるdataframeを出力する\n",
        "- やること\n",
        "  1. 値として0(元々は欠損値)を持つ場合、\"None\"で補完する\n",
        "  2. 列を結合してstaetment, metadata, justification, credit_scoreを作成する\n",
        "  3. credit_scoreのnanを0.5で補完する\n",
        "  4. 作成したdataframeを保存する\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "Y5D6XvWcefqX"
      },
      "outputs": [],
      "source": [
        "def data_pre_processor_for_LIAR_PLUS_Dataset(data: pd.DataFrame, path: str) -> pd.DataFrame:\n",
        "\n",
        "  # 1. 値として0(元々は欠損値)を持つ場合、\"None\"で補完する\n",
        "\n",
        "  data[4].replace(0, 'None', inplace=True)\n",
        "  data[5].replace(0, 'None', inplace=True)\n",
        "  data[6].replace(0, 'None', inplace=True)\n",
        "  data[7].replace(0, 'None', inplace=True)\n",
        "  data[8].replace(0, 'None', inplace=True)\n",
        "  data[14].replace(0, 'None', inplace=True)\n",
        "  data[15].replace(0, 'None', inplace=True)\n",
        "\n",
        "  # 2. 列を結合してstaetment, metadata, justificationを作成する\n",
        "\n",
        "  data_for_LIAR_PLUS_Dataset = pd.DataFrame(\n",
        "      data = {\n",
        "      'id' : data[0],\n",
        "      'label' : data[2],\n",
        "      'statement' : data[3],\n",
        "      'metadata' : data[4]+ ' ' + data[5] + ' ' + data[6] + ' ' + data[7] + ' ' + data[8] + ' ' + data[14],\n",
        "      'justification' : data[15],\n",
        "      'credit_score' : (data[12]*0.2 + data[11]*0.5 + data[9]*0.75 + data[10]*0.9 + data[13]*1)/(data[12] + data[11] + data[9] + data[10] + data[13]),\n",
        "      'unique_id' : data[1]\n",
        "      })\n",
        "\n",
        "  data_for_LIAR_PLUS_Dataset['credit_score'] = data_for_LIAR_PLUS_Dataset['credit_score'].fillna(0.5)\n",
        "\n",
        "  write_dataframe_in_tsv(data_for_LIAR_PLUS_Dataset, path)\n",
        "\n",
        "  print(\"dataframe was saved in\" + path + \"\\n\")\n",
        "\n",
        "  data_for_LIAR_PLUS_Dataset.info()\n",
        "\n",
        "\n",
        "  return data_for_LIAR_PLUS_Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeBeWogIddoI"
      },
      "source": [
        "# dataset_separator_on_labels\n",
        "- プロンプトを作成する前にラベルで条件付けられた集合を作成\n",
        "- 名前以外はgenerated_prompt_separatorと同じ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "mHRtX4hxdtCE"
      },
      "outputs": [],
      "source": [
        "def dataset_separator_on_labels(path: dict, data: pd.DataFrame) -> dict:\n",
        "\n",
        "  true_set = pd.DataFrame()\n",
        "  false_set = pd.DataFrame()\n",
        "  total_len = len(data)\n",
        "\n",
        "  for i in range(len(data)):\n",
        "\n",
        "    data_row = data.iloc[i]\n",
        "   # print(data_row)\n",
        "\n",
        "    if data_row[1] == \"true\":\n",
        "        true_set = true_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"half-true\":\n",
        "        true_set = true_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"mostly-true\":\n",
        "        true_set = true_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"false\":\n",
        "        false_set = false_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"barely-true\":\n",
        "        false_set = false_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"pants-fire\":\n",
        "        false_set = false_set.append(data_row, ignore_index=True)\n",
        "    else:\n",
        "        print(f\"Invalid label was found! {data_row[1]}\")\n",
        "\n",
        "  print(\"===================================================\\n\")\n",
        "  print(f\"total len : {total_len}\")\n",
        "  print(f\"true_set : {len(true_set)}\")\n",
        "  print(f\"false_set : {len(false_set)}\")\n",
        "  print(\"===================================================\\n\")\n",
        "\n",
        "  write_dataframe_in_tsv(true_set, path[\"true_set\"])\n",
        "  write_dataframe_in_tsv(false_set, path[\"false_set\"])\n",
        "\n",
        "\n",
        "  return dict(\n",
        "      true_set=true_set,\n",
        "      false_set=true_set\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbE1iUnbvV1w"
      },
      "source": [
        "# prompt_generator\n",
        "・プロンプトの作成を一任\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "yghLEX3Uvc53"
      },
      "outputs": [],
      "source": [
        "# def prompt_generator(data: pd.DataFrame, path :str) -> pd.DataFrame :\n",
        "#   \"\"\"\n",
        "#   this function is for generating and saving prompts\n",
        "\n",
        "#   param  : stage(\"fine-tuning\" of \"completion\")\n",
        "#   param  : data(row data for prompt generation)\n",
        "#   param  : path(path to save generated promps dataframe)\n",
        "#   return : prompts(generated prompts)\n",
        "#   \"\"\"\n",
        "#   generated_prompts = \"Statement: \" + data[2] + \" Metadata: \" + data[3]\n",
        "\n",
        "#   prompts_df = pd.DataFrame(\n",
        "#       data = {\n",
        "#       'id' : data[0],\n",
        "#       'label' : data[1],\n",
        "#       'concat' : generated_prompts,\n",
        "#       'justification': data[4],\n",
        "#       'credit_score' : data[5],\n",
        "#       'unique_id' : data[6]\n",
        "#       })\n",
        "\n",
        "#   write_dataframe_in_tsv(prompts_df, path)\n",
        "\n",
        "#   return prompts_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "XMKn8TNcYi1g"
      },
      "outputs": [],
      "source": [
        "# def prompt_generator(data: pd.DataFrame, path:str, config_dict:dict) -> pd.DataFrame :\n",
        "#   \"\"\"\n",
        "#   this function is for generating and saving prompts\n",
        "\n",
        "#   param  : stage(\"fine-tuning\" of \"completion\")\n",
        "#   param  : data(row data for prompt generation)\n",
        "#   param  : path(path to save generated promps dataframe)\n",
        "#   return : prompts(generated prompts)\n",
        "#   \"\"\"\n",
        "\n",
        "\n",
        "#   generated_prompts = ''\n",
        "\n",
        "#   if 'statement' in config_dict['PROMPT_COMPONENTS']:\n",
        "#     generated_prompts = 'Statement: ' + data[2]\n",
        "\n",
        "#   if 'metadata' in config_dict['PROMPT_COMPONENTS']:\n",
        "#     generated_prompts += \" Metadata: \" + data[3]\n",
        "\n",
        "#   prompts_df = pd.DataFrame(\n",
        "#       data = {\n",
        "#       'id' : data[0],\n",
        "#       'label' : data[1],\n",
        "#       'concat' : generated_prompts,\n",
        "#       'justification': data[4],\n",
        "#       'credit_score' : data[5],\n",
        "#       'unique_id' : data[6]\n",
        "#       })\n",
        "\n",
        "#   return prompts_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UubLg77lkNfo"
      },
      "source": [
        "# concatinated_inputs_generator\n",
        "- concatとfull-promptを作成し諸々の情報を返す\n",
        "- 例外処理のコードはまだ書いていない"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "WVwpH7jzkvku"
      },
      "outputs": [],
      "source": [
        "def concatinated_inputs_generator(data: pd.Series, config_dict:dict, tokenizer, prepare_tokenizer, input_flip) -> dict :\n",
        "\n",
        "  concatinated_input = ''\n",
        "  full_input = ''\n",
        "\n",
        "  if input_flip == 0:\n",
        "\n",
        "    if 'statement' in config_dict['PROMPT_COMPONENTS']:\n",
        "        concatinated_input = 'Statement: ' + data[2]\n",
        "    if 'metadata' in config_dict['PROMPT_COMPONENTS']:\n",
        "        concatinated_input += ' Metadata: ' + data[3]\n",
        "    concatinated_input += prepare_tokenizer.exp_token\n",
        "    concatinated_input_length = len(tokenizer.tokenize(concatinated_input))\n",
        "    if 'justification' in config_dict['PROMPT_COMPONENTS']:\n",
        "        full_input = concatinated_input + data[4] + prepare_tokenizer.eos_token\n",
        "\n",
        "  elif input_flip == 1:\n",
        "\n",
        "    if 'metadata' in config_dict['PROMPT_COMPONENTS']:\n",
        "        concatinated_input += 'Metadata: ' + data[3]\n",
        "    if 'statement' in config_dict['PROMPT_COMPONENTS']:\n",
        "        concatinated_input += ' Statement: ' + data[2]\n",
        "    concatinated_input += prepare_tokenizer.exp_token\n",
        "    concatinated_input_length = len(tokenizer.tokenize(concatinated_input))\n",
        "    if 'justification' in config_dict['PROMPT_COMPONENTS']:\n",
        "        full_input = concatinated_input + data[4] + prepare_tokenizer.eos_token\n",
        "\n",
        "  else:\n",
        "    ValueError('wrong input_flip value')\n",
        "\n",
        "\n",
        "  full_input_length = len(tokenizer.tokenize(full_input))\n",
        "\n",
        "  # bartではspecial_token<s>が文頭に付くため、その分だけ入力長が1大きくなる\n",
        "  if config_dict[\"MODEL_NAME\"] == 'facebook/bart-base':\n",
        "      concatinated_input_length = concatinated_input_length + 1\n",
        "\n",
        "  if concatinated_input_length > config_dict['MAX_LENGTH']:\n",
        "      concatinated_input_length = config_dict['MAX_LENGTH']\n",
        "\n",
        "  if full_input_length > config_dict['MAX_LENGTH']:\n",
        "      full_input_length = config_dict['MAX_LENGTH']\n",
        "\n",
        "  print(f'concatinated_input : {concatinated_input}')\n",
        "  print(f'full_input_length : {full_input_length}')\n",
        "\n",
        "\n",
        "  return concatinated_input, full_input, concatinated_input_length, full_input_length\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "nlnS8xxD3yZv"
      },
      "outputs": [],
      "source": [
        "# train_prompt_path = \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts/main/train_prompts.tsv\"\n",
        "# test_prompt_path = \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts/main/test_prompts.tsv\"\n",
        "# val_prompt_path = \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts/main/val_prompts.tsv\"\n",
        "\n",
        "# train_prompts = read_tsv(\"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/main/input_train2.tsv\")\n",
        "# test_prompts = read_tsv(\"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/main/input_test2.tsv\")\n",
        "# val_prompts = read_tsv(\"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/main/input_val2.tsv\")\n",
        "\n",
        "# train_prompts = prompt_generator(train_prompts, train_prompt_path)\n",
        "# test_prompts = prompt_generator(test_prompts, test_prompt_path)\n",
        "# val_prompts = prompt_generator(val_prompts, val_prompt_path)\n",
        "\n",
        "# print(len(train_prompts))\n",
        "# train_prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "pQTF6thEEeYI"
      },
      "outputs": [],
      "source": [
        "# x = read_tsv(train_prompt_path)\n",
        "# x.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMUYg-qvy_Yw"
      },
      "source": [
        "# generated_prompts_separator\n",
        "- ラベルごとに分類する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "Rz-tzHX9B1yv"
      },
      "outputs": [],
      "source": [
        "# def generated_prompts_separator(path: dict, data: pd.DataFrame) -> dict:\n",
        "\n",
        "#   true_set = pd.DataFrame()\n",
        "#   false_set = pd.DataFrame()\n",
        "#   total_len = len(data)\n",
        "\n",
        "#   for i in range(len(data)):\n",
        "\n",
        "#     data_row = data.iloc[i]\n",
        "#    # print(data_row)\n",
        "\n",
        "#     if data_row[1] == \"true\":\n",
        "#         true_set = true_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"half-true\":\n",
        "#         true_set = true_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"mostly-true\":\n",
        "#         true_set = true_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"false\":\n",
        "#         false_set = false_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"barely-true\":\n",
        "#         false_set = false_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"pants-fire\":\n",
        "#         false_set = false_set.append(data_row, ignore_index=True)\n",
        "#     else:\n",
        "#         print(f\"Invalid label was found! {data_row[1]}\")\n",
        "\n",
        "#   print(\"===================================================\\n\")\n",
        "#   print(f\"total len : {total_len}\")\n",
        "#   print(f\"true_set : {len(true_set)}\")\n",
        "#   print(f\"false_set : {len(false_set)}\")\n",
        "#   print(\"===================================================\\n\")\n",
        "\n",
        "#   write_dataframe_in_tsv(true_set, path[\"true_set\"])\n",
        "#   write_dataframe_in_tsv(false_set, path[\"false_set\"])\n",
        "\n",
        "\n",
        "#   return dict(\n",
        "#       true_set=true_set,\n",
        "#       false_set=true_set\n",
        "#   )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "RGPWmYuKW7AT"
      },
      "outputs": [],
      "source": [
        "# fine_main_train_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/train/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/train/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_main_test_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/test/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/test/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_main_val_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/val/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/val/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_toy_train_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/train/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/train/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_toy_test_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/test/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/test/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_toy_val_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/val/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/val/false_sets.tsv\"\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "OGnq4oOKMIN-"
      },
      "outputs": [],
      "source": [
        "# main_train_df = read_tsv(train_prompt_path)\n",
        "# main_test_df = read_tsv(test_prompt_path)\n",
        "# main_val_df = read_tsv(val_prompt_path)\n",
        "\n",
        "# toy_train_df = main_train_df[:600]\n",
        "# toy_test_df = main_test_df[:200]\n",
        "# toy_val_df = main_val_df[:200]\n",
        "\n",
        "\n",
        "# main_train_set = generated_prompts_separator(fine_main_train_path_dict, main_train_df)\n",
        "# main_test_set = generated_prompts_separator(fine_main_test_path_dict, main_test_df)\n",
        "# main_val_set = generated_prompts_separator(fine_main_val_path_dict, main_val_df)\n",
        "\n",
        "# toy_train_set = generated_prompts_separator(fine_toy_train_path_dict, toy_train_df)\n",
        "# toy_test_set = generated_prompts_separator(fine_toy_test_path_dict, toy_test_df)\n",
        "# toy_val_set = generated_prompts_separator(fine_toy_val_path_dict, toy_val_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "-pyPfFDRkUFP"
      },
      "outputs": [],
      "source": [
        "# y = read_tsv(\"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/train/false_sets.tsv\")\n",
        "# y.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# データ前処理の全行程"
      ],
      "metadata": {
        "id": "SxZNpMBC4_xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path_dict = {\n",
        "#     # 生データのパス\n",
        "#     'raw_train_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/train2.tsv',\n",
        "#     'raw_test_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/test2.tsv',\n",
        "#     'raw_val_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/val2.tsv',\n",
        "\n",
        "#     # 欠損値処理を行った後のデータのパス\n",
        "#     'missing_value_processed_train_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_train2.tsv',\n",
        "#     'missing_value_processed_test_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_test2.tsv',\n",
        "#     'missing_value_processed_val_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_val2.tsv',\n",
        "\n",
        "#     # 欠損値処理を行った後、さらなる整形を行ったデータ\n",
        "#     'input_for_lightning_model_train_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_train2.tsv',\n",
        "#     'input_for_lightning_model_test_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_test2.tsv',\n",
        "#     'input_for_lightning_model_val_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_val2.tsv',\n",
        "\n",
        "#     # 集合に分割した後のデータ (main)\n",
        "#     'main_input_for_lightning_model_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/train/true_set2.tsv',\n",
        "#     'main_input_for_lightning_model_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/train/false_set2.tsv',\n",
        "#     'main_input_for_lightning_model_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/test/true_set2.tsv',\n",
        "#     'main_input_for_lightning_model_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/test/false_set2.tsv',\n",
        "#     'main_input_for_lightning_model_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/val/true_set2.tsv',\n",
        "#     'main_input_for_lightning_model_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/val/false_set2.tsv',\n",
        "\n",
        "#     # 集合に分割した後のデータ (toy)\n",
        "#     'toy_input_for_lightning_model_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/train/true_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/train/false_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/test/true_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/test/false_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/val/true_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/val/false_set2.tsv',\n",
        "\n",
        "#   }\n"
      ],
      "metadata": {
        "id": "1xrPuNRyL4Sv"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 生データの読み込み\n",
        "# raw_train_data = read_tsv(path_dict['raw_train_data'])\n",
        "# raw_test_data = read_tsv(path_dict['raw_test_data'])\n",
        "# raw_val_data = read_tsv(path_dict['raw_val_data'])\n",
        "\n",
        "# # 基本的な前処理\n",
        "# processed_train_data = data_missing_value_processor(\n",
        "#     raw_train_data,\n",
        "#     path_dict['missing_value_processed_train_data']\n",
        "# )\n",
        "# processed_test_data = data_missing_value_processor(\n",
        "#     raw_test_data,\n",
        "#     path_dict['missing_value_processed_test_data']\n",
        "# )\n",
        "# processed_val_data = data_missing_value_processor(\n",
        "#     raw_val_data,\n",
        "#     path_dict[\"missing_value_processed_val_data\"]\n",
        "# )\n",
        "\n",
        "# # タスクに向けた前処理と保存\n",
        "# train_data_for_LIAR_PLUS_Dataset = data_pre_processor_for_LIAR_PLUS_Dataset(\n",
        "#     processed_train_data,\n",
        "#     path_dict['input_for_lightning_model_train_base']\n",
        "#     )\n",
        "# test_data_for_LIAR_PLUS_Dataset = data_pre_processor_for_LIAR_PLUS_Dataset(\n",
        "#     processed_test_data,\n",
        "#     path_dict['input_for_lightning_model_test_base']\n",
        "#     )\n",
        "# val_data_for_LIAR_PLUS_Dataset = data_pre_processor_for_LIAR_PLUS_Dataset(\n",
        "#     processed_val_data,\n",
        "#     path_dict['input_for_lightning_model_val_base']\n",
        "#     )\n",
        "\n",
        "# # 集合に分割し保存 (main)\n",
        "# main_train_true_set, main_train_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['main_input_for_lightning_model_train_data_true'],\n",
        "#      'false_set': path_dict['main_input_for_lightning_model_train_data_false']},\n",
        "#     train_data_for_LIAR_PLUS_Dataset\n",
        "# )\n",
        "\n",
        "# main_test_true_set, main_test_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['main_input_for_lightning_model_test_data_true'],\n",
        "#      'false_set': path_dict['main_input_for_lightning_model_test_data_false']},\n",
        "#     test_data_for_LIAR_PLUS_Dataset\n",
        "# )\n",
        "\n",
        "# main_val_true_set, main_val_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['main_input_for_lightning_model_val_data_true'],\n",
        "#      'false_set': path_dict['main_input_for_lightning_model_val_data_false']},\n",
        "#     val_data_for_LIAR_PLUS_Dataset\n",
        "# )\n",
        "\n",
        "\n",
        "# # 集合に分割し保存 (toy)\n",
        "# toy_train_true_set, toy_train_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "#      'false_set': path_dict['toy_input_for_lightning_model_train_data_false']},\n",
        "#     train_data_for_LIAR_PLUS_Dataset[:120]\n",
        "# )\n",
        "\n",
        "# toy_test_true_set, toy_test_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "#      'false_set': path_dict['toy_input_for_lightning_model_test_data_false']},\n",
        "#     test_data_for_LIAR_PLUS_Dataset[:40]\n",
        "# )\n",
        "\n",
        "# toy_val_true_set, toy_val_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "#      'false_set': path_dict['toy_input_for_lightning_model_val_data_false']},\n",
        "#     val_data_for_LIAR_PLUS_Dataset[:40]\n",
        "# )"
      ],
      "metadata": {
        "id": "SBQLyeNu4_Lw"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ_ZwRAby_hP"
      },
      "source": [
        "# Google Driveへ接続しディレクトリを移動"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "L31wefW7zMo9"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "vpFyWy0BzU0v"
      },
      "outputs": [],
      "source": [
        "# %cd /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning\n",
        "# %ls\n",
        "# %pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB5rz2m-y_uq"
      },
      "source": [
        "# TransformersとPytorch Lightningをインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "izeLGyCTznqQ"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X4Le2kXY8K3",
        "outputId": "beb050df-854a-4dd4-c527-264406d3f50a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.35.2\n",
            "2.1.3\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "import pytorch_lightning as pl\n",
        "import pandas as pd\n",
        "print(transformers.__version__)\n",
        "print(pl.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT-scoreに関わるコード"
      ],
      "metadata": {
        "id": "wMvtLR2fs6j7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install bert_score\n",
        "import evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz5Zrc0as-Eh",
        "outputId": "2febd1a2-cc19-4fae-da1c-1bcf64cc488a"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: bert_score in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.1.0+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.5.3)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.35.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.66.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bertscore = evaluate.load(\"bertscore\")\n",
        "bertscore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXBWdjsDy9Em",
        "outputId": "f4f37c5f-51bf-4668-f728-b9ba7463bf84"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EvaluationModule(name: \"bert_score\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\n",
              "BERTScore Metrics with the hashcode from a source against one or more references.\n",
              "\n",
              "Args:\n",
              "    predictions (list of str): Prediction/candidate sentences.\n",
              "    references (list of str or list of list of str): Reference sentences.\n",
              "    lang (str): Language of the sentences; required (e.g. 'en').\n",
              "    model_type (str): Bert specification, default using the suggested\n",
              "        model for the target language; has to specify at least one of\n",
              "        `model_type` or `lang`.\n",
              "    num_layers (int): The layer of representation to use,\n",
              "        default using the number of layers tuned on WMT16 correlation data.\n",
              "    verbose (bool): Turn on intermediate status update.\n",
              "    idf (bool or dict): Use idf weighting; can also be a precomputed idf_dict.\n",
              "    device (str): On which the contextual embedding model will be allocated on.\n",
              "        If this argument is None, the model lives on cuda:0 if cuda is available.\n",
              "    nthreads (int): Number of threads.\n",
              "    batch_size (int): Bert score processing batch size,\n",
              "        at least one of `model_type` or `lang`. `lang` needs to be\n",
              "        specified when `rescale_with_baseline` is True.\n",
              "    rescale_with_baseline (bool): Rescale bertscore with pre-computed baseline.\n",
              "    baseline_path (str): Customized baseline file.\n",
              "    use_fast_tokenizer (bool): `use_fast` parameter passed to HF tokenizer. New in version 0.3.10.\n",
              "\n",
              "Returns:\n",
              "    precision: Precision.\n",
              "    recall: Recall.\n",
              "    f1: F1 score.\n",
              "    hashcode: Hashcode of the library.\n",
              "\n",
              "Examples:\n",
              "\n",
              "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
              "    >>> references = [\"hello there\", \"general kenobi\"]\n",
              "    >>> bertscore = evaluate.load(\"bertscore\")\n",
              "    >>> results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
              "    >>> print([round(v, 2) for v in results[\"f1\"]])\n",
              "    [1.0, 1.0]\n",
              "\"\"\", stored examples: 0)"
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [\"I have an apple\"]\n",
        "references = [\"I have a pen\"]\n",
        "\n",
        "results = bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\")\n",
        "\n",
        "print(results)\n",
        "# {'precision': [0.8849452137947083], 'recall': [0.8849452137947083],\n",
        "# 'f1': [0.8849452137947083], 'hashcode': 'distilbert-base-uncased_L5_no-idf_version=0.3.11(hug_trans=4.11.0.dev0)'}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyloaipQziyB",
        "outputId": "ff6cfe06-3f72-42c1-b660-3f59643557ea"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'precision': [0.8849450349807739], 'recall': [0.8849450349807739], 'f1': [0.8849450349807739], 'hashcode': 'distilbert-base-uncased_L5_no-idf_version=0.3.12(hug_trans=4.35.2)'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciFDDUpS572n"
      },
      "source": [
        "# class LIAR_PLUS_Dataset_For_CEG(Dataset):を定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "NnU6wf8TntZG"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# pylint: disable-msg=import-error\n",
        "# pylint: disable-msg=no-member\n",
        "# ========================================================\n",
        "\"\"\"dataset module is written for create data module\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import time\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "class LIAR_PLUS_Dataset_For_CEG(Dataset):\n",
        "    \"\"\"\n",
        "    this class is for encoding input dataframe for GPT2\n",
        "\n",
        "    Input dataframe needs to be consist of 5 columns, \"id\", \"label\", \"prompt\", \"justification\" and \"credit_score\"\n",
        "\n",
        "    later we wrap a lightning data module around it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data: pd.DataFrame, config_dict: dict):\n",
        "      # max_token_length should be flexible depending on input sequence so it needs to be changed later.\n",
        "        self.data = data\n",
        "        self.config_dict = config_dict\n",
        "        self.prepare_tokenizer = Prepare_Tokenizer(self.config_dict)\n",
        "        self.tokenizer = self.prepare_tokenizer.get_tokenizer()\n",
        "        self.prepare_tokenizer.get_tokenizer_info()\n",
        "        self.max_token_len = config_dict['MAX_LENGTH']\n",
        "        # print(\"Before\")\n",
        "        # print(self.tokenizer.all_special_tokens)\n",
        "        # print(self.tokenizer.all_special_ids)\n",
        "        # special_tokens_dict = {\n",
        "        # 'additional_special_tokens': ['[EXP]'],\n",
        "        # \"eos_token\" : \"<|endoftext|>\",\n",
        "        # \"pad_token\" : \"<|endoftext|>\"\n",
        "        # }\n",
        "        # self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "        # self.exp_id = tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "        # self.eos_id = tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        # self.pad_id = tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        # print(\"After\")\n",
        "        # print(tokenizer.all_special_tokens)\n",
        "        # print(tokenizer.all_special_ids)\n",
        "        # print(f\"exp_id : {self.exp_id}\")\n",
        "        # print(f\"eos_id : {self.eos_id}\")\n",
        "        # print(f\"pad_id : {self.pad_id}\\n\")\n",
        "        # print(f\"len(tokenizer) : {len(self.tokenizer)}\")\n",
        "\n",
        "         # it could be 512\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "\n",
        "        data_row = self.data.iloc[index]\n",
        "        # print('------------------------------------------------------------------\\n')\n",
        "        # print(f'data_row : {data_row}\\n')\n",
        "        # print('------------------------------------------------------------------\\n')\n",
        "        concat_text, full_prompt_text, concat_text_length, full_prompt_text_length = concatinated_inputs_generator(data_row, self.config_dict, self.tokenizer, self.prepare_tokenizer, self.config_dict['INPUT_FLIP'])\n",
        "\n",
        "        # print('------------------------------------------------------------------\\n')\n",
        "        # print(f'concat_text : {concat_text}\\n')\n",
        "        # print(f'full_prompt_text : {full_prompt_text}\\n')\n",
        "        # print(f'concat_text_length : {concat_text_length}\\n')\n",
        "        # print(f'full_prompt_text_length : {full_prompt_text_length}\\n')\n",
        "        # print('------------------------------------------------------------------\\n')\n",
        "        concat_encoding = self.tokenizer.encode_plus(\n",
        "            concat_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length = self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        full_prompt_encoding = self.tokenizer.encode_plus(\n",
        "            full_prompt_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length = self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        # labels = full_prompt_encoding[\"input_ids\"].flatten()\n",
        "        # labels[:concat_text_length] = -100 # cross_entropy_ignore_index\n",
        "\n",
        "        # print(\"=================================================================\\n\")\n",
        "        # print(f\"concat_text : {concat_text}\\n\")\n",
        "        # print(f\"concat_text_length : {concat_text_length}\\n\")\n",
        "        # print(f\"justification_text : {justification_text}\\n\")\n",
        "        # print(f\"justification_text_length : {justification_text_length}\\n\")\n",
        "        # print(f\"full_prompt_text : {full_prompt_text}\\n\")\n",
        "        # print(f\"full_prompt_text_length : {full_prompt_text_length}\\n\")\n",
        "        # print(f\"labels before padding : {labels}\\n\")\n",
        "        # print(f\"labels after padding: {labels}\\n\")\n",
        "        # print(\"=================================================================\\n\")\n",
        "\n",
        "        return dict(\n",
        "            input_ids1=concat_encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask1=concat_encoding[\"attention_mask\"].flatten(),\n",
        "            input_ids2=full_prompt_encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask2=full_prompt_encoding[\"attention_mask\"].flatten(),\n",
        "            concat_text_length=concat_text_length,\n",
        "            full_prompt_text_length=full_prompt_text_length,\n",
        "            # labels = labels\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf6XZBBC58F9"
      },
      "source": [
        "# class LIAR_PLUS_DataModule_For_CEG(pl.LightningDataModule)を定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "gFsOZ6LW6zLM"
      },
      "outputs": [],
      "source": [
        "class LIAR_PLUS_DataModule_For_CEG(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, config_dict, train_df: pd.DataFrame, test_df: pd.DataFrame, val_df: pd.DataFrame):\n",
        "        super().__init__()\n",
        "        self.config_dict = config_dict\n",
        "        self.train_df = train_df\n",
        "        self.test_df = test_df\n",
        "        self.val_df = val_df\n",
        "        self.training_batch_size = config_dict[\"TRAINING_BATCH_SIZE\"]\n",
        "        self.validation_batch_size = config_dict[\"VALIDATION_BATCH_SIZE\"]\n",
        "        self.test_batch_size = config_dict[\"TEST_BATCH_SIZE\"]\n",
        "        self.max_token_len = self.config_dict[\"MAX_LENGTH\"]\n",
        "        self.train_dataset, self.test_dataset, self.val_dataset = None, None, None\n",
        "\n",
        "        # 以下の処理はDataset内でやってくれるのかも\n",
        "        # print(\"Before\")\n",
        "        # print(self.tokenizer.all_special_tokens)\n",
        "        # print(self.tokenizer.all_special_ids)\n",
        "        # special_tokens_dict = {'additional_special_tokens': ['[EXP]'], \"eos_token\" : \"[EOS]\"}\n",
        "        # self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "        # self.exp_id = tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "        # self.eos_id = tokenizer.convert_tokens_to_ids('[EOS]')\n",
        "        # print(\"After\")\n",
        "        # print(tokenizer.all_special_tokens)\n",
        "        # print(tokenizer.all_special_ids)\n",
        "        # print(f\"exp_id : {self.exp_id}\")\n",
        "        # print(f\"eos_id : {self.eos_id}\\n\")\n",
        "\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None) -> None:\n",
        "        self.train_dataset = LIAR_PLUS_Dataset_For_CEG(self.train_df, self.config_dict)\n",
        "        print(f\"train_dataset size : {len(self.train_dataset)}\\n\")\n",
        "        self.test_dataset = LIAR_PLUS_Dataset_For_CEG(self.test_df, self.config_dict)\n",
        "        print(f\"test_dataset size : {len(self.test_dataset)}\\n\")\n",
        "        self.val_dataset = LIAR_PLUS_Dataset_For_CEG(self.val_df, self.config_dict)\n",
        "        print(f\"val_dataset size : {len(self.val_dataset)}\\n\")\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.training_batch_size,\n",
        "            shuffle=True,\n",
        "            drop_last=True,\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.validation_batch_size,\n",
        "            drop_last=True,\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            drop_last=True,\n",
        "            num_workers=2\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FKESVDD58Nr"
      },
      "source": [
        "# build_checkpoint_callbackを定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "zVqrSIBr7hYX"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"helper module is written for write useful function in indexer package\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "def build_checkpoint_callback(config_dict, base_time, batch_size, set_type, filename=\"QTag-{epoch:02d}-{val_loss:.2f}\",\n",
        "                              monitor=\"val_loss\"):\n",
        "    \"\"\"\n",
        "\n",
        "    :param save_top_k:\n",
        "    :param filename:\n",
        "    :param monitor:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # dirpath = config_dict['SAVED_MODEL_PATH'] + f'/{base_time}/' + config_dict['TensorBoardLogger_NAME'] + f'/batch_size={batch_size}'\n",
        "    dirpath = config_dict['SAVED_MODEL_PATH'] + '/' + config_dict['TensorBoardLogger_NAME'] + f'/{base_time}/{set_type}_batch_size={batch_size}'\n",
        "    print(f'dirpath : {dirpath}')\n",
        "    checkpoint_callback = ModelCheckpoint(monitor=monitor,  # monitored quantity\n",
        "                                          filename=filename,\n",
        "                                          save_top_k=config_dict['SAVE_TOP_K'],  # save the top k models\n",
        "                                          dirpath=dirpath,\n",
        "                                          mode=\"min\",  # mode of the monitored quantity for optimization\n",
        "                                          )\n",
        "    print(f\"checkpoint_callback : {checkpoint_callback}\")\n",
        "    return checkpoint_callback, dirpath"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mqQ2UHy58SM"
      },
      "source": [
        "# class CEG(pl.LightningModule)を定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "E9axfgge7tV3"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# pylint: disable=too-many-arguments\n",
        "# pylint: disable=import-error\n",
        "# ========================================================\n",
        "\"\"\"This module is written for write BERT classifier.\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "from typing import List\n",
        "import pytorch_lightning as pl\n",
        "from torch import nn\n",
        "import torch\n",
        "import torchmetrics\n",
        "from transformers import GPT2Tokenizer, AutoModel, AutoConfig, AdamW, get_linear_schedule_with_warmup, GPT2LMHeadModel, T5ForConditionalGeneration, BartForConditionalGeneration\n",
        "\n",
        "class CEG(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    creates a pytorch lightning model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config_dict,\n",
        "                 n_warmup_steps: int = None,\n",
        "                 n_training_steps: int = None,\n",
        "                 n_classes: int = None,\n",
        "                 result_manager = None):\n",
        "        super().__init__()\n",
        "        self.config_dict = config_dict\n",
        "        self.result_manager = result_manager\n",
        "        self.prepare_tokenizer = Prepare_Tokenizer(self.config_dict)\n",
        "        self.tokenizer = self.prepare_tokenizer.get_tokenizer()\n",
        "        self.prepare_tokenizer.get_tokenizer_info()\n",
        "        self.prepare_model = Prepare_Model(config=self.config_dict, prepare_tokenizer=self.prepare_tokenizer)\n",
        "        self.model = self.prepare_model.get_model()\n",
        "        self.prepare_model.get_model_info()\n",
        "        # self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "        # print(\"Before\")\n",
        "        # print(self.tokenizer.all_special_tokens)\n",
        "        # print(self.tokenizer.all_special_ids)\n",
        "        # special_tokens_dict = {\n",
        "        #     'additional_special_tokens': ['[EXP]'],\n",
        "        #     \"eos_token\" : \"<|endoftext|>\",\n",
        "        #     \"pad_token\" : \"<|endoftext|>\"\n",
        "        #     }\n",
        "        # self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "        # exp_id = self.tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "        # eos_id = self.tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        # pad_id = self.tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        # print(\"After\")\n",
        "        # print(self.tokenizer.all_special_tokens)\n",
        "        # print(self.tokenizer.all_special_ids)\n",
        "        # print(f\"exp_id : {exp_id}\")\n",
        "        # print(f\"eos_id : {eos_id}\")\n",
        "        # print(f\"pad_id : {pad_id}\\n\")\n",
        "        # print(f\"len(self.tokenizer) : {len(self.tokenizer)}\")\n",
        "\n",
        "        # gpt2config = AutoConfig.from_pretrained('gpt2', bos_token_id=self.tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id, additional_special_tokens_id = exp_id, output_hidden_states=False)\n",
        "\n",
        "        # self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=gpt2config)\n",
        "        # self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "        # self.lm_head = self.model.lm_head\n",
        "        # new_weights = torch.cat([self.lm_head.weight[:-1, :], torch.zeros(1, self.lm_head.weight.shape[1]) -10000])\n",
        "        # self.model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.batch_size = config_dict['TRAINING_BATCH_SIZE']\n",
        "        self.n_training_steps = n_training_steps\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.config_dict['CROSS_ENTROPY_IGNORE_INDEX'])\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, concat_text_length, full_prompt_text_length):\n",
        "\n",
        "        # print(\"===============================forward=================================\\n\")\n",
        "        labels = input_ids2\n",
        "        print(f'labels : {input_ids2}')\n",
        "        # print(f\"labels : {labels}\")\n",
        "        # print(f\"len(labels) : {len(labels)}\")\n",
        "        # print(full_prompt_text_length)\n",
        "        batch_max_length = max(full_prompt_text_length)\n",
        "        # print(f\"batch_max_length : {batch_max_length}\")\n",
        "\n",
        "\n",
        "        for i in range(len(labels)):\n",
        "            labels[i][:concat_text_length[i]] = -100 # cross_entropy_ignore_index\n",
        "            labels[i][full_prompt_text_length[i]:] = -100\n",
        "        # print(f\"input_ids1 before reshaping : {input_ids1.shape}\")\n",
        "        # print(f\"labels before reshaping : {labels.shape}\")\n",
        "        # print(f\"attention_mask1 before reshaping : {attention_mask1.shape}\")\n",
        "        labels = labels[:,:batch_max_length].contiguous()\n",
        "        print(f'labels : {labels}')\n",
        "        input_ids1 = input_ids1[:,:batch_max_length].contiguous()\n",
        "        attention_mask1 = attention_mask1[:,:batch_max_length].contiguous()\n",
        "        # print(f\"input_ids1 after reshaping : {input_ids1.shape}\")\n",
        "        # print(f\"labels after reshaping : {labels.shape}\")\n",
        "        # print(f\"attention_mask1 after reshaping : {attention_mask1.shape}\")\n",
        "            # print(f\"labels after -100 padding : {labels[0]}\")\n",
        "        # print(f\"len(labels[0]) before shifting : {len(labels[0])}\\n\")\n",
        "        # print(f\"input_ids1 : {input_ids1}\\n\")\n",
        "        # print(f\"attention_mask1 : {attention_mask1}\\n\")\n",
        "        # print(f\"input_ids2 : {input_ids2}\\n\")\n",
        "        # print(f\"concat_text_length : {concat_text_length}\\n\")\n",
        "        # print(f\"full_prompt_length : {full_prompt_text_length}\\n\")\n",
        "        # print(f\"labels : {labels}\\n\")\n",
        "        # print(f\"labels before shifting : {labels}, labels.size() : {labels.size()}\\n\")\n",
        "        # labels = labels[0][1:]\n",
        "        # labels = torch.Tensor(labels)\n",
        "        # print(f\"labels after shifting : {labels}, labels.size() : {labels.size()}\\n\")\n",
        "        # additional_pad = tokenizer.tokenize(\"[PAD]\")\n",
        "        # additional_pad = torch.Tensor(tokenizer.convert_tokens_to_ids(additional_pad))\n",
        "        # additional_pad = torch.Tensor(additional_pad)\n",
        "        # labels = torch.cat((labels, additional_pad))\n",
        "        # print(f\"labels after shifting : {labels}, labels.size() : {labels.size()}\\n\")\n",
        "\n",
        "        #outputs = self.model(input_ids=input_ids1, attention_mask=attention_mask1, labels=labels)\n",
        "\n",
        "        outputs = self.model(input_ids=input_ids1, attention_mask=attention_mask1, labels=labels)\n",
        "        logits = outputs.logits\n",
        "        loss = 0\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # print(f\"outputs : {outputs}\\n\")\n",
        "        # print(f\"outputs type : {type(outputs)}\")\n",
        "        # print(f\"input.shape : {outputs.shape}\")\n",
        "        # print(f\"logits : {logits}\")\n",
        "        # print(f\"logits type : {type(logits)}\")\n",
        "        # print(f\"logits shape : {logits.shape}\")\n",
        "        # print(f\"loss type : {type(loss)}\")\n",
        "        # print(f\"loss : {loss}\")\n",
        "        softmax_output = self.softmax(logits)\n",
        "        arg = torch.argmax(softmax_output, dim=2)\n",
        "        # print(f\"argmax(ouput) : {arg}\") # --> tensor\n",
        "        # print(f\"argmax(ouput).size() : {arg.size()}\")\n",
        "        # print(f\"argmax(output).dtype() : {arg.dtype}\")\n",
        "        # print(arg)\n",
        "        generated = self.tokenizer.decode(arg[0])\n",
        "        # print(f\"generated text: {generated}\\n\")\n",
        "        # print(f\"len(generated) : {len(self.tokenizer.tokenize(generated))}\\n\")\n",
        "        # print(f\"loss type : {type(loss)}\\n\")\n",
        "        # print(f\"loss shape : {loss.shape}\\n\")\n",
        "\n",
        "        return loss, outputs\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "\n",
        "        :param batch:\n",
        "        :param batch_idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # print(f\"training batch : {batch}\")\n",
        "        input_ids1 = batch[\"input_ids1\"]\n",
        "        attention_mask1 = batch[\"attention_mask1\"]\n",
        "        input_ids2 = batch[\"input_ids2\"]\n",
        "        attention_mask2 = batch[\"attention_mask2\"]\n",
        "        concat_text_length = batch[\"concat_text_length\"]\n",
        "        full_prompt_text_length = batch[\"full_prompt_text_length\"]\n",
        "        # labels = batch[\"labels\"]\n",
        "\n",
        "        loss, output = self(input_ids1, attention_mask1, input_ids2, attention_mask2, concat_text_length, full_prompt_text_length)\n",
        "\n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "        # print(f'type(np_loss) : {type(np_loss)}')\n",
        "        # print(f'np_loss : {np_loss}')\n",
        "\n",
        "        self.result_manager.trainLoss_batch.append(np_loss)\n",
        "        # print(\"===============================training=================================\\n\")\n",
        "        # print(f\"output : {outputs}\\n\")\n",
        "        # print(f\"output type : {type(outputs)}\\n\")\n",
        "        # print(f\"loss : {loss}\\n\")\n",
        "        # print(f\"loss type : {type(loss)}\\n\")\n",
        "        # print(f\"loss shape : {loss.shape}\\n\")\n",
        "\n",
        "        return {\"loss\": loss, \"predictions\": output}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "\n",
        "        :param batch:\n",
        "        :param batch_idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # print(f\"validation batch : {batch}\")\n",
        "        input_ids1 = batch[\"input_ids1\"]\n",
        "        attention_mask1 = batch[\"attention_mask1\"]\n",
        "        input_ids2 = batch[\"input_ids2\"]\n",
        "        attention_mask2 = batch[\"attention_mask2\"]\n",
        "        concat_text_length = batch[\"concat_text_length\"]\n",
        "        full_prompt_text_length = batch[\"full_prompt_text_length\"]\n",
        "        # labels = batch[\"labels\"]\n",
        "\n",
        "        loss, output = self(input_ids1, attention_mask1, input_ids2, attention_mask2, concat_text_length, full_prompt_text_length)\n",
        "\n",
        "        self.log(\"val_loss\", loss, prog_bar=True, logger=True, on_step=False)\n",
        "\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "        # print(f'type(np_loss) : {type(np_loss)}')\n",
        "        # print(f'np_loss : {np_loss}')\n",
        "        self.result_manager.valLoss_batch.append(np_loss)\n",
        "        # print(\"===============================validating=================================\\n\")\n",
        "        # print(f\"output : {outputs}\\n\")\n",
        "        # print(f\"output type : {type(outputs)}\\n\")\n",
        "        # print(f\"loss : {loss}\\n\")\n",
        "        # print(f\"loss type : {type(loss)}\\n\")\n",
        "        # print(f\"loss shape : {loss.shape}\\n\")\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "\n",
        "        :param batch:\n",
        "        :param batch_idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        # print(\"test batch : {batch}\")\n",
        "        input_ids1 = batch[\"input_ids1\"]\n",
        "        attention_mask1 = batch[\"attention_mask1\"]\n",
        "        input_ids2 = batch[\"input_ids2\"]\n",
        "        attention_mask2 = batch[\"attention_mask2\"]\n",
        "        concat_text_length = batch[\"concat_text_length\"]\n",
        "        full_prompt_text_length = batch[\"full_prompt_text_length\"]\n",
        "        # labels = batch[\"labels\"]\n",
        "\n",
        "        loss, output = self(input_ids1, attention_mask1, input_ids2, attention_mask2, concat_text_length, full_prompt_text_length)\n",
        "\n",
        "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
        "\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "        # print(f'type(np_loss) : {type(np_loss)}')\n",
        "        # print(f'np_loss : {np_loss}')\n",
        "        self.result_manager.testLoss_batch.append(np_loss)\n",
        "\n",
        "        # print(\"===============================testing=================================\\n\")\n",
        "        # print(f\"output : {outputs}\\n\")\n",
        "        # print(f\"output type : {type(outputs)}\\n\")\n",
        "        # print(f\"loss : {loss}\\n\")\n",
        "        # print(f\"loss type : {type(loss)}\\n\")\n",
        "        # print(f\"loss shape : {loss.shape}\\n\")\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        optimizer = AdamW(self.parameters(), lr=self.config_dict['LR'])\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.n_warmup_steps,\n",
        "                                                    num_training_steps=self.n_training_steps)\n",
        "        return dict(optimizer=optimizer, lr_scheduler=dict(scheduler=scheduler, interval=\"step\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA2UW3Xf58Vh"
      },
      "source": [
        "# calculate_warmup_stepsを定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "4ljK1VdE8BP5"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"This module is written for write useful function.\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_warmup_steps(train_df: pd.DataFrame, num_epochs: int, batch_size: int):\n",
        "    steps_per_epoch = len(train_df) // batch_size\n",
        "    total_training_steps = steps_per_epoch * num_epochs\n",
        "    warmup_steps = total_training_steps // 5\n",
        "    return total_training_steps, warmup_steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8eAFYfq3-CD"
      },
      "source": [
        "# ModelConfigを定義する\n",
        "- colab上では辞書\n",
        "- モジュール分けするときはjsonに"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dPiXl1ej7b3"
      },
      "source": [
        "## GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "kpgeqGQOkCpL"
      },
      "outputs": [],
      "source": [
        "# GPT2-small\n",
        "gpt2_small_config = {\n",
        "    'MODEL_NAME' : 'gpt2',\n",
        "    'TOKENIZER_NAME' : 'gpt2',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/small',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'small'],\n",
        "    'INPUT_FLIP' : 1\n",
        "\n",
        "}\n",
        "\n",
        "# GPT2-medium\n",
        "gpt2_medium_config = {\n",
        "    'MODEL_NAME' : 'gpt2-medium',\n",
        "    'TOKENIZER_NAME' : 'gpt2-medium',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/medium',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'medium'],\n",
        "    'INPUT_FLIP' : 1\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXnvK6fjj9uf"
      },
      "source": [
        "## T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "27MGdEADkWyL"
      },
      "outputs": [],
      "source": [
        "# T5-small\n",
        "t5_small_config = {\n",
        "    'MODEL_NAME' : 't5-small',\n",
        "    'TOKENIZER_NAME' : 't5-small',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 't5/small',\n",
        "    'MODEL_FOLDER' : ['t5', 'small'],\n",
        "    'INPUT_FLIP' : 0\n",
        "}\n",
        "\n",
        "# T5-base\n",
        "t5_base_config = {\n",
        "    'MODEL_NAME' : 't5-base',\n",
        "    'TOKENIZER_NAME' : 't5-base',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 't5/base',\n",
        "    'MODEL_FOLDER' : ['t5', 'base'],\n",
        "    'INPUT_FLIP' : 0\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy9zBiRqkAC3"
      },
      "source": [
        "## BART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "5d4CXx8T4BWp"
      },
      "outputs": [],
      "source": [
        "# BART_base\n",
        "bart_base_config = {\n",
        "    'MODEL_NAME' : 'facebook/bart-base',\n",
        "    'TOKENIZER_NAME' : 'facebook/bart-base',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 'bart/base',\n",
        "    'MODEL_FOLDER' : ['bart', 'base'],\n",
        "    'INPUT_FLIP' : 0\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model_Dict"
      ],
      "metadata": {
        "id": "QoWojRc90I8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model_Config_Dict = {\n",
        "    'gpt2_small_config' : gpt2_small_config,\n",
        "    'gpt2_medium_config' : gpt2_medium_config,\n",
        "    't5_small_config' : t5_small_config,\n",
        "    't5_base_config' : t5_base_config,\n",
        "    'bart_base_config' : bart_base_config\n",
        "}"
      ],
      "metadata": {
        "id": "IILYrc640NFr"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shph_NDW3HpB"
      },
      "source": [
        "# class Prepare_tokenizerを定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "D0mJ9pIG3MHe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import warnings\n",
        "\n",
        "class Prepare_Tokenizer():\n",
        "    def __init__(self, config:dict):\n",
        "        self.config = config\n",
        "        self.TOKENIZER_NAME = self.config['TOKENIZER_NAME']\n",
        "        self.tokenizer = None\n",
        "        self.tokenizer_length = None\n",
        "        self.additional_special_tokens = None\n",
        "        self.eos_token = None\n",
        "        self.eos_token_id = None\n",
        "        self.pad_token = None\n",
        "        self.pad_token_id = None\n",
        "        self.exp_token = '[EXP]'\n",
        "        self.exp_token_id = None\n",
        "        self.initialization_flag = 0\n",
        "\n",
        "    def get_tokenizer(self):\n",
        "        if self.initialization_flag != 0:\n",
        "            raise ValueError(f\"tokenizer is already initialized. This instance is for {self.tokenizer}\")\n",
        "\n",
        "        if self.TOKENIZER_NAME in ['gpt2', 'gpt2-medium']:\n",
        "            self.initialization_flag = 1\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.TOKENIZER_NAME)\n",
        "            print(f\"tokenizer : AutoTokenizer.from_pretrained({self.TOKENIZER_NAME})\")\n",
        "            print(\"Before adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            special_tokens_dict = {\n",
        "            'additional_special_tokens': ['[EXP]'],\n",
        "            \"eos_token\" : \"<|endoftext|>\",\n",
        "            \"pad_token\" : \"<|endoftext|>\"\n",
        "            }\n",
        "            self.eos_token = \"<|endoftext|>\"\n",
        "            self.pad_token = \"<|endoftext|>\"\n",
        "            self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "            self.exp_token_id = self.tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "            self.eos_token_id = self.tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "            self.pad_token_id = self.tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "            print(\"After adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            print(f\"exp_id : {self.exp_token_id}\")\n",
        "            print(f\"eos_id : {self.eos_token_id}\")\n",
        "            print(f\"pad_id : {self.pad_token_id}\")\n",
        "            self.tokenizer_length = len(self.tokenizer)\n",
        "            print(f\"len(tokenizer) : {self.tokenizer_length}\")\n",
        "\n",
        "            return self.tokenizer\n",
        "\n",
        "        elif self.TOKENIZER_NAME in ['t5-small','t5-base']:\n",
        "            self.initialization_flag = 1\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.TOKENIZER_NAME)\n",
        "            print(f\"tokenizer : AutoTokenizer.from_pretrained({self.TOKENIZER_NAME})\")\n",
        "            print(\"Before adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            special_tokens_dict = {\n",
        "            'additional_special_tokens': ['[EXP]'],\n",
        "            }\n",
        "            self.eos_token = '</s>'\n",
        "            self.pad_token = '<pad>'\n",
        "            self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "            self.exp_token_id = self.tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "            self.eos_token_id = self.tokenizer.convert_tokens_to_ids('</s>')\n",
        "            self.pad_token_id = self.tokenizer.convert_tokens_to_ids('<pad>')\n",
        "            print(\"After adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            print(f\"exp_id : {self.exp_token_id}\")\n",
        "            print(f\"eos_id : {self.eos_token_id}\")\n",
        "            print(f\"pad_id : {self.pad_token_id}\")\n",
        "            self.tokenizer_length = len(self.tokenizer)\n",
        "            print(f\"len(tokenizer) : {self.tokenizer_length}\")\n",
        "\n",
        "            return self.tokenizer\n",
        "\n",
        "        elif self.TOKENIZER_NAME in ['facebook/bart-base']:\n",
        "            self.initialization_flag = 1\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.TOKENIZER_NAME)\n",
        "            print(f\"tokenizer : AutoTokenizer.from_pretrained({self.TOKENIZER_NAME})\")\n",
        "            print(\"Before adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            special_tokens_dict = {\n",
        "            'additional_special_tokens': ['[EXP]'],\n",
        "            }\n",
        "            self.eos_token = '</s>'\n",
        "            self.pad_token = '<pad>'\n",
        "            self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "            self.exp_token_id = self.tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "            self.eos_token_id = self.tokenizer.convert_tokens_to_ids('</s>')\n",
        "            self.pad_token_id = self.tokenizer.convert_tokens_to_ids('<pad>')\n",
        "            print(\"After adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            print(f\"exp_id : {self.exp_token_id}\")\n",
        "            print(f\"eos_id : {self.eos_token_id}\")\n",
        "            print(f\"pad_id : {self.pad_token_id}\")\n",
        "            self.tokenizer_length = len(self.tokenizer)\n",
        "            print(f\"len(tokenizer) : {self.tokenizer_length}\")\n",
        "\n",
        "            return self.tokenizer\n",
        "        else:\n",
        "            raise ValueError('Wrong Tokenizer Type : you have to indicate tokenizer type out of the folloing list.\\n [\"gpt2\", \"gpt2-medium\", \"t5-small\", \"t5-base\", \"bart-base\"]\\n')\n",
        "\n",
        "    def get_tokenizer_info(self):\n",
        "        if self.initialization_flag == 0:\n",
        "            print(\"there is no information. you need to call get_tokenizer() first.\")\n",
        "        else:\n",
        "            print(\"---------- tokenizer information ----------\")\n",
        "            print(f\"self.TOKENIZER_NAME : {self.TOKENIZER_NAME}\")\n",
        "            print(f\"self.tokenizer : {self.tokenizer}\")\n",
        "            print(f\"self.tokenizer_length : {self.tokenizer_length}\")\n",
        "            print(f\"self.additional_special_tokens : {self.additional_special_tokens}\")\n",
        "            print(f\"self.eos_token : {self.eos_token}\")\n",
        "            print(f\"self.eos_token_id : {self.eos_token_id}\")\n",
        "            print(f\"self.pad_token : {self.pad_token}\")\n",
        "            print(f\"self.pad_token_id : {self.pad_token_id}\")\n",
        "            print(f\"self.exp_token : {self.exp_token}\")\n",
        "            print(f\"self.exp_token_id : {self.exp_token_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkBEZqVYGCj7"
      },
      "source": [
        "# class Prepare_Modelを定義する\n",
        "- 今のところ、モデルごとに操作の大きな違いはなさそう\n",
        "- BARTに関して,文の頭に`<s>`が付く"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "HE6f_5MZGr0k"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, T5ForConditionalGeneration, BartForConditionalGeneration, AutoConfig\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Prepare_Model():\n",
        "\n",
        "    def __init__(self, config:dict, prepare_tokenizer):\n",
        "        self.MODEL_NAME = config['MODEL_NAME']\n",
        "        self.TOKENIZER_LENGTH = prepare_tokenizer.tokenizer_length\n",
        "        self.additional_model_config = None\n",
        "        self.prepare_tokenizer = prepare_tokenizer\n",
        "        self.model = None\n",
        "        self.initialization_flag = 0\n",
        "\n",
        "\n",
        "    def get_model(self):\n",
        "        if self.initialization_flag == 1:\n",
        "            raise ValueError('You cannot call get_model() because this is already initialized.\\n You are supossed to instantiate Prepare_Model_Class and then call get_model() again.')\n",
        "\n",
        "        if self.MODEL_NAME in ['gpt2', 'gpt2-medium']:\n",
        "\n",
        "            if self.prepare_tokenizer is None:\n",
        "                raise ValueError('prepare_tokenizer is None and this is not good for gpt2!')\n",
        "            self.initialization_flag = 1\n",
        "            self.additional_model_config = AutoConfig.from_pretrained(self.MODEL_NAME, eos_token_id=self.prepare_tokenizer.eos_token_id, pad_token_id=self.prepare_tokenizer.pad_token_id, additional_special_tokens_id = self.prepare_tokenizer.exp_token_id, output_hidden_states=False)\n",
        "            self.model = GPT2LMHeadModel.from_pretrained(self.MODEL_NAME, config=self.additional_model_config)\n",
        "            self.model.resize_token_embeddings(self.TOKENIZER_LENGTH)\n",
        "            self.lm_head = self.model.lm_head\n",
        "            new_weights = torch.cat([self.lm_head.weight[:-1, :], torch.zeros(1, self.lm_head.weight.shape[1]) -10000])\n",
        "            self.model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "\n",
        "            return self.model\n",
        "\n",
        "        elif self.MODEL_NAME in ['t5-small','t5-base']:\n",
        "\n",
        "            self.initialization_flag = 1\n",
        "            self.model = T5ForConditionalGeneration.from_pretrained(self.MODEL_NAME)\n",
        "            self.model.resize_token_embeddings(self.TOKENIZER_LENGTH)\n",
        "            # not sure the following 2 lines are actually necessary\n",
        "            print(f\"weights.shape : {self.model.lm_head.weight.shape}\")\n",
        "            # new_weights = torch.cat([self.model.lm_head.weight[:-1, :], torch.zeros(1, self.model.lm_head.weight.shape[1]) -10000])\n",
        "            # self.model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "\n",
        "\n",
        "            # print(f\"new_weights.shape : {self.model.lm_head.weight.shape}\")\n",
        "\n",
        "            return self.model\n",
        "\n",
        "        elif self.MODEL_NAME in ['facebook/bart-base']:\n",
        "\n",
        "            self.initialization_flag = 1\n",
        "            self.model = BartForConditionalGeneration.from_pretrained(self.MODEL_NAME)\n",
        "            self.model.resize_token_embeddings(self.TOKENIZER_LENGTH)\n",
        "            # not sure the following 2 lines are actually necessary\n",
        "            new_weights = torch.cat([self.model.lm_head.weight[:-1, :], torch.zeros(1, self.model.lm_head.weight.shape[1]) -10000])\n",
        "            self.model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "            return self.model\n",
        "        else:\n",
        "            raise ValueError('Wrong Model Type : you have to indicate model type out of the folloing list.\\n [\"gpt2\", \"gpt2-medium\", \"t5-small\", \"t5-base\", \"bart-base\"]\\n')\n",
        "\n",
        "\n",
        "    def get_model_info(self):\n",
        "        if self.initialization_flag == 0:\n",
        "            print(\"there is no information. you need to call get_model() first.\")\n",
        "        else:\n",
        "            print(\"---------- model information ----------\")\n",
        "            print(f\"self.MODEL_NAME : {self.MODEL_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "2WCJwC68PT4G"
      },
      "outputs": [],
      "source": [
        "# my_prepare_tokenizer = Prepare_Tokenizer(bart_base_config)\n",
        "# my_prepare_tokenizer.get_tokenizer_info()\n",
        "# my_tokenizer = my_prepare_tokenizer.get_tokenizer()\n",
        "# my_prepare_tokenizer.get_tokenizer_info()\n",
        "# my_prepare_model = Prepare_Model(config=bart_base_config, prepare_tokenizer=my_prepare_tokenizer)\n",
        "# my_prepare_model.get_model_info()\n",
        "# my_model = my_prepare_model.get_model()\n",
        "# my_prepare_model.get_model_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1HyKsIv73SD"
      },
      "source": [
        "# 結果を確認する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "xRHqmgR0F59h"
      },
      "outputs": [],
      "source": [
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir=lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "157MXEFW9qNZ"
      },
      "source": [
        "# class Fine_Tuned_Model\n",
        "- fine-tuning済みのモデルをロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "EmN_mlcS9xxq"
      },
      "outputs": [],
      "source": [
        "class Fine_Tuned_Model():\n",
        "    def __init__(self, config_dict, ckpt_file_name, file_name:str):\n",
        "\n",
        "        self.config_dict = config_dict\n",
        "        self.ckpt_file_name = ckpt_file_name\n",
        "        self.file_name = file_name\n",
        "        self.component_name = config_dict['COMPONENT_NAME']\n",
        "        self.MODEL_NAME = config_dict[\"MODEL_NAME\"]\n",
        "        self.transformers_model_path = f'./model_transformers/{self.MODEL_NAME}/{self.file_name}'\n",
        "        self.initialization_flag = 0\n",
        "        self.model = None\n",
        "        self.pretrained_model = None\n",
        "\n",
        "    def create_directory_for_transformers_model(self):\n",
        "        if self.initialization_flag == 1:\n",
        "            raise ValueError('self.initialization_flag == 1 : you are supposed to instantiate Fine_Tuned_Model once and call it again.')\n",
        "        else:\n",
        "            self.initialization_flag = 1\n",
        "\n",
        "        if self.component_name == 'CEG':\n",
        "            self.model = CEG.load_from_checkpoint(gpt2_small_config['SAVED_MODEL_PATH']+ f'/{self.ckpt_file_name}')\n",
        "            self.model.model.save_pretrained(self.transformers_model_path)\n",
        "            print(f'{self.MODEL_NAME} was saved as {self.component_name} in {self.transformers_model_path}.')\n",
        "        elif self.component_name == 'EP':\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(f'self.component_name : {self.component_name} is not supported.')\n",
        "\n",
        "\n",
        "    def load_model(self):\n",
        "        if self.initialization_flag == 0:\n",
        "            raise ValueError('self.initialization_flag == 0 : you are supposed to call create_directory_for_transformer_model() first.')\n",
        "\n",
        "        if self.MODEL_NAME in ['gpt2', 'gpt2-medium']:\n",
        "            self.pretrained_model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/' + f'{self.MODEL_NAME}/{self.file_name}')\n",
        "            print(self.pretrained_model)\n",
        "\n",
        "            return self.pretrained_model\n",
        "\n",
        "        elif self.MODEL_NAME in ['t5-small','t5-base']:\n",
        "            self.pretrained_model = T5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/' + f'{self.MODEL_NAME}/{self.file_name}')\n",
        "            print(self.pretrained_model)\n",
        "\n",
        "            return self.pretrained_model\n",
        "\n",
        "        elif self.MODEL_NAME in ['facebook/bart-base']:\n",
        "            self.pretrained_model = BartForConditionalGeneration.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/' + f'{self.MODEL_NAME}/{self.file_name}')\n",
        "            print(self.pretrained_model)\n",
        "\n",
        "            return self.pretrained_model\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Wrong Model Type : you have to indicate model type out of the folloing list.\\n [\"gpt2\", \"gpt2-medium\", \"t5-small\", \"t5-base\", \"bart-base\"]\\n')\n",
        "\n",
        "\n",
        "    def get_model_info(self):\n",
        "        if self.initialization_flag == 0:\n",
        "            print(\"there is no information. you need to call load_model() first.\")\n",
        "        else:\n",
        "            print(\"---------- model information ----------\")\n",
        "            print(f'self.config_dict : {self.config_dict}')\n",
        "            print(f'self.ckpt_file_name : {self.ckpt_file_name}')\n",
        "            print(f'self.file_name : {self.file_name}')\n",
        "            print(f'self.component_name : {self.component_name}')\n",
        "            print(f'self.MODEL_NAME : {self.MODEL_NAME}')\n",
        "            print(f'self.transformers_model_path : {self.transformers_model_path}')\n",
        "            print(f'self.initialization_flag : {self.initialization_flag}')\n",
        "            print(f'self.model : {self.model}')\n",
        "            print(f'self.pretrained_model : {self.pretrained_model}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class Result_Manager\n",
        "- 結果の保存や表示を行う\n",
        "- 次の指標を保存する\n",
        "## バッチごと\n",
        "- trainAcc\n",
        "- trainF1\n",
        "- trainloss\n",
        "- valAcc\n",
        "- valF1\n",
        "- valloss\n",
        "- testAcc\n",
        "- testF1\n",
        "- testloss\n",
        "\n",
        "\n",
        "## エポックごと\n",
        "- trainAcc\n",
        "- trainF1\n",
        "- trainloss\n",
        "- valAcc\n",
        "- valF1\n",
        "- valoss\n"
      ],
      "metadata": {
        "id": "ylG8KaRfYSd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "import datetime\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "yHeo6y028rRe"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class  Result_Manager():\n",
        "    def __init__(self, base_time, batch_size, model_config, set_type):\n",
        "        self.model_config = model_config\n",
        "        self.batch_size = batch_size\n",
        "        self.set_type = set_type\n",
        "        self.epoch_size = None\n",
        "        self.dt_now = base_time\n",
        "        self.metrics_name_list = [\n",
        "            'trainAcc_batch',\n",
        "            'trainF1_batch',\n",
        "            'trainLoss_batch',\n",
        "            'valAcc_batch',\n",
        "            'valF1_batch',\n",
        "            'valLoss_batch',\n",
        "            'testAcc_batch',\n",
        "            'testF1_batch',\n",
        "            'testLoss_batch',\n",
        "            'trainAcc_epoch',\n",
        "            'trainF1_epoch',\n",
        "            'trainLoss_epoch',\n",
        "            'valAcc_epoch',\n",
        "            'valF1_epoch',\n",
        "            'valLoss_epoch',\n",
        "        ]\n",
        "\n",
        "        self.trainAcc_batch = []\n",
        "        self.trainF1_batch = []\n",
        "        self.trainLoss_batch = []\n",
        "        self.valAcc_batch = []\n",
        "        self.valF1_batch = []\n",
        "        self.valLoss_batch = []\n",
        "        self.testAcc_batch = []\n",
        "        self.testF1_batch = []\n",
        "        self.testLoss_batch = []\n",
        "\n",
        "        self.trainAcc_epoch = []\n",
        "        self.trainF1_epoch = []\n",
        "        self.trainLoss_epoch = []\n",
        "        self.valAcc_epoch = []\n",
        "        self.valF1_epoch = []\n",
        "        self.valLoss_epoch = []\n",
        "\n",
        "        self.metrics_list = [\n",
        "            self.trainAcc_batch,\n",
        "            self.trainF1_batch,\n",
        "            self.trainLoss_batch,\n",
        "            self.valAcc_batch,\n",
        "            self.valF1_batch,\n",
        "            self.valLoss_batch,\n",
        "            self.testAcc_batch,\n",
        "            self.testF1_batch,\n",
        "            self.testLoss_batch,\n",
        "            self.trainAcc_epoch,\n",
        "            self.trainF1_epoch,\n",
        "            self.trainLoss_epoch,\n",
        "            self.valAcc_epoch,\n",
        "            self.valF1_epoch,\n",
        "            self.valLoss_epoch,\n",
        "        ]\n",
        "\n",
        "    def _build_folder(self, base_path: str):\n",
        "\n",
        "        for folder in self.model_config['MODEL_FOLDER']:\n",
        "            base_path = f'{base_path}/{folder}'\n",
        "            if not os.path.exists(base_path):\n",
        "                os.mkdir(base_path)\n",
        "\n",
        "        return base_path\n",
        "\n",
        "\n",
        "    def save_result_log(self, base_path: str):\n",
        "\n",
        "        base_path = f'{base_path}/result_log/{self.dt_now}'\n",
        "        if not os.path.exists(base_path):\n",
        "            os.mkdir(base_path)\n",
        "\n",
        "        model_path = self._build_folder(base_path)\n",
        "\n",
        "        folder = f'{model_path}/{self.set_type}_batch_size:{self.batch_size}'\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "\n",
        "        for idx in range(len(self.metrics_list)):\n",
        "\n",
        "            if len(self.metrics_list[idx]) != 0:\n",
        "                file_name = self.metrics_name_list[idx]+'.csv'\n",
        "                # print(f'file_name : {file_name}')\n",
        "                # np_metric = [item.detach().cpu().numpy().astype(float) for item in self.metrics_list[idx]]\n",
        "                np_metric = self.metrics_list[idx]\n",
        "                print(self.metrics_name_list[idx])\n",
        "                print(f'len(np_metric) : {len(np_metric)}')\n",
        "                data = pd.DataFrame({self.metrics_name_list[idx]:np_metric})\n",
        "                data.to_csv(f'{folder}/{file_name}', header=True, index=False)\n",
        "\n",
        "\n",
        "        # pd.Series(self.trainAcc_batch).to_csv(folder+'/trainAcc_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainF1_batch).to_csv(folder+'/trainF1_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainLoss_batch).to_csv(folder+'/trainLoss_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valAcc_batch).to_csv(folder+'/valAcc_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valF1_batch).to_csv(folder+'/valF1_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valLoss_batch).to_csv(folder+'/valLoss_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.testAcc_batch).to_csv(folder+'/testAcc_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.testF1_batch).to_csv(folder+'/testF1_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.testLoss_batch).to_csv(folder+'/testLoss_batch.csv', header=False, index=False)\n",
        "\n",
        "        # pd.Series(self.trainAcc_epoch).to_csv(folder+'/trainAcc_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainF1_epoch).to_csv(folder+'/trainF1_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainLoss_epoch).to_csv(folder+'/trainLoss_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valAcc_epoch).to_csv(folder+'/valAcc_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valF1_epoch).to_csv(folder+'/valF1_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valLoss_epoch).to_csv(folder+'/valLoss_epoch.csv', header=False, index=False)\n",
        "\n",
        "\n",
        "    def make_single_result_img(self, base_path:str):\n",
        "\n",
        "        base_path = f'{base_path}/plot_figures/{self.dt_now}'\n",
        "        if not os.path.exists(base_path):\n",
        "            os.mkdir(base_path)\n",
        "\n",
        "        model_path = self._build_folder(base_path)\n",
        "\n",
        "        folder = f'{model_path}/{self.set_type}_batch_size:{self.batch_size}'\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "\n",
        "        for idx in range(len(self.metrics_name_list)):\n",
        "\n",
        "            save_fig_path = f'/{folder}/' + self.metrics_name_list[idx] + '.png'\n",
        "\n",
        "            if len(self.metrics_list[idx]) == 0:\n",
        "                continue\n",
        "            else:\n",
        "                plt.figure()\n",
        "                np_metric = self.metrics_list[idx]\n",
        "                # print(f'type(np_metric) : {type(np_metric)}')\n",
        "                # print(self.metrics_name_list[idx])\n",
        "                # print(f'np_metric : {np_metric}')\n",
        "                data = pd.DataFrame({self.metrics_name_list[idx]:np_metric})\n",
        "                # print(f'type(data) : {type(data)}')\n",
        "                data = data.astype(float)\n",
        "                # print(f'type(data) : {type(data)}')\n",
        "                data.plot(\n",
        "                title=self.metrics_name_list[idx],\n",
        "                legend=True\n",
        "                )\n",
        "                plt.savefig(save_fig_path)\n",
        "                # plt.close('all')\n",
        "\n",
        "\n",
        "            # plt.figure()\n",
        "            # if len(self.metrics_list[idx]) != 0:\n",
        "            #     np_metric = self.metrics_list[idx]\n",
        "            #     print(f'type(np_metric) : {type(np_metric)}')\n",
        "            #     print(self.metrics_name_list[idx])\n",
        "            #     print(f'np_metric : {np_metric}')\n",
        "            #     data = pd.Series(np_metric)\n",
        "            #     print(f'type(data) : {type(data)}')\n",
        "            #     data = data.astype(float)\n",
        "            #     print(f'type(data) : {type(data)}')\n",
        "            #     data.plot(\n",
        "            #     title=self.metrics_name_list[idx],\n",
        "            #     legend=True\n",
        "            #     )\n",
        "            # plt.savefig(save_fig_path)\n",
        "            # plt.close('all')\n",
        "\n",
        "    # def make_imgs_into_one(self, base_path:str, metrics: list, figure_name:str):\n",
        "\n",
        "    #     base_path = base_path + '/plot_figures/' + self.dt_now + '/'\n",
        "    #     imgs = []\n",
        "\n",
        "    #     for i in range(len(metrics)):\n",
        "    #         j = base_path + metrics[i] + '.png'\n",
        "    #         imgs.append(j)\n",
        "    #         j = ''\n",
        "\n",
        "    #     save_fig_path = base_path + figure_name +'.png'\n",
        "    #     fig, axs = plt.subplots(1, len(metrics), figsize=(10, 5))\n",
        "\n",
        "    #     for i, im in zip(range(len(axs)), imgs):\n",
        "    #         img = Image.open(im)\n",
        "    #         axs[i].imshow(img)\n",
        "    #         axs[i].set_title(metrics[i])\n",
        "    #         axs[i].axis('off')\n",
        "\n",
        "    #     plt.show()\n",
        "    #     plt.savefig(save_fig_path)\n",
        "\n",
        "    # def make_data_into_one(self, base_path:str, metrics: list, figure_name:str):\n",
        "\n",
        "    #     base_path = base_path + '/result_log/' + self.dt_now + '/'\n",
        "    #     data = []\n",
        "\n",
        "    #     for i in range(len(metrics)):\n",
        "    #         data_path = base_path + i + '.csv'\n",
        "    #         data.append(pd.read_csv(data_path))\n",
        "    #         data_path = ''\n",
        "\n",
        "    #     save_fig_path = base_path + figure_name +'.png'\n",
        "    #     fig, axs = plt.subplots(1, len(metrics), figsize=(10, 5))\n",
        "\n",
        "    #     for i, im in zip(range(len(axs)), imgs):\n",
        "    #         img = Image.open(im)\n",
        "    #         axs[i].imshow(img) # cmap=カラーマップの選択\n",
        "    #         axs[i].set_title(metrics[i])\n",
        "    #         axs[i].axis('off')\n",
        "\n",
        "    #     plt.show()\n",
        "    #     plt.savefig(save_fig_path)\n",
        "\n",
        "    # def get_result_img(self, base_path:str, image_name:str):\n",
        "    #     imgs = base_path + '/plot_figures/' + self.dt_now + '/' + image_name\n",
        "    #     img = imread(fname=imgs, format='png')\n",
        "    #     plt.imshow(\n",
        "    #         img,\n",
        "    #     )\n",
        "    #     plt.show()"
      ],
      "metadata": {
        "id": "s0FmH8BGY5Zw"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# base_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning'\n",
        "\n",
        "# my_Result_Manager = Result_Manager()\n",
        "# my_Result_Manager.save_result_log(base_path)\n",
        "# my_Result_Manager.make_single_result_img(base_path)\n",
        "# my_Result_Manager.make_imgs_into_one(base_path, ['trainLoss_batch', 'valLoss_batch','trainLoss_batch', 'valLoss_batch', 'trainLoss_batch', 'valLoss_batch'], 'example_image')"
      ],
      "metadata": {
        "id": "zx_rIKKtDDqm"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class myclass():\n",
        "#     def __init__(self, num:int):\n",
        "#         self.num = num\n",
        "\n",
        "#     def _add_one(self):\n",
        "#         return self.num +1\n",
        "\n",
        "#     def add_one(self):\n",
        "#         return self._add_one()\n"
      ],
      "metadata": {
        "id": "G3lf9p7sLyYO"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mc = myclass(99)\n",
        "# print(mc.add_one())"
      ],
      "metadata": {
        "id": "wdXGjnkbMm9s"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train_and_test.py"
      ],
      "metadata": {
        "id": "yJThITDcX55U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer"
      ],
      "metadata": {
        "id": "sBM_FaGPz8Bm"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train_and_test_config"
      ],
      "metadata": {
        "id": "SDufpKSVE3ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_test_config =  {\n",
        "    'MODEL_NAME' : ['gpt2_small_config', 'gpt2_medium_config', 't5_small_config', 't5_base_config', 'bart_base_config'],\n",
        "    'BATCH_SIZE' : [1, 2, 4, 8], # train\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "lwiMog4M1qv9"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save_pretrained_model_as_transformers_CEG.py"
      ],
      "metadata": {
        "id": "asT-m5I2Zb8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_pretrained_model_as_transformers_CEG(saved_model_path:str):\n",
        "\n",
        "    file_type=''\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    for i in range(len(saved_model_path)):\n",
        "        # print(saved_model_path[-i])\n",
        "        if saved_model_path[-i-1] == '/':\n",
        "            counter += 1\n",
        "        if counter == 4:\n",
        "            path_for_model_name = saved_model_path[-i:]\n",
        "            continue\n",
        "        if counter == 6:\n",
        "            print(saved_model_path[-i:])\n",
        "            file_type = saved_model_path[-i:]\n",
        "            print(f'file_type: {file_type}')\n",
        "            break\n",
        "\n",
        "    for j in range(len(path_for_model_name)):\n",
        "        if path_for_model_name[j] == '/':\n",
        "            path_for_model_name = path_for_model_name[j+1:]\n",
        "            print(path_for_model_name)\n",
        "            break\n",
        "\n",
        "    file_list = os.listdir(saved_model_path)\n",
        "    ckpt_name = file_list[0]\n",
        "    ckpt_path = f'{file_type}/{ckpt_name}'\n",
        "    print(f'ckpt_name : {ckpt_name}')\n",
        "    print(f'ckpt_path : {ckpt_path}')\n",
        "\n",
        "    tr_model_path = f'./model_transformers/{path_for_model_name}'\n",
        "    model = CEG.load_from_checkpoint(ckpt_path)\n",
        "    model.model.save_pretrained(tr_model_path)\n",
        "\n",
        "    # if model_name in ['gpt2', 'gpt2-medium']:\n",
        "    #     model_from_pretrained = GPT2LMHeadModel.from_pretrained(tr_model_path)\n",
        "    # elif model_name in ['t5-small','t5-base']:\n",
        "    #     model_from_pretrained = T5ForConditionalGeneration.from_pretrained(tr_model_path)\n",
        "    # elif model_name in ['facebook/bart-base']:\n",
        "    #     model_from_pretrained = BartForConditionalGeneration.from_pretrained(tr_model_path)\n",
        "    # else:\n",
        "    #     raise ValueError('Wrong Model Type')\n",
        "\n",
        "    # print(f'pretrained model was saved in {tr_model_path}')\n",
        "\n",
        "    # return model_from_pretrained"
      ],
      "metadata": {
        "id": "1rhdfYSVZhjv"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def train_and_test_CEG\n",
        "- バッチをいろいろ変えて実験を行う"
      ],
      "metadata": {
        "id": "F8DU2vwmi62H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_test_CEG(model_config, train_and_test_config, batch_size:int, base_time, set_type):\n",
        "\n",
        "    print(f'----------- training and testing has just started (batch_size : {batch_size}) -----------')\n",
        "    pl.seed_everything(train_and_test_config['RANDOM_SEED'])\n",
        "\n",
        "    train_data = read_tsv(train_and_test_config[\"TRAINING_DATA_PATH\"])\n",
        "    test_data = read_tsv(train_and_test_config[\"TEST_DATA_PATH\"])\n",
        "    val_data = read_tsv(train_and_test_config[\"VALIDATION_DATA_PATH\"])\n",
        "    model_config['TRAINING_BATCH_SIZE'] = batch_size\n",
        "    result_manager = Result_Manager(base_time, batch_size, model_config, set_type)\n",
        "    base_path = path_dict['TensorBoardLogger'] + f'/{base_time}'\n",
        "    folder = model_config['TensorBoardLogger_NAME'] + f'/batch_size:{batch_size}'\n",
        "\n",
        "    data_module =  LIAR_PLUS_DataModule_For_CEG(\n",
        "                    model_config,\n",
        "                    train_data,\n",
        "                    test_data,\n",
        "                    val_data\n",
        "                    )\n",
        "\n",
        "    total_training_steps, warmup_steps = calculate_warmup_steps(train_data, model_config[\"TRAINING_EPOCHS\"],\n",
        "                                                        model_config[\"TRAINING_BATCH_SIZE\"])\n",
        "    print(f\"total_training_steps : {total_training_steps} , warmup_steps : {warmup_steps} \")\n",
        "    model = CEG(\n",
        "            config_dict=model_config,\n",
        "            n_warmup_steps=warmup_steps,\n",
        "            n_training_steps=total_training_steps,\n",
        "            n_classes=2,\n",
        "            result_manager=result_manager)\n",
        "\n",
        "    checkpoint_callback, saved_model_path = build_checkpoint_callback(model_config, base_time, batch_size, set_type)\n",
        "    logger = TensorBoardLogger(base_path, name=folder)\n",
        "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        logger=logger,\n",
        "        callbacks=[checkpoint_callback, early_stopping_callback],\n",
        "        max_epochs=model_config['TRAINING_EPOCHS'])\n",
        "\n",
        "    trainer.fit(model, datamodule=data_module)\n",
        "    trainer.test(datamodule=data_module)\n",
        "\n",
        "    model.result_manager.save_result_log('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning')\n",
        "    model.result_manager.make_single_result_img('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning')\n",
        "\n",
        "    save_pretrained_model_as_transformers_CEG(saved_model_path)\n",
        "    # print(f'model_from_pretrained : {model_from_pretrained}')\n"
      ],
      "metadata": {
        "id": "gYTebhHDjLvC"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def get_base_time\n",
        "- ベースタイムを取得する"
      ],
      "metadata": {
        "id": "4gN4z8_DGBp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_time():\n",
        "\n",
        "    now_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "    base_time = str(now_time)[:-16]\n",
        "\n",
        "    print(f'you got base_time : {base_time}')\n",
        "\n",
        "    return base_time\n"
      ],
      "metadata": {
        "id": "g97VCxNfGZFk"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base_time = get_base_time()\n",
        "# train_and_test_CEG(\n",
        "#     model_config=gpt2_small_config,\n",
        "#     train_and_test_config=train_and_test_config,\n",
        "#     batch_size=1,\n",
        "#     base_time=base_time,\n",
        "#     set_type='true')"
      ],
      "metadata": {
        "id": "5uW-MK4zqqvJ"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train_and_test_CEGの実行による各モデルの訓練"
      ],
      "metadata": {
        "id": "60u4GTAT0Cvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## True"
      ],
      "metadata": {
        "id": "_-HuZdnZ0Nrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train_and_test_config_true"
      ],
      "metadata": {
        "id": "hCkruQmdooH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_test_config_true =  {\n",
        "    'MODEL_NAME' : ['gpt2_small_config', 'gpt2_medium_config', 't5_small_config', 't5_base_config', 'bart_base_config'],\n",
        "    'BATCH_SIZE' : [1, 2, 4, 8], # train\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'TRAINING_DATA_PATH' : path_dict['main_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['main_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['main_input_for_lightning_model_test_data_true'],\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "0MgNzNB603K0"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main_train_data_true = read_tsv(train_and_test_config_true['TRAINING_DATA_PATH'])\n",
        "# main_test_data_true = read_tsv(train_and_test_config_true['TEST_DATA_PATH'])\n",
        "# main_val_data_true = read_tsv(train_and_test_config_true['VALIDATION_DATA_PATH'])\n",
        "\n",
        "# print(len(main_train_data_true))\n",
        "# print(len(main_test_data_true))\n",
        "# print(len(main_val_data_true))\n"
      ],
      "metadata": {
        "id": "XD5RHS2dXOuU"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gpt2-small"
      ],
      "metadata": {
        "id": "2N3AvG7lydzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # gpt2-small\n",
        "# base_time = get_base_time()\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_small_config,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=true)"
      ],
      "metadata": {
        "id": "2LoaFzU1zb37"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gpt2-medium"
      ],
      "metadata": {
        "id": "h46osNIZygU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # gpt2-medium\n",
        "# base_time = get_base_time()\n",
        "# # for batch_size in [1, 2]:\n",
        "# #     train_and_test_CEG(\n",
        "# #         model_config=gpt2_medium_config,\n",
        "# #         train_and_test_config=train_and_test_config_true,\n",
        "# #         batch_size=batch_size,\n",
        "# #         base_time=base_time,\n",
        "# #         set_type=true)"
      ],
      "metadata": {
        "id": "IbWO94Io0RLO"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### T5-small"
      ],
      "metadata": {
        "id": "La7hJpKuyvWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # t5-small\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=t5_small_config,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=true)"
      ],
      "metadata": {
        "id": "sHL8J0lI0Rlx"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### T5-base"
      ],
      "metadata": {
        "id": "tAizUat5yyqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # t5-base\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=t5_base_config,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=true)"
      ],
      "metadata": {
        "id": "a9ZSBLd60Rs8"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BART-base"
      ],
      "metadata": {
        "id": "nqx7eVMOy1Fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # bart-base\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=bart_base_config,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=true)"
      ],
      "metadata": {
        "id": "SVbTeLvm0Rwf"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## False"
      ],
      "metadata": {
        "id": "JfJfTUNn0X11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train_and_test_config_false"
      ],
      "metadata": {
        "id": "tpAontdzo0Le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_test_config_false =  {\n",
        "    'MODEL_NAME' : ['gpt2_small_config', 'gpt2_medium_config', 't5_small_config', 't5_base_config', 'bart_base_config'],\n",
        "    'BATCH_SIZE' : [1, 2, 4, 8], # train\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'TRAINING_DATA_PATH' : path_dict['main_input_for_lightning_model_train_data_false'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['main_input_for_lightning_model_val_data_false'],\n",
        "    'TEST_DATA_PATH' : path_dict['main_input_for_lightning_model_test_data_false'],\n",
        "}"
      ],
      "metadata": {
        "id": "FusPgqAy0_Kf"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main_train_data_false = read_tsv(train_and_test_config_false['TRAINING_DATA_PATH'])\n",
        "# main_test_data_false = read_tsv(train_and_test_config_false['TEST_DATA_PATH'])\n",
        "# main_val_data_false = read_tsv(train_and_test_config_false['VALIDATION_DATA_PATH'])\n",
        "\n",
        "# print(len(main_train_data_false))\n",
        "# print(len(main_test_data_false))\n",
        "# print(len(main_val_data_false))\n"
      ],
      "metadata": {
        "id": "JyoEbFt1ZH-A"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gpt2-small"
      ],
      "metadata": {
        "id": "Ka9DoL870X12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # gpt2-small\n",
        "# base_time = get_base_time()\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_small_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=false)"
      ],
      "metadata": {
        "id": "lY0r-H1w0X13"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gpt2-medium"
      ],
      "metadata": {
        "id": "CWmiAotx0X13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # gpt2-medium\n",
        "# base_time = get_base_time()\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_medium_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=false)"
      ],
      "metadata": {
        "id": "M8AC7YXA0X13"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### T5-small"
      ],
      "metadata": {
        "id": "-r4MVsod0X14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # t5-small\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=t5_small_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=false)"
      ],
      "metadata": {
        "id": "_VGsyVZb0X15"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### T5-base"
      ],
      "metadata": {
        "id": "okDGftAa0X15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # t5-base\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=t5_base_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=false)"
      ],
      "metadata": {
        "id": "A2TcfQFz0X16"
      },
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BART-base"
      ],
      "metadata": {
        "id": "IU5Bspoz0X16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # bart-base\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=bart_base_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=false)"
      ],
      "metadata": {
        "id": "lLz4ionR0X16"
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# predict_CEG.py"
      ],
      "metadata": {
        "id": "M6eFhN6gYCIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prediction_config"
      ],
      "metadata": {
        "id": "FClHb_Z7QKDM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6yLpINWQdYu"
      },
      "source": [
        "### GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "ZcS52jgHQdZJ"
      },
      "outputs": [],
      "source": [
        "# GPT2-small\n",
        "gpt2_small_config_for_prediction = {\n",
        "    'MODEL_NAME' : 'gpt2',\n",
        "    'TOKENIZER_NAME' : 'gpt2',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/small',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'small'],\n",
        "    'INPUT_FLIP' : 1\n",
        "}\n",
        "\n",
        "# GPT2-medium\n",
        "gpt2_medium_config_for_prediction = {\n",
        "    'MODEL_NAME' : 'gpt2-medium',\n",
        "    'TOKENIZER_NAME' : 'gpt2-medium',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/medium',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'medium'],\n",
        "    'INPUT_FLIP' : 1\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDWJumDfQdZK"
      },
      "source": [
        "### T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "ktq3nYX8QdZK"
      },
      "outputs": [],
      "source": [
        "# T5-small\n",
        "t5_small_config_for_prediction = {\n",
        "    'MODEL_NAME' : 't5-small',\n",
        "    'TOKENIZER_NAME' : 't5-small',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata'],\n",
        "    'TensorBoardLogger_NAME' : 't5/small',\n",
        "    'MODEL_FOLDER' : ['t5', 'small'],\n",
        "    'INPUT_FLIP' : 0\n",
        "}\n",
        "\n",
        "# T5-base\n",
        "t5_base_config_for_prediction = {\n",
        "    'MODEL_NAME' : 't5-base',\n",
        "    'TOKENIZER_NAME' : 't5-base',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata'],\n",
        "    'TensorBoardLogger_NAME' : 't5/base',\n",
        "    'MODEL_FOLDER' : ['t5', 'base'],\n",
        "    'INPUT_FLIP' : 0\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9471NurQdZL"
      },
      "source": [
        "### BART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "id": "YJ7l-9zUQdZM"
      },
      "outputs": [],
      "source": [
        "# BART_base\n",
        "bart_base_config_for_prediction = {\n",
        "    'MODEL_NAME' : 'facebook/bart-base',\n",
        "    'TOKENIZER_NAME' : 'facebook/bart-base',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata'],\n",
        "    'TensorBoardLogger_NAME' : 'bart/base',\n",
        "    'MODEL_FOLDER' : ['bart', 'base'],\n",
        "    'INPUT_FLIP' : 0\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def load_transformers_model()"
      ],
      "metadata": {
        "id": "7obqzggSvFZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_transformers_model(model_name:str, tr_model_path:str):\n",
        "\n",
        "    if model_name in ['gpt2', 'gpt2-medium']:\n",
        "        model_from_pretrained = GPT2LMHeadModel.from_pretrained(tr_model_path)\n",
        "    elif model_name in ['t5-small','t5-base']:\n",
        "        model_from_pretrained = T5ForConditionalGeneration.from_pretrained(tr_model_path)\n",
        "    elif model_name in ['facebook/bart-base']:\n",
        "        model_from_pretrained = BartForConditionalGeneration.from_pretrained(tr_model_path)\n",
        "    else:\n",
        "        raise ValueError('Wrong Model Type')\n",
        "\n",
        "    return model_from_pretrained"
      ],
      "metadata": {
        "id": "4LKKGgntvKDM"
      },
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tr_model_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/gpt2/small/2024-01-16 18:36/batch_size=1'\n",
        "# model_from_pretrained = load_transformers_model('gpt2', tr_model_path)\n",
        "# train_data = read_tsv(train_and_test_config[\"TRAINING_DATA_PATH\"])"
      ],
      "metadata": {
        "id": "AUJzynDQvyXs"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def get_textualized_outputs()"
      ],
      "metadata": {
        "id": "vNhDiYAaPCTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_textualized_outputs(outputs, input_length, tokenizer):\n",
        "\n",
        "    # print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "    generated_tokens = outputs.sequences[:, input_length:]\n",
        "    # print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "    tokens_list = outputs.sequences[0]\n",
        "    # print(tokens_list)\n",
        "    full_text = tokenizer.decode(tokens_list)\n",
        "    generated_text = tokenizer.decode(generated_tokens[0])\n",
        "\n",
        "    # print(f\"full text : {full_text}\\n\")\n",
        "    # print(f\"generated text: {generated_text}\")\n",
        "    # print(f\"len(generated) : {len(tokenizer.tokenize(generated_text))}\\n\")\n",
        "\n",
        "    return full_text, generated_text"
      ],
      "metadata": {
        "id": "u9HjMNUUPIOx"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def extend_data_with_generated_justification()"
      ],
      "metadata": {
        "id": "t8m33Kj6LRTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extend_data_with_generated_justification(data, data_row, generated_text):\n",
        "\n",
        "    # df_data_row = pd.DataFrame(data_row)\n",
        "    df = pd.DataFrame([generated_text])\n",
        "\n",
        "    concatinated_data_row = pd.concat([data_row, df], ignore_index=True, axis=0).T\n",
        "    data = pd.concat([data, concatinated_data_row], axis=0).reset_index(drop=True)\n",
        "\n",
        "# data = pd.DataFrame()\n",
        "# data_row = train_data.iloc[0]\n",
        "# j = pd.DataFrame(['1111'])\n",
        "# concatinated_data_row = pd.concat([data_row, j], axis=0).T\n",
        "# # print(len(concatinated_data_row))\n",
        "# # concatinated_data_row\n",
        "# data_1 = pd.concat([data, concatinated_data_row], axis=0).reset_index(drop=True)\n",
        "# data_2 = pd.concat([data_1, concatinated_data_row], axis=0).reset_index(drop=True)\n",
        "# data_3 = pd.concat([data_2, concatinated_data_row], axis=0).reset_index(drop=True)\n",
        "# data_3\n",
        "    return data"
      ],
      "metadata": {
        "id": "fRABoNMGLnt0"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def predict()"
      ],
      "metadata": {
        "id": "J_AtPhqnvb-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(tokenizer, prepare_tokenizer, model_from_pretrained, model_config, data:pd.DataFrame):\n",
        "\n",
        "    extended_data = pd.DataFrame()\n",
        "\n",
        "    for idx in range(len(data)):\n",
        "\n",
        "        # ilocでデータ列を取得\n",
        "        data_row = data.iloc[idx]\n",
        "\n",
        "        # get_concatinated_input\n",
        "        prompt, full_input, concatinated_input_length, full_input_length = \\\n",
        "        concatinated_inputs_generator(data_row, model_config, tokenizer, prepare_tokenizer, model_config[\"INPUT_FLIP\"])\n",
        "\n",
        "        # モデルに入力\n",
        "        inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
        "        prompt_length = len(inputs[\"input_ids\"][0])\n",
        "        print(f\"inputs_len : {prompt_length}\")\n",
        "\n",
        "        # greedy_search\n",
        "        # outputs = model.generate(**inputs, max_new_tokens=15, return_dict_in_generate=True, output_scores=True)\n",
        "        # beam_search\n",
        "        outputs = model_from_pretrained.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            num_beams=4,\n",
        "            num_return_sequences=1,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "        )\n",
        "\n",
        "        # get_textualized_outputs()\n",
        "        full_text, generated_text = get_textualized_outputs(outputs, prompt_length, tokenizer)\n",
        "\n",
        "        print(f'generated text : {generated_text}')\n",
        "\n",
        "        # extend_data_with_generated_justification()\n",
        "        extended_data = extend_data_with_generated_justification(extended_data, data_row, generated_text)\n",
        "\n",
        "        del data_row\n",
        "\n",
        "    return extended_data\n"
      ],
      "metadata": {
        "id": "qCQsJDlcYObg"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def predict_CEG.py"
      ],
      "metadata": {
        "id": "XMye9Hsw2RDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def predict_CEG(model_config, model_name:str, data:pd.DataFrame, ckpt_path, stage:str, binary_type:str, is_toy, base_time):\n",
        "    replaced_ckpt_path = ckpt_path.replace('/', '_')\n",
        "    tr_model_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/' + model_config['TensorBoardLogger_NAME'] +f'/{ckpt_path}'\n",
        "\n",
        "    if is_toy == 0:\n",
        "        saved_data_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/' + model_config['TensorBoardLogger_NAME'] + f'/{stage}/{binary_type}_{replaced_ckpt_path}.tsv'\n",
        "    elif is_toy == 1:\n",
        "        base_path = f'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/{base_time}'\n",
        "        if not os.path.exists(base_path):\n",
        "            os.mkdir(base_path)\n",
        "\n",
        "        saved_data_path = f'{base_path}/{model_name}_{replaced_ckpt_path}_{binary_type}.tsv'\n",
        "    else:\n",
        "        ValueError('Wrong is_toy type')\n",
        "\n",
        "\n",
        "    model_from_pretrained = load_transformers_model(model_name, tr_model_path)\n",
        "\n",
        "    prepare_tokenizer = Prepare_Tokenizer(model_config)\n",
        "    tokenizer = prepare_tokenizer.get_tokenizer()\n",
        "\n",
        "    extended_data = predict(tokenizer, prepare_tokenizer, model_from_pretrained, model_config, data)\n",
        "    write_dataframe_in_tsv(extended_data, saved_data_path)\n",
        "\n",
        "    print(f'the extended data was saved in {saved_data_path}')"
      ],
      "metadata": {
        "id": "HnPtr-4jrobd"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def jsut_predict_CEG()"
      ],
      "metadata": {
        "id": "lSPUHtxpKskA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def just_predict_CEG(model_config, model_name:str, data:pd.DataFrame, ckpt_path, stage:str, binary_type:str):\n",
        "    replaced_ckpt_path = ckpt_path.replace('/', '_')\n",
        "    tr_model_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/' + model_config['TensorBoardLogger_NAME'] +f'/{ckpt_path}'\n",
        "    saved_data_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/' + model_config['TensorBoardLogger_NAME'] + f'/{stage}/{binary_type}_{replaced_ckpt_path}.tsv'\n",
        "    model_from_pretrained = load_transformers_model(model_name, tr_model_path)\n",
        "\n",
        "    prepare_tokenizer = Prepare_Tokenizer(model_config)\n",
        "    tokenizer = prepare_tokenizer.get_tokenizer()\n",
        "\n",
        "    extended_data = predict(tokenizer, prepare_tokenizer, model_from_pretrained, model_config, data)\n",
        "    # write_dataframe_in_tsv(extended_data, saved_data_path)\n",
        "\n",
        "    # print(f'the extended data was saved in {saved_data_path}')\n",
        "    return extended_data"
      ],
      "metadata": {
        "id": "p11tfx25tR73"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 学習・推論を行うコード"
      ],
      "metadata": {
        "id": "PN7vcmrFlXkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 通しで学習"
      ],
      "metadata": {
        "id": "QAg8ZtHUbfBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_train_data_true = read_tsv(train_and_test_config_true['TRAINING_DATA_PATH'])\n",
        "main_test_data_true = read_tsv(train_and_test_config_true['TEST_DATA_PATH'])\n",
        "main_val_data_true = read_tsv(train_and_test_config_true['VALIDATION_DATA_PATH'])\n",
        "main_train_data_false = read_tsv(train_and_test_config_false['TRAINING_DATA_PATH'])\n",
        "main_test_data_false = read_tsv(train_and_test_config_false['TEST_DATA_PATH'])\n",
        "main_val_data_false = read_tsv(train_and_test_config_false['VALIDATION_DATA_PATH'])\n"
      ],
      "metadata": {
        "id": "jIjbxdtgt-9A"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base_time = get_base_time()\n",
        "# print(base_time)"
      ],
      "metadata": {
        "id": "_mcrYbAbaE0J"
      },
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base_time = 'delite this'\n",
        "\n",
        "# # gpt2-small\n",
        "# # true\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_small_config,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='true')\n",
        "# # false\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_small_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='false')\n",
        "\n",
        "# # gpt2-medium\n",
        "# # true\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_medium_config,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='true')\n",
        "# # false\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_medium_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='false')\n",
        "\n",
        "# t5-small\n",
        "# # true\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=t5_small_config,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='true')\n",
        "# # false\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=t5_small_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='false')\n",
        "\n",
        "# t5-base\n",
        "# true\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=t5_base_config,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='true')\n",
        "# # false\n",
        "# for batch_size in [1, 2]:\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=t5_base_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='false')\n",
        "\n",
        "# # bart-base\n",
        "\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=bart_base_config,\n",
        "#         train_and_test_config=train_and_test_config_true,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='true')\n",
        "\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=bart_base_config,\n",
        "#         train_and_test_config=train_and_test_config_false,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type='false')\n",
        "\n",
        "# from time import sleep\n",
        "# sleep(120)\n",
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ],
      "metadata": {
        "id": "n6V8_2bLbnzg"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ちまちま学習"
      ],
      "metadata": {
        "id": "pV9QBB8xZUVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# base_time = get_base_time()\n",
        "# # print(base_time)"
      ],
      "metadata": {
        "id": "6Bd46fWbrxCq"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flip\n",
        "base_time = '2024-01-24 13:17 flip'\n",
        "gpt2_small_config1 = {\n",
        "    'MODEL_NAME' : 'gpt2',\n",
        "    'TOKENIZER_NAME' : 'gpt2',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement','metadata','justification'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/small',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'small'],\n",
        "    'INPUT_FLIP' : 1\n",
        "\n",
        "}\n",
        "\n",
        "# GPT2-medium\n",
        "gpt2_medium_config2 = {\n",
        "    'MODEL_NAME' : 'gpt2-medium',\n",
        "    'TOKENIZER_NAME' : 'gpt2-medium',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/medium',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'medium'],\n",
        "    'INPUT_FLIP' : 1\n",
        "}\n",
        "# gpt2-small\n",
        "# true\n",
        "for batch_size in [1, 2]:\n",
        "    train_and_test_CEG(\n",
        "        model_config=gpt2_small_config1,\n",
        "        train_and_test_config=train_and_test_config_true,\n",
        "        batch_size=batch_size,\n",
        "        base_time=base_time,\n",
        "        set_type='true')\n",
        "# # false\n",
        "for batch_size in [1, 2]:\n",
        "    train_and_test_CEG(\n",
        "        model_config=gpt2_small_config1,\n",
        "        train_and_test_config=train_and_test_config_false,\n",
        "        batch_size=batch_size,\n",
        "        base_time=base_time,\n",
        "        set_type='false')\n",
        "\n",
        "# gpt2-medium\n",
        "# true\n",
        "for batch_size in [1, 2]:\n",
        "    train_and_test_CEG(\n",
        "        model_config=gpt2_medium_config1,\n",
        "        train_and_test_config=train_and_test_config_true,\n",
        "        batch_size=batch_size,\n",
        "        base_time=base_time,\n",
        "        set_type='true')\n",
        "# # false\n",
        "for batch_size in [1, 2]:\n",
        "    train_and_test_CEG(\n",
        "        model_config=gpt2_medium_config1,\n",
        "        train_and_test_config=train_and_test_config_false,\n",
        "        batch_size=batch_size,\n",
        "        base_time=base_time,\n",
        "        set_type='false')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dc8e6502ca524d56b850fb907f2db571",
            "53b96e0a9ddc404ba0951cd97b9a955c",
            "e2d739710f5449bc9ae2a1b9f4c72390",
            "6b298d9a51d5495ba37d60522f63631d",
            "5db6968e683e45b5be0ac029dbbb6d2b",
            "33860f83168a4a50afae2d5d68324cd6",
            "90273627371a4e3ba12886a285ea1256",
            "5ab71d209e0042718b4a2779d6525974",
            "e5ffb6b6912e423d81a3939df1483f78",
            "17edcb814d6748cbb45c114955a30667",
            "e6720586bf5546a49c062cb0c6467136",
            "bf2ea8ca8ed445e7b04aa41cd79386b2",
            "780af9616aff427eb6efdaf7237909f2",
            "5c86a7747f7841aca00493973c6708bb",
            "c230e4433cc44daba470d60cc7b28070",
            "a89780dfc78448cdbcd1bde0a75fb28c",
            "59919f1f1ec54976978b656c109b2282",
            "578cfcd81d7f4cc19cc034dd6a869327",
            "00178289c1054b7cb9f4d4affd1621a6",
            "413e580643074e629c2927541652837a",
            "028bc788895b4939b36e9a77fc2ba21b",
            "75f4e4ec02c142339374b1eb0febaebe"
          ]
        },
        "id": "LBv4FL5Zlp09",
        "outputId": "3043667b-ca59-45a2-e63f-f172899141be"
      },
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- training and testing has just started (batch_size : 1) -----------\n",
            "total_training_steps : 11504 , warmup_steps : 2300 \n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : gpt2\n",
            "self.tokenizer : GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['[EXP]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t50257: AddedToken(\"[EXP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : 50258\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : <|endoftext|>\n",
            "self.eos_token_id : 50256\n",
            "self.pad_token : <|endoftext|>\n",
            "self.pad_token_id : 50256\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : 50257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- model information ----------\n",
            "self.MODEL_NAME : gpt2\n",
            "dirpath : /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/src/models/gpt2/small/2024-01-24 13:17 flip/true_batch_size=1\n",
            "checkpoint_callback : <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7ff2e9c36bc0>\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : gpt2\n",
            "self.tokenizer : GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['[EXP]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t50257: AddedToken(\"[EXP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : 50258\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : <|endoftext|>\n",
            "self.eos_token_id : 50256\n",
            "self.pad_token : <|endoftext|>\n",
            "self.pad_token_id : 50256\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : 50257\n",
            "train_dataset size : 5752\n",
            "\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : gpt2\n",
            "self.tokenizer : GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['[EXP]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t50257: AddedToken(\"[EXP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : 50258\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : <|endoftext|>\n",
            "self.eos_token_id : 50256\n",
            "self.pad_token : <|endoftext|>\n",
            "self.pad_token_id : 50256\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : 50257\n",
            "test_dataset size : 714\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | GPT2LMHeadModel  | 163 M \n",
            "1 | softmax   | Softmax          | 0     \n",
            "2 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "163 M     Trainable params\n",
            "0         Non-trainable params\n",
            "163 M     Total params\n",
            "652.155   Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : gpt2\n",
            "self.tokenizer : GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['[EXP]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t50257: AddedToken(\"[EXP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : 50258\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : <|endoftext|>\n",
            "self.eos_token_id : 50256\n",
            "self.pad_token : <|endoftext|>\n",
            "self.pad_token_id : 50256\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : 50257\n",
            "val_dataset size : 668\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc8e6502ca524d56b850fb907f2db571"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "concatinated_input : Metadata: poverty jim-francesconi Member of the State Board of Higher Education Oregon none an opinion article Statement: Says nearly half of Oregons children are poor.[EXP]\n",
            "full_input_length : 141\n",
            "concatinated_input : Metadata: crime,government-efficiency,medicare peter-roskam U.S. Representative Illinois republican an interview with Fox News Statement: Worldwide credit card transactions, the credit card fraud rate is 0.04 percent, compared to almost 8 percent, 9 percent, 10 percent of Medicare fraud.[EXP]concatinated_input : Metadata: economy,stimulus barack-obama President Illinois democrat interview with CBS News Statement: On attacks by Republicans that various programs in the economic stimulus plan are not stimulative, \"If you add all that stuff up, it accounts for less than 1 percent of the overall package.\"[EXP]\n",
            "\n",
            "full_input_length : 168full_input_length : 130\n",
            "\n",
            "concatinated_input : Metadata: economy,workers mitt-romney Former governor Massachusetts republican a speech to the Conservative Political Action Conference Statement: In the month of January, Canada created more new jobs than we did.[EXP]\n",
            "full_input_length : 160concatinated_input : Metadata: education,state-budget andy-berke Lawyer and state senator Tennessee democrat a letter to state Senate education committee chairwoman Dolores Gresham. Statement: Says Tennessee is providing millions of dollars to virtual school company for results at the bottom of the bottom.[EXP]\n",
            "\n",
            "full_input_length : 209concatinated_input : Metadata: health-care,medicaid,message-machine-2014,poverty,public-health scott-walker Milwaukee County Executive Wisconsin republican an ad Statement: Under his leadership, more people in Wisconsin have access to health care.[EXP]\n",
            "\n",
            "full_input_length : 187\n",
            "concatinated_input : Metadata: candidates-biography,diversity,housing hillary-clinton Presidential candidate New York democrat the first presidential debate Statement: Says Donald Trump started his career back in 1973 being sued by the Justice Department for racial discrimination because he would not rent apartments in one of his developments to African-Americans.[EXP]\n",
            "full_input_length : 179\n",
            "concatinated_input : Metadata: economy,jobs,taxes ted-cruz Senator Texas republican an interview on \"Meet the Press\" Statement: Says Donald Trumps only economic agenda is imposing massive taxes on the American people with a 40 percent tax hike of a giant tariff.[EXP]\n",
            "full_input_length : 111\n",
            "concatinated_input : Metadata: military republican-party-texas None Texas republican an e-mail Statement: Bill White has a long history of trying to limit or even disenfranchise military voters.[EXP]\n",
            "full_input_length : 143\n",
            "concatinated_input : Metadata: energy,federal-budget david-plouffe Senior Adviser to the President None democrat an interview on \"Meet the Press\" Statement: The loan guarantee program that helped Solyndra was a program that was supported by President Bush.[EXP]\n",
            "full_input_length : 150\n",
            "concatinated_input : Metadata: economy tim-kaine U.S. Senator Virginia democrat a speech at the Democratic National Convention in Philadelphia Statement: John McCains chief economic adviser during the 08 race estimated that Trumps promises would cause America to lose 3.5 million jobs.[EXP]\n",
            "full_input_length : 224\n",
            "concatinated_input : Metadata: elections,florida,job-accomplishments,message-machine jeb-bush None Florida republican a campaign commercial for Bill McCollum Statement: Bill McCollumhas \"recovered $200 million in Medicaid fraud.\"[EXP]\n",
            "concatinated_input : Metadata: state-budget steve-henson State Senator Georgia democrat a press release Statement: State revenue projections have missed the mark month after month.[EXP]full_input_length : 189\n",
            "\n",
            "full_input_length : 103\n",
            "concatinated_input : Metadata: income,new-hampshire-2012 joe-biden U.S. senator Delaware democrat speaking at New Hampshire’s Plymouth State University Statement: The median income of a middle class family went down $2,100 from 2001 to 2007.[EXP]concatinated_input : Metadata: economy,energy,environment,jobs,transportation tammy-baldwin U.S. Representative Wisconsin democrat a news release Statement: Over 3 million Americans are employed in the growing green-collar workforce, which is more than the number of people working in the fossil fuel industry.[EXP]\n",
            "\n",
            "full_input_length : 155full_input_length : 139\n",
            "\n",
            "concatinated_input : Metadata: economy,stimulus barack-obama President Illinois democrat his State of the Union address Statement: Because of the steps we took, there are about 2 million Americans working right now who would otherwise be unemployed.[EXP]concatinated_input : Metadata: medicaid,social-security,taxes margaret-carlson Columnist District of Columbia none a politics column. Statement: Rick Perry has advocated abandoning Social Security, scuttling Medicaid and ending the federal income tax.[EXP]\n",
            "\n",
            "full_input_length : 130full_input_length : 130\n",
            "\n",
            "concatinated_input : Metadata: city-budget,city-government,environment,public-health,water amanda-fritz Portland city commissioner None democrat in campaign literature Statement: Says that In 2009, I saved ratepayers around $500 million by persuading the Council to pursue a less expensive compliance mechanism if the City is required to treat Bull Run drinking water.[EXP]concatinated_input : Metadata: health-care,poverty,public-health,welfare elizabeth-roberts Lieutenant Governor Rhode Island democrat a panel discussion on \"A Lively Experiment\" Statement: Two thirds to three quarters of people without [health] insurance in Rhode Island work.[EXP]\n",
            "\n",
            "full_input_length : 121\n",
            "full_input_length : 151\n",
            "concatinated_input : Metadata: congress john-barrow Congressman Georgia democrat a letter Statement: Congress has spent 66 of the first 100 days of this term in recess.[EXP]concatinated_input : Metadata: military joe-morrissey Delegate Virginia independent a floor speech. Statement: The military has spent $500 million enforcing the Dont Ask, Dont Tell policy regarding gays and lesbians in the military.[EXP]\n",
            "\n",
            "full_input_length : 142full_input_length : 104\n",
            "\n",
            "concatinated_input : Metadata: taxes philadelphia-daily-news None None none In an editorial Statement: If you dont buy cigarettes at your local supermarket, your grocery bill wont go up a dime. The same is true of the sugary drink tax. If passed, you can avoid paying the tax by not buying sugary drinks.[EXP]\n",
            "concatinated_input : Metadata: economy,job-accomplishments,jobs john-kasich Governor of Ohio as of Jan. 10, 2011 Ohio republican an ad from the Republican Governors Association Statement: Were. . . keeping and creating jobs in our state. From American Greetings, to Wendys, to Diebold, weve gone to their doorsteps to keep jobs right here in Ohio.[EXP]full_input_length : 177\n",
            "\n",
            "full_input_length : 443\n",
            "concatinated_input : Metadata: bankruptcy lynn-westmoreland None None republican a meeting Statement: Georgia has had ʺmore bank failures than any other state.ʺ[EXP]\n",
            "concatinated_input : Metadata: children,federal-budget jim-demint President, Heritage Foundation South Carolina republican a blog post Statement: From 2003 to 2006, Sesame Street made more than $211 million from toy and consumer product sales.[EXP]full_input_length : 93\n",
            "\n",
            "full_input_length : 171\n",
            "concatinated_input : Metadata: children,congress,economy,education,jobs,state-budget,state-finances emilys-list None None organization a television ad Statement: Thom Tillis cut almost $500 million from education.[EXP]\n",
            "full_input_length : 156concatinated_input : Metadata: economy,federal-budget,job-accomplishments,message-machine john-kasich Governor of Ohio as of Jan. 10, 2011 Ohio republican a campaign video Statement: (John) Kasich was the architect who balanced the budget, cut spending, created a surplus, igniting record job creation.[EXP]\n",
            "\n",
            "full_input_length : 113\n",
            "concatinated_input : Metadata: government-regulation,market-regulation rick-scott Governor Florida republican speech at CPAC Statement: We are poised to get rid of over 1,000 more regulations in 2012.[EXP]\n",
            "full_input_length : 122concatinated_input : Metadata: cap-and-trade,climate-change,environment ohio-coal-association Lobbying group for Ohio's coal industry Ohio none testimony to a House subcommittee Statement: Says that President Obama said in 2008 that his proposed greenhouse gas regulations will bankrupt anyone who wants to build a new coal-fired power plant.[EXP]\n",
            "\n",
            "full_input_length : 180\n",
            "concatinated_input : Metadata: education marco-rubio U.S. Senator Florida republican a book Statement: Administrative employees at colleges and universitieshave more than doubled over the last 25 years, outpacing the growth of students by more than 2 to 1.[EXP]\n",
            "full_input_length : 117\n",
            "concatinated_input : Metadata: debt chris-christie Governor of New Jersey New Jersey republican a speech at the Republican National Convention in Tampa Statement: There has been $5 trillion in debt added over the last four years.[EXP]\n",
            "full_input_length : 99\n",
            "concatinated_input : Metadata: immigration,public-safety bill-white Former mayor of Houston Texas democrat an interview. Statement: Says that under Gov. Rick Perry, Texas Department of Public Safety troopers have had standing orders not to inquire into the immigration status of people unless theyre under arrest.[EXP]\n",
            "full_input_length : 99\n",
            "concatinated_input : Metadata: federal-budget facebook-posts Social media posting None none a Facebook post Statement: The cost of the food stamp program is at an all-time high.[EXP]\n",
            "full_input_length : 62\n",
            "concatinated_input : Metadata: federal-budget,health-care,medicaid scott-walker Milwaukee County Executive Wisconsin republican a TV ad Statement: Says opponent Mary Burke says she supports Obamacare unequivocally and wants to expand it.[EXP]\n",
            "concatinated_input : Metadata: federal-budget,pundits bill-burton Managing director, Global Strategy Group None democrat an appearance on CNN's \"Crossfire\" Statement: Democrats already agreed to a deal that Republicans wanted, that Eric Cantor said would be a win.[EXP]full_input_length : 80\n",
            "\n",
            "full_input_length : 218\n",
            "concatinated_input : Metadata: taxes joseph-chiusolo Municipal Councilman New Jersey republican a press release Statement: Says Essex County residents suffer the second highest property taxes in the nation.[EXP]\n",
            "full_input_length : 55\n",
            "concatinated_input : Metadata: campaign-finance,supreme-court charles-schumer Senator New York democrat a press conference Statement: Eight of the nine justices in the Supreme Court decision (on campaign finance) said that not only is it constitutional for Congress to require disclosure of the special interest money, but they recommend we do it.[EXP]\n",
            "full_input_length : 159concatinated_input : Metadata: candidates-biography ken-lanci chairman and chief executive of Consolidated Graphics Group Inc., Ohio independent news release, website Statement: Ken Lanci is a lifelong Clevelander[EXP]\n",
            "\n",
            "full_input_length : 99\n",
            "concatinated_input : Metadata: terrorism richard-durbin Senator Illinois democrat a speech on the Senate floor. Statement: The reality is, we are holding some of the most dangerous terrorists in the world right now in our federal prisons, including the mastermind of the 1993 World Trade Center bombing, the 'shoe bomber,' the 'Unabomber,' and many others.[EXP]\n",
            "concatinated_input : Metadata: health-care,state-budget,state-finances,unions,workers mike-tate None None democrat an interview on MSNBC Statement: Says Wisconsin Gov. Scott Walker slashed pensions and benefits for public employees.[EXP]full_input_length : 189\n",
            "\n",
            "full_input_length : 134\n",
            "concatinated_input : Metadata: foreign-policy,legal-issues,military,terrorism republican-national-committee-republican None None republican a GOP survey Statement: Under the new and little known 'global justice' initiative, the Obama administration has ordered FBI agents to \"read Miranda Rights to high-value terrorist detainees captured on the battlefield.\"[EXP]concatinated_input : Metadata: agriculture,energy james-durkan Candidate for U.S. Senate Tennessee republican a statement on his campaign website. Statement: Adding ethanol to gas raises food costs.[EXP]\n",
            "\n",
            "full_input_length : 104full_input_length : 260\n",
            "\n",
            "concatinated_input : Metadata: homeland-security,technology,terrorism edward-snowden None Russia libertarian an interview on \"Last Week Tonight\" Statement: Wholly domestic communication between you and your wife can go to New York to London and back and get caught up in the (NSA) database.[EXP]concatinated_input : Metadata: drugs,legal-issues,marijuana william-devereaux lawyer Rhode Island none a legislative hearing  Statement: You can import as many hemp products into this country as you want...but we cant grow it.[EXP]\n",
            "full_input_length : 190\n",
            "\n",
            "full_input_length : 108concatinated_input : Metadata: government-efficiency kate-brown Secretary of State Oregon democrat a speech at the Oregon Summit. Statement: Says for every dollar the state spent on audits last year, it delivered $64 in cost savings.[EXP]\n",
            "\n",
            "concatinated_input : Metadata: taxes barack-obama President Illinois democrat a campaign speech in Ohio Statement: Mitt Romney has proposed cutting his own taxes while raising them on 18 million working families.[EXP]full_input_length : 105\n",
            "\n",
            "full_input_length : 157\n",
            "concatinated_input : Metadata: education mitt-romney Former governor Massachusetts republican a speech to the NAACP Statement: When I was governor, not only did test scores improve we also narrowed the achievement gap.[EXP]\n",
            "concatinated_input : Metadata: taxes joe-pennacchio State Senator New Jersey republican a news release Statement: Says Pennsylvania charges a top income tax rate of 3 percent and Delaware has no state income tax at all.[EXP]full_input_length : 173\n",
            "full_input_length : 74\n",
            "\n",
            "concatinated_input : Metadata: redistricting nikema-williams Interim  of the Democratic Party of Georgia Georgia democrat a party press release Statement: Georgia is nearly 50 percent Democratic and (the Republican majority) diminished our voting strength to 32 percent through gerrymandered maps.[EXP]\n",
            "full_input_length : 106concatinated_input : Metadata: candidates-biography,elections,message-machine kendrick-meek None Florida democrat a TV ad. Statement: Jeff Greene can buy anything ... he owns two mansions.[EXP]\n",
            "\n",
            "full_input_length : 125concatinated_input : Metadata: elections,states chris-christie Governor of New Jersey New Jersey republican a news conference Statement: Says a Republican hasnt won [an election] for a presidency in New Jersey since 1988.[EXP]\n",
            "\n",
            "full_input_length : 122\n",
            "concatinated_input : Metadata: corrections-and-updates,guns hillary-clinton Presidential candidate New York democrat a debate in Iowa Statement: Since we last debated in Las Vegas, nearly 3,000 people have been killed by guns.[EXP]\n",
            "full_input_length : 182concatinated_input : Metadata: agriculture,homeland-security,immigration,jobs,population rand-paul Candidate for U.S. Senate and physician Kentucky republican a news conference Statement: 40 percent of illegal immigrants had a visa and then became illegal, mostly because they changed jobs.[EXP]\n",
            "\n",
            "full_input_length : 110\n",
            "concatinated_input : Metadata: campaign-finance,occupy-wall-street occupy-wall-street Protest movement New York activist a sign at an Occupy Wall Street protest Statement: 94 percent of winning candidates in 2010 had more money than their opponents.[EXP]\n",
            "full_input_length : 193concatinated_input : Metadata: candidates-biography,congress david-dewhurst Lieutenant governor Texas republican a press release. Statement: Says Ted Cruz slurred Republican senators including John Cornyn as graybeards and spineless jellyfish.[EXP]\n",
            "concatinated_input : Metadata: economy,health-care michael-mccaul congressman Texas republican an e-mail Statement: Says the federal agency in charge of Medicare and Medicaid will disburse $803 billion in benefits this year, making it larger than all but 15 of the worlds economies.[EXP]\n",
            "full_input_length : 125\n",
            "\n",
            "full_input_length : 115concatinated_input : Metadata: veterans tom-graves None None republican a ZPolitics op-ed Statement: Despite having their budget increased by over 40 percent since 2009 pending claims for benefits with the (Department of Veterans Affairs) have increased from 391,000 to 890,000 under the Obama Administration.[EXP]\n",
            "\n",
            "concatinated_input : Metadata: state-budget charlie-crist None Florida democrat a statement on a campaign Web site Statement: Marco Rubio \"supported $800,000 for AstroTurf for a field where he played flag football.\"[EXP]full_input_length : 115\n",
            "\n",
            "full_input_length : 120concatinated_input : Metadata: elections gary-johnson Presidential candidate New Mexico libertarian an interview with Evan Smith on “Overheard,” premiered Sept. 6, 2012 Statement: I am going to be on the ballot in all 50 states. There is no other third-party candidate thats going to come close to achieving that.[EXP]\n",
            "\n",
            "full_input_length : 115\n",
            "concatinated_input : Metadata: economy virginia-tea-party-patriots None None none a letter to Congress. Statement: The dollar has fallen over 44% since Bernanke began pumping money into the system beginning back in 2002.[EXP]\n",
            "concatinated_input : Metadata: history,terrorism bobby-scott U.S. Congressman Virginia democrat a news release. Statement: After World War II, we tried, convicted, and, in some cases, executed Japanese soldiers for war crimes that included charges of waterboarding.[EXP]full_input_length : 161\n",
            "labels : tensor([[ 9171, 14706,    25,  ..., 50256, 50256, 50256],\n",
            "        [ 9171, 14706,    25,  ..., 50256, 50256, 50256],\n",
            "        [ 9171, 14706,    25,  ..., 50256, 50256, 50256],\n",
            "        ...,\n",
            "        [ 9171, 14706,    25,  ..., 50256, 50256, 50256],\n",
            "        [ 9171, 14706,    25,  ..., 50256, 50256, 50256],\n",
            "        [ 9171, 14706,    25,  ..., 50256, 50256, 50256]])\n",
            "labels : tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
            "        [-100, -100, -100,  ..., -100, -100, -100],\n",
            "        [-100, -100, -100,  ..., -100, -100, -100],\n",
            "        ...,\n",
            "        [-100, -100, -100,  ..., -100, -100, -100],\n",
            "        [-100, -100, -100,  ..., -100, -100, -100],\n",
            "        [-100, -100, -100,  ..., -100, -100, -100]])\n",
            "\n",
            "concatinated_input : Metadata: public-health hillary-clinton Presidential candidate New York democrat a post on Medium Statement: Congress will begin its recess without having allocated one penny to fight Zika.[EXP]full_input_length : 170\n",
            "full_input_length : 134\n",
            "\n",
            "concatinated_input : Metadata: crime,message-machine,states jane-norton None Colorado republican a television ad Statement: Ken Bucks (District Attorneys) office? His spending skyrocketed by 40 percent.[EXP]concatinated_input : Metadata: energy,federal-budget,health-care judd-gregg U.S. Senator None republican a speech on the Senate Floor. Statement: On the propriety of budget reconciliation.[EXP]\n",
            "\n",
            "full_input_length : 112full_input_length : 127\n",
            "\n",
            "concatinated_input : Metadata: health-care planned-parenthood None None none Denver Statement: McCain \"said he was 'stumped' when asked whether contraceptives help stop the spread of HIV.\"[EXP]concatinated_input : Metadata: labor,small-business,workers armond-budish Minority leader in the Ohio House of Representatives Ohio democrat a letter to editor in The Plain Dealer Statement: Says legislation pending in the House would effectively limit or eliminate time-and-a-half for people who work overtime.[EXP]\n",
            "\n",
            "full_input_length : 130\n",
            "full_input_length : 134\n",
            "concatinated_input : Metadata: federal-budget cliff-stearns U.S. representative Florida republican a press release Statement: The cost for renovating the headquarters of the U.N. has doubled from the original estimate.[EXP]\n",
            "full_input_length : 164\n",
            "concatinated_input : Metadata: message-machine-2012,taxes barack-obama President Illinois democrat a TV ad Statement: Mitt Romney is proposing a tax plan that would give millionaires another tax break and raises taxes on middle class families by up to $2,000 a year.[EXP]\n",
            "full_input_length : 251\n",
            "concatinated_input : Metadata: crime,homeland-security,legal-issues,military,terrorism joe-biden U.S. senator Delaware democrat an interview on \"Face the Nation\" Statement: There have been three people tried and convicted by the last administration in military courts. Two are walking the street right now.[EXP]\n",
            "full_input_length : 183\n",
            "concatinated_input : Metadata: education,sports arian-foster Running back, Houston Texans None none a tweet Statement: The NCAA will get billions from the mens basketball tournament. Players get a trophy.[EXP]\n",
            "full_input_length : 206\n",
            "concatinated_input : Metadata: state-budget ron-amstutz State representative Ohio republican a newspaper column Statement: The budget currently being debated significantly decreases the use of one-time resources.[EXP]\n",
            "full_input_length : 162\n",
            "concatinated_input : Metadata: jobs keith-ellison Member, U.S. House of Representatives Minnesota democrat comments on ABC's \"This Week\" Statement: Americans work way more than an average of industrialized countries around the world.[EXP]\n",
            "full_input_length : 139\n",
            "concatinated_input : Metadata: guns rob-portman U.S. senator from Ohio Ohio republican the Ohio Senate debate Statement: Says Ted Strickland in this campaign bragged about his A-plus rating with the NRA. ... He has said he has a mixed and spotty record on this issue and that he can be criticized for it. Those are his words, not mine. So I dont know where he is on this issue.[EXP]\n",
            "full_input_length : 153\n",
            "concatinated_input : Metadata: animals,government-regulation,health-care,medicaid,medicare,market-regulation ted-poe U.S. House Representative Texas republican an interview on Fox Business Network Statement: Says new Medicare billing guidelines have nine codes for (injuries by) turkeys.[EXP]\n",
            "full_input_length : 135\n",
            "concatinated_input : Metadata: iraq john-mccain U.S. senator Arizona republican a TV ad Statement: One man opposed a flawed strategy in Iraq. One man had the courage to call for change. One man didn't play politics with the truth.[EXP]\n",
            "full_input_length : 96\n",
            "concatinated_input : Metadata: bipartisanship,health-care,supreme-court,abc-news-week bill-clinton former president Arkansas democrat an interview on ABC's 'This Week' Statement: I never had a filibuster-proof Senate.[EXP]\n",
            "full_input_length : 138\n",
            "concatinated_input : Metadata: education dan-patrick Lieutenant governor-elect Texas republican Senate Education Committee hearing Statement: Says 100,000 are on waiting list to attend Texas charter schools.[EXP]\n",
            "full_input_length : 183\n",
            "concatinated_input : Metadata: foreign-policy russia-today None None none a posting on their website Statement: The new Ukrainian government introduced a law abolishing the use of languages other than Ukrainian in official circumstances.[EXP]\n",
            "full_input_length : 183\n",
            "concatinated_input : Metadata: children,diversity,poverty donald-trump President-Elect New York republican a rally in Springfield, Ohio Statement: Nearly half of African-American children under the age of 6 are living in abject poverty.[EXP]\n",
            "full_input_length : 154\n",
            "concatinated_input : Metadata: foreign-policy,terrorism donald-trump President-Elect New York republican a speech to the American Israel Public Affairs Committee Statement: During the last five years, Iran has perpetrated terror attacks in 25 different countries on five continents.[EXP]\n",
            "full_input_length : 148\n",
            "concatinated_input : Metadata: military jack-kingston U.S. Representative Georgia republican Campaign ad Statement: President Obama has left the U.S. with the lowest number of active-duty troops since before World War II, putting the nation at risk.[EXP]\n",
            "full_input_length : 110\n",
            "concatinated_input : Metadata: immigration james-jolly None None none a University of Georgia System newsletter Statement: Every student paying out-of-state tuition actually covers more than the cost of instruction.[EXP]\n",
            "full_input_length : 165\n",
            "concatinated_input : Metadata: education nancy-gordeuk founder, TNT Academy Georgia none an online posting Statement: The Georgia Department of Education has implemented a new policy beginning in August that states that public schools will no longer accept credits from home school entities or non-traditional education centers.[EXP]\n",
            "full_input_length : 88\n",
            "concatinated_input : Metadata: county-budget,county-government,crime chris-abele Philanthropist Wisconsin none a speech Statement: The Milwaukee County sheriffs department plays only a limited role as a traditional law enforcement agency and in 2009 reported far fewer crimes to the FBI than the University of Wisconsin-Milwaukee police did.[EXP]\n",
            "full_input_length : 166\n",
            "concatinated_input : Metadata: campaign-finance bernie-s U.S. Senator Vermont independent comments on Twitter. Statement: Unlike virtually every other campaign, we dont have a super PAC.[EXP]\n",
            "full_input_length : 124\n",
            "concatinated_input : Metadata: immigration donald-trump President-Elect New York republican a speech in Arizona Statement: Within just a few years, immigration as a share of national population is set to break all historical records.[EXP]\n",
            "full_input_length : 136\n",
            "concatinated_input : Metadata: candidates-biography,message-machine-2014 mark-warner U.S. Senator Virginia democrat a TV ad. Statement: Ed Gillespies firm even lobbied for five foreign governments, including a dictator now awaiting trial for war crimes.[EXP]\n",
            "full_input_length : 212\n",
            "concatinated_input : Metadata: education,sports,state-budget jim-moran U. S. Congressman Virginia democrat a Facebook post Statement: We have a system now where in 40 states, the highest-paid public employee is the state universitys head football or basketball coach.[EXP]\n",
            "full_input_length : 166\n",
            "concatinated_input : Metadata: crime,drugs,homeland-security,immigration rick-perry Governor Texas republican an interview Statement: Because of violence spreading from Mexico, youve got bullets hitting the city hall in El Paso. Youve got bombs exploding in El Paso.[EXP]\n",
            "full_input_length : 156\n",
            "concatinated_input : Metadata: education,guns lisa-moore Professor, Department of English, The University of Texas at Austin Texas none an interview for NPR's \"All Things Considered\" Statement: Says there are concrete examples of University of Texas job applicants or prospective applicants and students as well as invited speakers changing their minds because of handguns being allowed in campus buildings and classrooms.[EXP]\n",
            "full_input_length : 197\n",
            "concatinated_input : Metadata: immigration lars-larson radio talk show host Oregon none  an email Statement: TSA WILL ACCEPT DRIVERS PRIV CARDS FOR ID AT THE AIRPORT[EXP]\n",
            "full_input_length : 120\n",
            "concatinated_input : Metadata: immigration marco-rubio U.S. Senator Florida republican an appearance on 'Fox & Friends' Statement: Forty percent of people in this country, illegally, are overstaying visas.[EXP]\n",
            "full_input_length : 143\n",
            "concatinated_input : Metadata: economy,message-machine,stimulus,taxes american-crossroads None None republican an election ad Statement: Says Lee Fisher wanted a $1.1 billion tax increase which could have driven countless jobs out of state.[EXP]\n",
            "full_input_length : 78\n",
            "concatinated_input : Metadata: candidates-biography,transparency josh-mandel Ohio treasurer Ohio republican a news release Statement: Ilana Shafran Mandels stake in Forest City Enterprises constitutes significantly less than 1 percent of the companys shares and any implication of a conflict of interest is legally incorrect.[EXP]\n",
            "full_input_length : 147\n",
            "concatinated_input : Metadata: state-budget jeff-amason attorney Georgia libertarian interview on The Monica Perez Show, WSB Statement: The state budget has increased almost $800 million a year in each of the last two years.[EXP]\n",
            "full_input_length : 160\n",
            "concatinated_input : Metadata: water brigid-shea Principal, Carbon Shrinks LLC Texas democrat an op-ed column. Statement: In the past 10 years, our (Austin) water rates have increased by 100 percent and we now have the highest water cost of the top 10 cities in Texas.[EXP]\n",
            "full_input_length : 133\n",
            "labels : tensor([[ 9171, 14706,    25,  ..., 50256, 50256, 50256],\n",
            "        [ 9171, 14706,    25,  ..., 50256, 50256, 50256],\n",
            "        [ 9171, 14706,    25,  ..., 50256, 50256, 50256],\n",
            "        ...,\n",
            "        [ 9171, 14706,    25,  ..., 50256, 50256, 50256],\n",
            "        [ 9171, 14706,    25,  ..., 50256, 50256, 50256],\n",
            "        [ 9171, 14706,    25,  ..., 50256, 50256, 50256]])\n",
            "labels : tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
            "        [-100, -100, -100,  ..., -100, -100, -100],\n",
            "        [-100, -100, -100,  ..., -100, -100, -100],\n",
            "        ...,\n",
            "        [-100, -100, -100,  ..., -100, -100, -100],\n",
            "        [-100, -100, -100,  ..., -100, -100, -100],\n",
            "        [-100, -100, -100,  ..., -100, -100, -100]])\n",
            "concatinated_input : Metadata: congress,economy,elections,job-accomplishments,jobs gwen-moore U.S. House member -- 4th District Wisconsin democrat a tweet Statement: Says Mitch McConnell credits Republicans for recent economic improvements even though they took control of the Senate only days ago.[EXP]\n",
            "full_input_length : 88\n",
            "concatinated_input : Metadata: taxes rudy-giuliani Attorney New York republican Dearborn, Mich. Statement: I believe in tax cuts. I believe in being a supply-sider. I cut the income tax I think it was 24 percent. We got 42 percent more revenues.[EXP]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf2ea8ca8ed445e7b04aa41cd79386b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "full_input_length : 203\n",
            "concatinated_input : Metadata: history,immigration tim-kaine U.S. Senator Virginia democrat a speech in Phoenix Statement: Spanish was the first European language spoken in this country.[EXP]\n",
            "full_input_length : 283\n",
            "concatinated_input : Metadata: jobs rick-scott Governor Florida republican a statement Statement: Since I took office, weve created 76,800 jobs.[EXP]\n",
            "full_input_length : 119\n",
            "concatinated_input : Metadata: foreign-policy,iraq,terrorism michael-mccaul congressman Texas republican the Republican weekly radio/TV address Statement: Irans regime is responsible for more than 1,000 American casualties during the Iraq war and has plotted a terrorist attack here in our nations capital.[EXP]\n",
            "full_input_length : 186\n",
            "labels : tensor([[ 9171, 14706,    25,  8681,    11, 13926,    88,    11,  9509,   507,\n",
            "            11, 21858,    12,   330, 23855,   680,   902,    11, 43863,   308,\n",
            "         21006,    12,  5908,   382,   471,    13,    50,    13,  2097,  2888,\n",
            "          1377,   604,   400,  5665,  9279, 43268,   257,  6126, 21983,    25,\n",
            "         28628, 20472, 18184, 10824,  4734,   329,  2274,  3034,  8561,   772,\n",
            "           996,   484,  1718,  1630,   286,   262,  3845,   691,  1528,  2084,\n",
            "            13, 50257, 38708,   468, 11434,   262,  5466,   286,   262, 13016,\n",
            "            11,  2282,   484,   717,  2622,   262,  8281,   286,   262, 20815,\n",
            "            13,   383, 13016,   389,   890,  3750,    13, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100, 38708,   468, 11434,   262,  5466,   286,   262, 13016,\n",
            "            11,  2282,   484,   717,  2622,   262,  8281,   286,   262, 20815,\n",
            "            13,   383, 13016,   389,   890,  3750,    13, 50256]])\n",
            "concatinated_input : Metadata: education rick-scott Governor Florida republican the State of the State speech Statement: Florida high schools are four out of the top 10 in the entire United States.[EXP]\n",
            "full_input_length : 167\n",
            "labels : tensor([[ 9171, 14706,    25,  5704, 28906,    88,    12, 12397,   377, 25111,\n",
            "          8123,   968,  1971, 41477, 23420,  6286,    11,  2843,    13, 21983,\n",
            "            25,   314,  1975,   287,  1687,  6630,    13,   314,  1975,   287,\n",
            "           852,   257,  5127,    12,    82,  1304,    13,   314,  2005,   262,\n",
            "          3739,  1687,   314,   892,   340,   373,  1987,  1411,    13,   775,\n",
            "          1392,  5433,  1411,   517, 13089,    13, 50257,     1,  1890,   607,\n",
            "            11,   262, 23082,   286,   428,   966,   318,   326,  2717,  1042,\n",
            "          1377,   262,   471,    13,   311,    13,   220,  7965,   338,  1598,\n",
            "         46925,   341,   286,  5635,  1022,   262,  2717,   290,  1181,  6905,\n",
            "          1377,   468,  1464,   587,   257,  7531,  4843,   286,   674,  7965,\n",
            "           290,   286,   674,  2260, 15012,    13, 13248,   306,  5475,   422,\n",
            "           597,  2450,    12,  3106, 37990,   351,   262,  2717, 11409,  1099,\n",
            "            11, 24497,   318,  5364,   284,  4744,   338,  8087, 10582,   780,\n",
            "           673,  7224,   340,   355,  3306,   284, 12201,   674,  2717,   396,\n",
            "          1080,   286,  1230,    13,   366,  4677,  6648,   319,  5426,  3000,\n",
            "            11, 12812,    72,  1392,   572,  2610,   287, 11142,   257,  2742,\n",
            "          4427,   284,   262,  2717,  1535,  1337,  1099,   416,  2282,   262,\n",
            "          1578,  1829,   468,   262,   366,  8807,  1080,   286,  2717,  1042,\n",
            "           287,   262,   995,    13,   366,   464,  1578,  1829,   743,   423,\n",
            "           587,   262,   717,    11,   475,   340,   338,  1290,   422,   262,\n",
            "           691,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,     1,  1890,   607,\n",
            "            11,   262, 23082,   286,   428,   966,   318,   326,  2717,  1042,\n",
            "          1377,   262,   471,    13,   311,    13,   220,  7965,   338,  1598,\n",
            "         46925,   341,   286,  5635,  1022,   262,  2717,   290,  1181,  6905,\n",
            "          1377,   468,  1464,   587,   257,  7531,  4843,   286,   674,  7965,\n",
            "           290,   286,   674,  2260, 15012,    13, 13248,   306,  5475,   422,\n",
            "           597,  2450,    12,  3106, 37990,   351,   262,  2717, 11409,  1099,\n",
            "            11, 24497,   318,  5364,   284,  4744,   338,  8087, 10582,   780,\n",
            "           673,  7224,   340,   355,  3306,   284, 12201,   674,  2717,   396,\n",
            "          1080,   286,  1230,    13,   366,  4677,  6648,   319,  5426,  3000,\n",
            "            11, 12812,    72,  1392,   572,  2610,   287, 11142,   257,  2742,\n",
            "          4427,   284,   262,  2717,  1535,  1337,  1099,   416,  2282,   262,\n",
            "          1578,  1829,   468,   262,   366,  8807,  1080,   286,  2717,  1042,\n",
            "           287,   262,   995,    13,   366,   464,  1578,  1829,   743,   423,\n",
            "           587,   262,   717,    11,   475,   340,   338,  1290,   422,   262,\n",
            "           691,    13, 50256]])\n",
            "concatinated_input : Metadata: drugs,health-care,marijuana grady-judd Polk County sheriff Florida republican a \"Sun-Sentinel\" editorial Statement: Floridas proposed amendment for medical marijuana would allow people who alleged minor ailments such as muscle spasms, neck pain, back pain and even menstrual cramps (to qualify) for government-sanctioned pot-smoking.[EXP]\n",
            "full_input_length : 220\n",
            "labels : tensor([[ 9171, 14706,    25,  2106,    11, 47620,  4628,    12,    74,  5718,\n",
            "           471,    13,    50,    13,  8962,  6025, 43268,   257,  4046,   287,\n",
            "          9643, 21983,    25,  7897,   373,   262,   717,  3427,  3303,  9635,\n",
            "           287,   428,  1499,    13, 50257, 18102, 14931, 20320,  4705,   494,\n",
            "         18798,   531,    11,   366,  1858,  4398,   470,   587,  1687,  6630,\n",
            "           329,   262,  5527,     1,   287, 24545,  5451,    13,   679,  1568,\n",
            "          7368,   257,   640,  5739,    25,  7236,    13,   383,  3050,  1687,\n",
            "          2458,  1377,   543,  5710,   262,  1353,  1687,  2494,   416,  1440,\n",
            "          5873,  2173,  1377,   318,  1690, 11987,   355,   257,  1687,  2005,\n",
            "           329,   262,  5527,    11,   475,   262, 11574,  1682,  3432,   517,\n",
            "            13,  1119,   635,  3432,   517,   706,   262,  3717,  1487,    11,\n",
            "           543,  1444,   329, 36587,  3139,  8810,   355,  8850,  3739,    13,\n",
            "           887,   356,   691,   550,   284,   467,   736,   284,  4793,   284,\n",
            "          1064,  1687,  5520,   326, 11673,  1028,   465,   966,    13,  4900,\n",
            "           645,  2392,   287,  1245,    11,   340, 10893,   281,  1683,    12,\n",
            "         21037,  1451,   319,  1181,  5704,   326,  2204,  2175,   517,   290,\n",
            "           517,  5527,   661,   355,   262,  1687,  1451,  5710,  2793,   290,\n",
            "          2793,    13,   843,   262,  1946,  1487,   287,   262,  7964,  1687,\n",
            "          4001,   326, 49311, 42252,  6304,   470, 31075,   355,  7272,    13,\n",
            "         14674,    11,   356,  1975,   262,  5527, 28719,  4414,   422,   428,\n",
            "          1487,   772,   996,   477, 40862,    11,   279,   559, 19276,   290,\n",
            "         42676, 12936,    11,   651,   262,   976,  2270,    13,   843,  1061,\n",
            "           514,   319,  3009,    25,  2488, 34470, 29660,   380,    13,  1267,\n",
            "             7, 42779,   507,    25, 11646,  5426,   373,  2097, 22171, 10540,\n",
            "           618,   339,  6619,   284,   262,  8894,  1044,  5136,   546,   262,\n",
            "          4793,  1687,  2458,    13,   383,  4238,  2196,   286,   428,  2378,\n",
            "         23175,   531,   339,   373,  2097, 14931,    13,  3759,  1879,    66,\n",
            "         29864,   373,   262,  8153,   618,   262,  3050,  1687,  2458,   547,\n",
            "          3804,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100, 18102, 14931, 20320,  4705,   494,\n",
            "         18798,   531,    11,   366,  1858,  4398,   470,   587,  1687,  6630,\n",
            "           329,   262,  5527,     1,   287, 24545,  5451,    13,   679,  1568,\n",
            "          7368,   257,   640,  5739,    25,  7236,    13,   383,  3050,  1687,\n",
            "          2458,  1377,   543,  5710,   262,  1353,  1687,  2494,   416,  1440,\n",
            "          5873,  2173,  1377,   318,  1690, 11987,   355,   257,  1687,  2005,\n",
            "           329,   262,  5527,    11,   475,   262, 11574,  1682,  3432,   517,\n",
            "            13,  1119,   635,  3432,   517,   706,   262,  3717,  1487,    11,\n",
            "           543,  1444,   329, 36587,  3139,  8810,   355,  8850,  3739,    13,\n",
            "           887,   356,   691,   550,   284,   467,   736,   284,  4793,   284,\n",
            "          1064,  1687,  5520,   326, 11673,  1028,   465,   966,    13,  4900,\n",
            "           645,  2392,   287,  1245,    11,   340, 10893,   281,  1683,    12,\n",
            "         21037,  1451,   319,  1181,  5704,   326,  2204,  2175,   517,   290,\n",
            "           517,  5527,   661,   355,   262,  1687,  1451,  5710,  2793,   290,\n",
            "          2793,    13,   843,   262,  1946,  1487,   287,   262,  7964,  1687,\n",
            "          4001,   326, 49311, 42252,  6304,   470, 31075,   355,  7272,    13,\n",
            "         14674,    11,   356,  1975,   262,  5527, 28719,  4414,   422,   428,\n",
            "          1487,   772,   996,   477, 40862,    11,   279,   559, 19276,   290,\n",
            "         42676, 12936,    11,   651,   262,   976,  2270,    13,   843,  1061,\n",
            "           514,   319,  3009,    25,  2488, 34470, 29660,   380,    13,  1267,\n",
            "             7, 42779,   507,    25, 11646,  5426,   373,  2097, 22171, 10540,\n",
            "           618,   339,  6619,   284,   262,  8894,  1044,  5136,   546,   262,\n",
            "          4793,  1687,  2458,    13,   383,  4238,  2196,   286,   428,  2378,\n",
            "         23175,   531,   339,   373,  2097, 14931,    13,  3759,  1879,    66,\n",
            "         29864,   373,   262,  8153,   618,   262,  3050,  1687,  2458,   547,\n",
            "          3804,    13, 50256]])\n",
            "concatinated_input : Metadata: agriculture,hunger,welfare barbara-lee Congresswoman from California's 13th district California democrat an interview with MSNBC Statement: Proposed cuts in the House farm bill mean 2 million less people on food stamps, 210,000 children will not receive school lunches or breakfasts.[EXP]\n",
            "full_input_length : 157\n",
            "labels : tensor([[ 9171, 14706,    25,  3946,   374,   624,    12,  1416,  1252, 10807,\n",
            "          4744, 41477,   257,  2643, 21983,    25,  4619,   314,  1718,  2607,\n",
            "            11,   356,   303,  2727,  8684,    11,  7410,  3946,    13, 50257,\n",
            "         11158,    11,   339,   531,    11,   867,  4165,  1337, 17206,  3588,\n",
            "           470,  4379,   649,  3871,   379,   477,    11,  1771,   484,   389,\n",
            "           319, 13594,   393,   423,  2839,  5096,    13,   554,   262,   717,\n",
            "           636,   286,   465,  2643,    11, 10009,   318,   287,   262, 40598,\n",
            "            25,  4377,  3003,   422,  1315,   284,  1248,  1510,   649,  3871,\n",
            "           714, 14627,   287, 13594,   611,   262,  1535,  1337, 18708,   318,\n",
            "          1234,   656,  1099,    13,   775,   635,  1043,   326, 10009,   338,\n",
            "         10238,   966,    11,   326,   617, 13594,  3871,  1244,   423,  5876,\n",
            "          4917,  7519,    11,   318,   257,  4938,  2328,    13, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         11158,    11,   339,   531,    11,   867,  4165,  1337, 17206,  3588,\n",
            "           470,  4379,   649,  3871,   379,   477,    11,  1771,   484,   389,\n",
            "           319, 13594,   393,   423,  2839,  5096,    13,   554,   262,   717,\n",
            "           636,   286,   465,  2643,    11, 10009,   318,   287,   262, 40598,\n",
            "            25,  4377,  3003,   422,  1315,   284,  1248,  1510,   649,  3871,\n",
            "           714, 14627,   287, 13594,   611,   262,  1535,  1337, 18708,   318,\n",
            "          1234,   656,  1099,    13,   775,   635,  1043,   326, 10009,   338,\n",
            "         10238,   966,    11,   326,   617, 13594,  3871,  1244,   423,  5876,\n",
            "          4917,  7519,    11,   318,   257,  4938,  2328,    13, 50256]])\n",
            "concatinated_input : Metadata: elections hillary-clinton Presidential candidate New York democrat a conference call to reporters led by Cilnton pollster Mark Penn Statement: Senator Obama has, in fact, never had a serious Republican challenger.[EXP]\n",
            "full_input_length : 126\n",
            "labels : tensor([[ 9171, 14706,    25,  3215,    12, 30586,    11,  8704,    80,    11,\n",
            "         19541,   285, 40302,    12,    76,   535,  2518, 29545,  3936, 41477,\n",
            "           262,  3415, 10273,  5243,    14,  6849,  2209, 21983,    25,  5686,\n",
            "           504,  7142,   318,  4497,   329,   517,   621,   352,    11,   830,\n",
            "          1605, 18499,  1141,   262,  3908,  1175,   290,   468, 37515,   257,\n",
            "          7417,  1368,   994,   287,   674,  7027,  3139,    13, 50257,  5122,\n",
            "         10681,  2494,   318,   625,   767,  1411,   290,  3957,    13,   775,\n",
            "         37788,   761,   281,  3034,  7628,  5301,   220,   290,   356,   761,\n",
            "           340,  3393,    13,   366,  2202,  2365,    13,   220,  2242,    11,\n",
            "          2486,  1138,   351,  3415,  5531,   284,   651,   511,  5128,   319,\n",
            "           262,  5150, 19819,  5301,    26,   290,   339,  4987,   284,  1826,\n",
            "           351,   262,  3415, 40100,   428,  1285,    11,   996,   867,  4734,\n",
            "         39247,   276,   379,   262, 13052,   340,   481,  1255,   287,  1997,\n",
            "          1969,   284,   257, 20953,  1410,    13,  1881,   460,  7267,  1771,\n",
            "           262, 25615,   286,   262,  3034,  4902, 31405,    82,   262,   761,\n",
            "           329, 18988,  9110,   832, 11702, 18921,    11,   475, 32342,   318,\n",
            "           826,    25,  1318,   423,   587,   645, 18921,   319,   262,  4858,\n",
            "         19819,  5301,   326,  3162,   318,  2938,   284,  3015,   319,   355,\n",
            "          1903,   355,   428,  1285,    13, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  5122,\n",
            "         10681,  2494,   318,   625,   767,  1411,   290,  3957,    13,   775,\n",
            "         37788,   761,   281,  3034,  7628,  5301,   220,   290,   356,   761,\n",
            "           340,  3393,    13,   366,  2202,  2365,    13,   220,  2242,    11,\n",
            "          2486,  1138,   351,  3415,  5531,   284,   651,   511,  5128,   319,\n",
            "           262,  5150, 19819,  5301,    26,   290,   339,  4987,   284,  1826,\n",
            "           351,   262,  3415, 40100,   428,  1285,    11,   996,   867,  4734,\n",
            "         39247,   276,   379,   262, 13052,   340,   481,  1255,   287,  1997,\n",
            "          1969,   284,   257, 20953,  1410,    13,  1881,   460,  7267,  1771,\n",
            "           262, 25615,   286,   262,  3034,  4902, 31405,    82,   262,   761,\n",
            "           329, 18988,  9110,   832, 11702, 18921,    11,   475, 32342,   318,\n",
            "           826,    25,  1318,   423,   587,   645, 18921,   319,   262,  4858,\n",
            "         19819,  5301,   326,  3162,   318,  2938,   284,  3015,   319,   355,\n",
            "          1903,   355,   428,  1285,    13, 50256]])\n",
            "concatinated_input : Metadata: campaign-finance ron-johnson None Wisconsin republican in a campaign ad Statement: Says Russ Feingold broke his 1992 promise to always get the majority of funding from Wisconsin residents.[EXP]\n",
            "full_input_length : 155\n",
            "labels : tensor([[ 9171, 14706,    25,  3707,   374,   624,    12,  1416,  1252, 10807,\n",
            "          4744, 41477,   262,  1812,   286,   262,  1812,  4046, 21983,    25,\n",
            "          4744,  1029,  4266,   389,  1440,   503,   286,   262,  1353,   838,\n",
            "           287,   262,  2104,  1578,  1829,    13, 50257, 41811,   531,    25,\n",
            "           366,   818,   262,   614,  2211,    11,   262,  2486,  3662,  2716,\n",
            "         14436,    11,   830,  4301,  5293, 16269,    13,  1119,  2716, 28817,\n",
            "         38313,   220,   661,   351, 19625, 19131,    11,   508,   389,   994,\n",
            "         15572,    13,   366,  1135,   766,   703,  8742,  4251,   465,  5538,\n",
            "            13,   887, 23358,  1139, 27191,  3925,   351,  5123, 19131,   547,\n",
            "          2716,   290,  7724,  1411,   286,   883, 10050,   547, 13677,   448,\n",
            "           286,   663,  1630,    13,  4418,    11,  8257,    11,   830,   286,\n",
            "           262,   661,  8742,  3417,   355,  2716,   547,  1239,  1682, 14847,\n",
            "           416,   262,  4086,    13,   554,  3090,    11,   428,  2643,   318,\n",
            "          4814,  9204,  4732,    13,  2329,   355,   356,   531,   287, 17217,\n",
            "          4176,    82, 14305,    11,   663, 35010,   284,  1950,   262,  3662,\n",
            "           468,  1336,  2551,    12,  8601,  4934,    13,  3078,  5370,   290,\n",
            "          2717,  3657,   711,  1593,  9176,    13, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100, 41811,   531,    25,\n",
            "           366,   818,   262,   614,  2211,    11,   262,  2486,  3662,  2716,\n",
            "         14436,    11,   830,  4301,  5293, 16269,    13,  1119,  2716, 28817,\n",
            "         38313,   220,   661,   351, 19625, 19131,    11,   508,   389,   994,\n",
            "         15572,    13,   366,  1135,   766,   703,  8742,  4251,   465,  5538,\n",
            "            13,   887, 23358,  1139, 27191,  3925,   351,  5123, 19131,   547,\n",
            "          2716,   290,  7724,  1411,   286,   883, 10050,   547, 13677,   448,\n",
            "           286,   663,  1630,    13,  4418,    11,  8257,    11,   830,   286,\n",
            "           262,   661,  8742,  3417,   355,  2716,   547,  1239,  1682, 14847,\n",
            "           416,   262,  4086,    13,   554,  3090,    11,   428,  2643,   318,\n",
            "          4814,  9204,  4732,    13,  2329,   355,   356,   531,   287, 17217,\n",
            "          4176,    82, 14305,    11,   663, 35010,   284,  1950,   262,  3662,\n",
            "           468,  1336,  2551,    12,  8601,  4934,    13,  3078,  5370,   290,\n",
            "          2717,  3657,   711,  1593,  9176,    13, 50256]])\n",
            "concatinated_input : Metadata: taxes michelle-obama None Illinois democrat Denver Statement: While in the Illinois Senate, Barack Obama passed \"tax cuts for hard-working families.\"[EXP]\n",
            "full_input_length : 137\n",
            "labels : tensor([[ 9171, 14706,    25,  5010,    11, 13948,    12,  6651,    11,    76,\n",
            "         42834,  3915,    88,    12,    73,  4185, 49292,  3418, 16570,  4744,\n",
            "         41477,   257,   366, 16012,    12, 31837, 20538,     1, 13684, 21983,\n",
            "            25,  4432, 24496,  5150, 11326,   329,  3315,  5727,   561,  1249,\n",
            "           661,   508,  4260,  4159, 42549,   884,   355,  8280,   599, 34432,\n",
            "            11,  7393,  2356,    11,   736,  2356,   290,   772, 37230,  1067,\n",
            "          9430,   357,  1462, 12780,     8,   329,  1230,    12, 12807,   596,\n",
            "           276,  1787,    12, 48783,    13, 50257,  1870,   356,  1053,  1541,\n",
            "          1775,   326,   357, 30464,   391,   318,     8,   407,  1016,   284,\n",
            "          2245,   262,   895,  4127,   290,  3434,   422,   465,  7681,  2491,\n",
            "           523,    12,  7174,   642,  1983,  2628,    11,   508,   481,  4341,\n",
            "          5242,   290,  5242,   286,  5054,   287, 15822, 10976,    13,   366,\n",
            "         18932,  1771,   484, 19189,   281,  4381,   351, 14264,    11,   262,\n",
            "          2486,  1923,  6235,   514,   284,   220,   220,   220,   220,   257,\n",
            "          1705,  1848,   220,   220,   220,   326,   531,  5811, 41971,    11,\n",
            "           281,  6136,   329,  2486,    11,  1138,   351, 14264,  9326,   284,\n",
            "          2112,   703,  1111,  9964,   714,  8076,   287,   262,  1171, 15435,\n",
            "          1080,    13,   887,   326,  1848,   373, 17548,    88,   290,  1422,\n",
            "           470,  2128,   284,   514,   588,   262,   366, 49639, 14748,     1,\n",
            "           326,  2486,   550,  8072,    13,   383,  1109,   318,   326,  2486,\n",
            "           531,   339,   561, 10660,  1171, 15435,    11,   475,  3066,   340,\n",
            "          2492,   470,   287,   262,  1923,   338, 16106,  1393,    13, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  1870,   356,  1053,  1541,\n",
            "          1775,   326,   357, 30464,   391,   318,     8,   407,  1016,   284,\n",
            "          2245,   262,   895,  4127,   290,  3434,   422,   465,  7681,  2491,\n",
            "           523,    12,  7174,   642,  1983,  2628,    11,   508,   481,  4341,\n",
            "          5242,   290,  5242,   286,  5054,   287, 15822, 10976,    13,   366,\n",
            "         18932,  1771,   484, 19189,   281,  4381,   351, 14264,    11,   262,\n",
            "          2486,  1923,  6235,   514,   284,   220,   220,   220,   220,   257,\n",
            "          1705,  1848,   220,   220,   220,   326,   531,  5811, 41971,    11,\n",
            "           281,  6136,   329,  2486,    11,  1138,   351, 14264,  9326,   284,\n",
            "          2112,   703,  1111,  9964,   714,  8076,   287,   262,  1171, 15435,\n",
            "          1080,    13,   887,   326,  1848,   373, 17548,    88,   290,  1422,\n",
            "           470,  2128,   284,   514,   588,   262,   366, 49639, 14748,     1,\n",
            "           326,  2486,   550,  8072,    13,   383,  1109,   318,   326,  2486,\n",
            "           531,   339,   561, 10660,  1171, 15435,    11,   475,  3066,   340,\n",
            "          2492,   470,   287,   262,  1923,   338, 16106,  1393,    13, 50256]])\n",
            "concatinated_input : Metadata: taxes jamie-weinstein Senior editor, Daily Caller None none comments on HBO's \"Real Time with Bill Maher\" Statement: The states that are doing better are the ones that have no state income tax.[EXP]\n",
            "full_input_length : 88\n",
            "labels : tensor([[ 9171, 14706,    25, 14510,    11, 20088,  1362,    11,    86, 27122,\n",
            "          2318, 39389,    12,  7197,  3162,  8580,   422,  3442,   338,  1511,\n",
            "           400,  4783,  3442, 43268,   281,  2720,   351, 22128, 21983,    25,\n",
            "          8772,  1335,  6630,   287,   262,  2097,  5318,  2855,  1612,   362,\n",
            "          1510,  1342,   661,   319,  2057, 25560,    11, 20064,    11,   830,\n",
            "          1751,   481,   407,  3328,  1524, 14678,  2052,   393,  2270,    69,\n",
            "          5773,    13, 50257,  1320,   373,  3940,   416,   257,  2068,   366,\n",
            "          2704,   541,     1,   736,   284,  1104,   286,  2035, 14218,   393,\n",
            "          7748,  2742,  3722,   287,   262,  4894,   286,  5581,  9299,    13,\n",
            "          3827,   262,   812,  5511,   468,   531,   339, 19344, 14218,   393,\n",
            "          2742, 27308,    11, 21135, 30913,   284, 11628,   326,   714,   307,\n",
            "          3177,  1626,   257, 10595,  4975,  3626,    13,  1406,   379,  1661,\n",
            "           339,   468, 18079,  1111,    11,   393,  2035,   530,    13,   383,\n",
            "           411,   645,  4719,    11,   996,    11,   326,   262, 24298,  5511,\n",
            "           287,   262,  1492,   550,   257,  1180,  4459,   422,   262, 24298,\n",
            "          5511,   319,   262,  1492,  4205,    13, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  1320,   373,  3940,   416,   257,  2068,   366,\n",
            "          2704,   541,     1,   736,   284,  1104,   286,  2035, 14218,   393,\n",
            "          7748,  2742,  3722,   287,   262,  4894,   286,  5581,  9299,    13,\n",
            "          3827,   262,   812,  5511,   468,   531,   339, 19344, 14218,   393,\n",
            "          2742, 27308,    11, 21135, 30913,   284, 11628,   326,   714,   307,\n",
            "          3177,  1626,   257, 10595,  4975,  3626,    13,  1406,   379,  1661,\n",
            "           339,   468, 18079,  1111,    11,   393,  2035,   530,    13,   383,\n",
            "           411,   645,  4719,    11,   996,    11,   326,   262, 24298,  5511,\n",
            "           287,   262,  1492,   550,   257,  1180,  4459,   422,   262, 24298,\n",
            "          5511,   319,   262,  1492,  4205,    13, 50256]])\n",
            "concatinated_input : Metadata: elections,states bob-bennett Ohio Republican Party chairman Ohio republican a news release  Statement: Ohio Republicans made significant gains during [Chris McNultys] time at the ORP, including the extremely successful re-election of President Bush in Ohio.[EXP]\n",
            "full_input_length : 146\n",
            "labels : tensor([[ 9171, 14706,    25,  7024, 12788,   560,    12, 37821, 17471,  4540,\n",
            "           968,  1971, 43268,   257,  4495,   869,   284,  7638,  2957,   416,\n",
            "           327,   346, 28936,  3278,  1706,  2940,  6595, 21983,    25,  8962,\n",
            "          2486,   468,    11,   287,  1109,    11,  1239,   550,   257,  2726,\n",
            "          3415, 32127,    13, 50257,   464,   411,   645,  4719,   326, 16210,\n",
            "           329,  1012,  2954,   364,   373,   636,   286,   262, 19819,   290,\n",
            "           326,   262,  1430, 29657,   262,  4220,  3951,   286, 24666,   469,\n",
            "           297,    82, 15737,  5748,    13,   887,   356, 18548,  1295,   257,\n",
            "          8872,  2033,   319,   262,  4414,   284, 24666,   695,    13,   383,\n",
            "           512, 31238,  5644,   326,   465, 15737,  5748,   720, 39710,    11,\n",
            "           830,   287,  3405,   689,   547,  3892, 10177,   290,   262,   411,\n",
            "          1738,   284,  1975, 24666, 19187,  4036,  4461,   422,  1012,  2954,\n",
            "           364,   373, 15394,  4833,    13, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,   464,   411,   645,  4719,   326, 16210,\n",
            "           329,  1012,  2954,   364,   373,   636,   286,   262, 19819,   290,\n",
            "           326,   262,  1430, 29657,   262,  4220,  3951,   286, 24666,   469,\n",
            "           297,    82, 15737,  5748,    13,   887,   356, 18548,  1295,   257,\n",
            "          8872,  2033,   319,   262,  4414,   284, 24666,   695,    13,   383,\n",
            "           512, 31238,  5644,   326,   465, 15737,  5748,   720, 39710,    11,\n",
            "           830,   287,  3405,   689,   547,  3892, 10177,   290,   262,   411,\n",
            "          1738,   284,  1975, 24666, 19187,  4036,  4461,   422,  1012,  2954,\n",
            "           364,   373, 15394,  4833,    13, 50256]])\n",
            "concatinated_input : Metadata: animals,education,health-care,public-health,science people-ethical-treatment-animals None None none a robo-call and video Statement: Live cats have holes drilled into their skulls, posts put into their heads and coils put into their eyes, and some have had their ears cut off or are intentionally deafened or starved at UW-Madison labs that do research to improve hearing in humans.[EXP]\n",
            "full_input_length : 183\n",
            "labels : tensor([[ 9171, 14706,    25,  1923,    12,    69, 14149,   374,   261,    12,\n",
            "         30686,  1559,  6045,  9279, 41477,   287,   257,  1923,   512, 21983,\n",
            "            25, 28628,  1887,  5452,   278,   727,  6265,   465,  9768,  6991,\n",
            "           284,  1464,   651,   262,  3741,   286,  4918,   422,  9279,  5085,\n",
            "            13, 50257, 41811,   531,   326,   366, 12518,   357, 23672,  1940,\n",
            "             8, 12823,  1625,   287,    11,   422, 15524,   284, 14489,    11,\n",
            "          3034,  3349, 16449,  1342,   621,   352,  1411,   257,   614,    13,\n",
            "          8742,  6825, 11545,   584,  1440,    12,  1941,  9574,   326,  4197,\n",
            "           262,  9987,    13,   554,  3090,    11,   465, 26863,  1377,   326,\n",
            "          4956,   389,  5688,   284,  8138,   329,   883,  3403,  1377, 46701,\n",
            "          1302,   510,   284, 12219, 15794,    11,  3573,   618,   339, 15009,\n",
            "           262,  2278,  3726,   287,  4793,   357,  4758,  3017,  1115,   812,\n",
            "          9944,   739,  5511,     8,   290,  4343,   357,  4758,  3017,   734,\n",
            "           812,  9944,   739,  5511,   737,   770,  2925,   319,  1353,   286,\n",
            "          2276, 13479,   546,   703,   881,  8138,   284,  8333, 19033,   329,\n",
            "          3595,  3034,  3403,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100, 41811,   531,   326,   366, 12518,   357, 23672,  1940,\n",
            "             8, 12823,  1625,   287,    11,   422, 15524,   284, 14489,    11,\n",
            "          3034,  3349, 16449,  1342,   621,   352,  1411,   257,   614,    13,\n",
            "          8742,  6825, 11545,   584,  1440,    12,  1941,  9574,   326,  4197,\n",
            "           262,  9987,    13,   554,  3090,    11,   465, 26863,  1377,   326,\n",
            "          4956,   389,  5688,   284,  8138,   329,   883,  3403,  1377, 46701,\n",
            "          1302,   510,   284, 12219, 15794,    11,  3573,   618,   339, 15009,\n",
            "           262,  2278,  3726,   287,  4793,   357,  4758,  3017,  1115,   812,\n",
            "          9944,   739,  5511,     8,   290,  4343,   357,  4758,  3017,   734,\n",
            "           812,  9944,   739,  5511,   737,   770,  2925,   319,  1353,   286,\n",
            "          2276, 13479,   546,   703,   881,  8138,   284,  8333, 19033,   329,\n",
            "          3595,  3034,  3403,    13, 50256]])\n",
            "concatinated_input : Metadata: population,water kirk-watson lawyer Texas democrat an answer for the League of Women Voters, Austiin Area, Voters Guide. Statement: Texas population is projected to double in the next 50 years or so, but our basic amount of water will remain about where it is now.[EXP]\n",
            "full_input_length : 181\n",
            "labels : tensor([[ 9171, 14706,    25,  5704, 12314, 34454,    12,   672,  1689,  6045,\n",
            "          9486, 43268, 10656, 21983,    25,  2893,   287,   262,  9486,  3845,\n",
            "            11,  8732,  2486,  3804,   366, 19290,  6630,   329,  1327,    12,\n",
            "         16090,  4172,   526, 50257, 10294,    11,  2048,   477,  1402,  5692,\n",
            "          1377,   883,   351,  7380,   621,  2026,  4409,  1377,   389, 13068,\n",
            "           422, 12970,    11,  1771,   484,  2897,  5096,   393,   407,    13,\n",
            "          2773,  1402,  5692,   389,  4025,   621,  2026,  4409,   290,   714,\n",
            "          1986, 17176,   611,   484,   836,   470,  2897,  4409,  5096,    13,\n",
            "           887,   257,  5909,  3741,   286,   471,    13,   311,    13,   220,\n",
            "          9611,   389,  4833,   621,  2026,  4409,   290,   389, 13068,   422,\n",
            "           262,  1535,  5096,  5359,    13,   383, 11847,   338,   512,   318,\n",
            "         18404,    11,   290,  1595,   470,  1848,   329,   597,   286,   262,\n",
            "          3967,  8617,   326,   836,   470,   366,  6098,  1530,     1,  1402,\n",
            "          1597,   475,  1682,  1037,   606,    13, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100, 10294,    11,  2048,   477,  1402,  5692,\n",
            "          1377,   883,   351,  7380,   621,  2026,  4409,  1377,   389, 13068,\n",
            "           422, 12970,    11,  1771,   484,  2897,  5096,   393,   407,    13,\n",
            "          2773,  1402,  5692,   389,  4025,   621,  2026,  4409,   290,   714,\n",
            "          1986, 17176,   611,   484,   836,   470,  2897,  4409,  5096,    13,\n",
            "           887,   257,  5909,  3741,   286,   471,    13,   311,    13,   220,\n",
            "          9611,   389,  4833,   621,  2026,  4409,   290,   389, 13068,   422,\n",
            "           262,  1535,  5096,  5359,    13,   383, 11847,   338,   512,   318,\n",
            "         18404,    11,   290,  1595,   470,  1848,   329,   597,   286,   262,\n",
            "          3967,  8617,   326,   836,   470,   366,  6098,  1530,     1,  1402,\n",
            "          1597,   475,  1682,  1037,   606,    13, 50256]])\n",
            "concatinated_input : Metadata: abortion ohio-right-life None Ohio none a news release Statement: Ohios Planned Parenthood operations received millions of taxpayer dollars via federal grants in 2010 and 2011.[EXP]\n",
            "full_input_length : 58\n",
            "labels : tensor([[ 9171, 14706,    25,  5704, 16853,   494,    12,   732, 11962, 14017,\n",
            "          5464,    11,  6714, 10244,  6045,  4844,  3651,   319, 18804,   338,\n",
            "           366, 15633,  3862,   351,  3941, 38137,     1, 21983,    25,   383,\n",
            "          2585,   326,   389,  1804,  1365,   389,   262,  3392,   326,   423,\n",
            "           645,  1181,  3739,  1687,    13, 50257,  1537,   314,  1101,  5597,\n",
            "          1909,  1011,   319,   477,   401,   364,   220,   477,   401,   364,\n",
            "           220,   287,   257,  2276,  3071,    13,   843,  4361,    11,   314,\n",
            "           423,  3066,   284,   307,   257,  4540,   329,   302,    12, 14300,\n",
            "           287,  3050,   287,   262,  4390,  4165,    13, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  1537,   314,  1101,  5597,\n",
            "          1909,  1011,   319,   477,   401,   364,   220,   477,   401,   364,\n",
            "           220,   287,   257,  2276,  3071,    13,   843,  4361,    11,   314,\n",
            "           423,  3066,   284,   307,   257,  4540,   329,   302,    12, 14300,\n",
            "           287,  3050,   287,   262,  4390,  4165,    13, 50256]])\n",
            "concatinated_input : Metadata: children,families,homeland-security,immigration reince-priebus Chairman, Republican National Committee Wisconsin republican an interview Statement: Says President Barack Obama promised a pathway to citizenship to undocumented immigrants and didnt deliver jack squat on any of it.[EXP]\n",
            "full_input_length : 136\n",
            "labels : tensor([[ 9171, 14706,    25,  7024,    11, 27219, 29202,    12,    65, 48151,\n",
            "          6835,  3415,  3615,  8900,  6835, 41477,   257,  1705,  2650,   220,\n",
            "         21983,    25,  6835,  4734,   925,  2383,  8810,  1141,   685, 15645,\n",
            "         22586,   586,   893,    60,   640,   379,   262,  6375,    47,    11,\n",
            "          1390,   262,  4457,  4388,   302,    12, 14300,   286,  1992,  5511,\n",
            "           287,  6835,    13, 50257, 41811,   531,    11,   366,   464, 10193,\n",
            "           468,  3088,   284,  8160,   257,   279, 24500,   393,   257, 37664,\n",
            "         30115,   319,   534,  5318,   284,   307, 20436,   540, 10150,   290,\n",
            "          4145,  2426,   284,  4858,  6142,  6647,    13,   366,   464, 14724,\n",
            "          1722,  1660,  3896,  5734, 36833,   279,  4185,   829,    13,   632,\n",
            "           691,  8991,   284,   288,  9249,   326,   389, 12006,   503,   286,\n",
            "         15190,   393,   288,  9249,   326,  2163,   588, 15190,   290,   714,\n",
            "          3283, 12231,   284, 33218, 10150,    13,  8742,   267, 24883,   326,\n",
            "           262,  1660,  3896,   373,  1234,   319,  1745,   416,   262,  8028,\n",
            "           287,  1853, 13310, 19284,    13, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100, 41811,   531,    11,   366,   464, 10193,\n",
            "           468,  3088,   284,  8160,   257,   279, 24500,   393,   257, 37664,\n",
            "         30115,   319,   534,  5318,   284,   307, 20436,   540, 10150,   290,\n",
            "          4145,  2426,   284,  4858,  6142,  6647,    13,   366,   464, 14724,\n",
            "          1722,  1660,  3896,  5734, 36833,   279,  4185,   829,    13,   632,\n",
            "           691,  8991,   284,   288,  9249,   326,   389, 12006,   503,   286,\n",
            "         15190,   393,   288,  9249,   326,  2163,   588, 15190,   290,   714,\n",
            "          3283, 12231,   284, 33218, 10150,    13,  8742,   267, 24883,   326,\n",
            "           262,  1660,  3896,   373,  1234,   319,  1745,   416,   262,  8028,\n",
            "           287,  1853, 13310, 19284,    13, 50256]])\n",
            "concatinated_input : Metadata: economy,state-finances arnold-schwarzenegger actor California republican an endorsement speech in Columbus, Ohio. Statement: When John Kasich became governor of Ohio, there was an $8 billion budget deficit and now theres a $2 billion surplus.[EXP]\n",
            "full_input_length : 212\n",
            "labels : tensor([[ 9171, 14706,    25,  4695,    11, 40796,    11, 13948,    12,  6651,\n",
            "            11, 11377,    12, 13948,    11, 16801,   661,    12, 32949,    12,\n",
            "         42487,    12, 11227,   874,  6045,  6045,  4844,   257,   686,  2127,\n",
            "            12, 13345,   290,  2008, 21983,    25,  7547, 11875,   423, 10421,\n",
            "         34936,   656,   511, 38556,    11,  6851,  1234,   656,   511,  6665,\n",
            "           290, 41331,  1234,   656,   511,  2951,    11,   290,   617,   423,\n",
            "           550,   511, 11368,  2005,   572,   393,   389, 16464, 21815,  2945,\n",
            "           393, 47224,   379, 33436,    12, 46845, 27887,   326,   466,  2267,\n",
            "           284,  2987,  4854,   287,  5384,    13, 50257,  3886,   530,  3210,\n",
            "           220,  2472,  5054,   220,   262,  1448,   318,   826,   326,  9068,\n",
            "           614,  3717,   481,   423,   262,  4387, 11807,   287,  2106,    13,\n",
            "           887,   340,  2058,   510,  1218,   611,   345,  3953,   340,   416,\n",
            "          2648,   286, 12396,    11,   355,   867, 16816,  4702,    13,  1081,\n",
            "           329,   262,  1218,   636,    11,   340,   338,   407,  3376,   284,\n",
            "          8138,   477,   262,  4581,   319,  4956,    13,  3363,    11,   262,\n",
            "          2486,  3662,   290,   262,  4390,    12,   992,  3162,   389,  4497,\n",
            "           329,   257,   922, 16058,   286,   326,  4581,   220,   475,   867,\n",
            "          4847,   547,  9763,   416,  6041,   286,  4734,    11,  1390,  1992,\n",
            "          5511,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  3886,   530,  3210,\n",
            "           220,  2472,  5054,   220,   262,  1448,   318,   826,   326,  9068,\n",
            "           614,  3717,   481,   423,   262,  4387, 11807,   287,  2106,    13,\n",
            "           887,   340,  2058,   510,  1218,   611,   345,  3953,   340,   416,\n",
            "          2648,   286, 12396,    11,   355,   867, 16816,  4702,    13,  1081,\n",
            "           329,   262,  1218,   636,    11,   340,   338,   407,  3376,   284,\n",
            "          8138,   477,   262,  4581,   319,  4956,    13,  3363,    11,   262,\n",
            "          2486,  3662,   290,   262,  4390,    12,   992,  3162,   389,  4497,\n",
            "           329,   257,   922, 16058,   286,   326,  4581,   220,   475,   867,\n",
            "          4847,   547,  9763,   416,  6041,   286,  4734,    11,  1390,  1992,\n",
            "          5511,    13, 50256]])\n",
            "concatinated_input : Metadata: elections barack-obama President Illinois democrat an interview on the Today Show. Statement: We've won twice as many states. We've won a greater share of the popular vote.[EXP]\n",
            "full_input_length : 115\n",
            "labels : tensor([[ 9171, 14706,    25,  3265,    11,  7050,   479, 14232,    12,    86,\n",
            "         13506,  6853,  3936, 43268,   281,  3280,   329,   262,  4041,   286,\n",
            "          6926, 35689,    11,  2517,    72,   259,  9498,    11, 35689, 10005,\n",
            "            13, 21983,    25,  3936,  3265,   318, 13301,   284,  4274,   287,\n",
            "           262,  1306,  2026,   812,   393,   523,    11,   475,   674,  4096,\n",
            "          2033,   286,  1660,   481,  3520,   546,   810,   340,   318,   783,\n",
            "            13, 50257,    47,   594,   531,  2605,  4054,   284, 16674,   257,\n",
            "          3722,   286,  3386,  4381,   351,  3908,    13,   632,   318,  7187,\n",
            "           284,   910,   326,  9825,  4054,    13,   632,  7584,  1165,   881,\n",
            "         12476,   319, 30234,  2597,    11,   618,   262,  1994,  5370,   290,\n",
            "          9984,  1718,  1295,  1022,   262,  2635,  2097,  2346,   290, 10420,\n",
            "          2766,    13,   383,  2486,  3662,  2227,   284,  1035,  5039,  1605,\n",
            "          6553,   422,  2742,  2223,   287,  3908,    13,  1320,   373,   257,\n",
            "          1964,  6164,  3245,   287,  3908,    13,  2893,   612,   318,  2119,\n",
            "           329,  4384,   319,  1771,  1605, 45373,   714,   423,  1043,   257,\n",
            "           835,  1088,   326,  1917,    11,   612,   318,   645,  1738,   284,\n",
            "          1975,   262,  3662,  2391,  6807,  1497,   422,   257,  1730,   284,\n",
            "          1394,   471,    13,   311,    13,   220,  6553,   287,  3908,    13,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,    47,   594,   531,  2605,  4054,   284, 16674,   257,\n",
            "          3722,   286,  3386,  4381,   351,  3908,    13,   632,   318,  7187,\n",
            "           284,   910,   326,  9825,  4054,    13,   632,  7584,  1165,   881,\n",
            "         12476,   319, 30234,  2597,    11,   618,   262,  1994,  5370,   290,\n",
            "          9984,  1718,  1295,  1022,   262,  2635,  2097,  2346,   290, 10420,\n",
            "          2766,    13,   383,  2486,  3662,  2227,   284,  1035,  5039,  1605,\n",
            "          6553,   422,  2742,  2223,   287,  3908,    13,  1320,   373,   257,\n",
            "          1964,  6164,  3245,   287,  3908,    13,  2893,   612,   318,  2119,\n",
            "           329,  4384,   319,  1771,  1605, 45373,   714,   423,  1043,   257,\n",
            "           835,  1088,   326,  1917,    11,   612,   318,   645,  1738,   284,\n",
            "          1975,   262,  3662,  2391,  6807,  1497,   422,   257,  1730,   284,\n",
            "          1394,   471,    13,   311,    13,   220,  6553,   287,  3908,    13,\n",
            "         50256]])\n",
            "concatinated_input : Metadata: transportation sam-adams Mayor of Portland Oregon democrat a video online Statement: Says for the equivalent cost of a single mile of freeway, we have a bike infrastructure.[EXP]\n",
            "full_input_length : 67\n",
            "labels : tensor([[ 9171, 14706,    25,  7950, 11752,   952,    12,  3506,    12,  6042,\n",
            "          6045,  6835,  4844,   257,  1705,  2650, 21983,    25,  3966,  4267,\n",
            "         18451, 18461,  4560,  2722,  5242,   286, 14776,  5054,  2884,  2717,\n",
            "         11455,   287,  3050,   290,  2813,    13, 50257,   464,  1637,   326,\n",
            "           561,   307, 45158,   416,   311,    13,   347,    13,   580,   318,\n",
            "           973,   284,  2148,   584,  1535,  2594,    13, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,   464,  1637,   326,\n",
            "           561,   307, 45158,   416,   311,    13,   347,    13,   580,   318,\n",
            "           973,   284,  2148,   584,  1535,  2594,    13, 50256]])\n",
            "concatinated_input : Metadata: economy,pundits,stimulus rush-limbaugh Radio host None none Fox News Sunday Statement: That 3.5 percent (increase in the third quarter GDP) came from two things government spending on Cash for Clunkers they just moved fourth-quarter auto sales into the third quarter and the first-time home buyer thing.[EXP]\n",
            "full_input_length : 168\n",
            "labels : tensor([[ 9171, 14706,    25,  1751,    11, 44769,  3922,    11, 26452,  8822,\n",
            "            12, 12961,    11, 47620,   302,   924,    12,  3448, 33209, 12787,\n",
            "            11,  3415,  2351,  4606,  9279, 41477,   281,  2720, 21983,    25,\n",
            "         28628,  1992,  8732,  2486,  8072,   257, 21182,   284, 14218,   284,\n",
            "         21829,  7971,   290, 42547,  5203, 14509, 22713,   319,   597,   286,\n",
            "           340,    13, 50257,  1870,  9033,   431, 49108,  9181,  2486,   338,\n",
            "          4640,  1502,    13,  3954,  7955,   220,   220, 37486,   531,  2486,\n",
            "          8072,   257,   366,  6978,  1014,   284, 14218,     1,   284,  7971,\n",
            "           366,   392,   339,  1422,   470,  5203, 14509, 22713,   319,   597,\n",
            "           286,   340,    13,   366, 15948,   468,   407,  6793,   262, 21182,\n",
            "           284, 14218,    11,  3584, 37486,   857,   407,   787,   340,  1598,\n",
            "           326,  1835, 17485,  3626,  1377,   262,   360, 32235,  2191,  1377,\n",
            "           373, 10226,   416,  4734,    13,  1320,  1838, 37486,  2643,  7187,\n",
            "           475, 18139,  3224,  1321,    13, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  1870,  9033,   431, 49108,  9181,  2486,   338,\n",
            "          4640,  1502,    13,  3954,  7955,   220,   220, 37486,   531,  2486,\n",
            "          8072,   257,   366,  6978,  1014,   284, 14218,     1,   284,  7971,\n",
            "           366,   392,   339,  1422,   470,  5203, 14509, 22713,   319,   597,\n",
            "           286,   340,    13,   366, 15948,   468,   407,  6793,   262, 21182,\n",
            "           284, 14218,    11,  3584, 37486,   857,   407,   787,   340,  1598,\n",
            "           326,  1835, 17485,  3626,  1377,   262,   360, 32235,  2191,  1377,\n",
            "           373, 10226,   416,  4734,    13,  1320,  1838, 37486,  2643,  7187,\n",
            "           475, 18139,  3224,  1321,    13, 50256]])\n",
            "concatinated_input : Metadata: bipartisanship,voting-record scott-brown None New Hampshire republican a meeting with Republicans in Portsmouth, N.H. Statement: I was the most bipartisan senator in the United States Senate.[EXP]\n",
            "full_input_length : 87\n",
            "labels : tensor([[ 9171, 14706,    25,  3773,    11,  5219,    12, 15643,  1817,   610,\n",
            "            77,   727,    12, 20601,  5767,    89, 44028,  8674,  3442, 41477,\n",
            "           281, 17819,  4046,   287, 14939,    11,  6835,    13, 21983,    25,\n",
            "          1649,  1757, 24931,  2627,  8153,   286,  6835,    11,   612,   373,\n",
            "           281,   720,    23,  2997,  4466, 11807,   290,   783,   262,   411,\n",
            "           257,   720,    17,  2997, 18201,    13, 50257,  5122,  7955,  3954,\n",
            "         20265, 12131,  1139, 10696,   365,  6520,   284,  1249,  1903,  6709,\n",
            "           319,   262, 33436,    12, 13719,  4696,  7611,   780,   673,   366,\n",
            "          9776,  7787,   340,   561,  1037,  4956,   553,  4478,   366,   448,\n",
            "          8394,   516, 19666, 10690,    13,   366, 36504,   365,   750,  4911,\n",
            "          2328,   546,  3501,   281,  4621,   284,  4956,   780,   286,   607,\n",
            "          4901,   326,   262,  2524,   561,   307,   287,  1989,   351,   517,\n",
            "          4446,   508,  1104,  4956,    11,   290,   780,   262,  2524,   373,\n",
            "          9167,   416,   257,  4390,  1181, 31340,    13,   887, 10696,   365,\n",
            "           635,  9181,   584,  3840,   329,   407,  6493,   262,  7611,  2524,\n",
            "           329,  1903,  6709,  1377,  1390,   257,  3092,   286,  4918,    11,\n",
            "           257,  2328,   329, 11100,  2324,   290,   257,  2328,   329,   257,\n",
            "          1181,  1099,   326, 24059,  6011,   257, 11210,  2524,   326,  3607,\n",
            "           281,  4621,   284,   597,  1964,  2151,    13, 24199,    11,   262,\n",
            "          1748,  3066,   407,   284,  1280,   597, 11210,  5043,   329,  1903,\n",
            "          6709,   329,   262,  5267,    13,   220,   807,    11,  1584,  3071,\n",
            "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  5122,  7955,  3954,\n",
            "         20265, 12131,  1139, 10696,   365,  6520,   284,  1249,  1903,  6709,\n",
            "           319,   262, 33436,    12, 13719,  4696,  7611,   780,   673,   366,\n",
            "          9776,  7787,   340,   561,  1037,  4956,   553,  4478,   366,   448,\n",
            "          8394,   516, 19666, 10690,    13,   366, 36504,   365,   750,  4911,\n",
            "          2328,   546,  3501,   281,  4621,   284,  4956,   780,   286,   607,\n",
            "          4901,   326,   262,  2524,   561,   307,   287,  1989,   351,   517,\n",
            "          4446,   508,  1104,  4956,    11,   290,   780,   262,  2524,   373,\n",
            "          9167,   416,   257,  4390,  1181, 31340,    13,   887, 10696,   365,\n",
            "           635,  9181,   584,  3840,   329,   407,  6493,   262,  7611,  2524,\n",
            "           329,  1903,  6709,  1377,  1390,   257,  3092,   286,  4918,    11,\n",
            "           257,  2328,   329, 11100,  2324,   290,   257,  2328,   329,   257,\n",
            "          1181,  1099,   326, 24059,  6011,   257, 11210,  2524,   326,  3607,\n",
            "           281,  4621,   284,   597,  1964,  2151,    13, 24199,    11,   262,\n",
            "          1748,  3066,   407,   284,  1280,   597, 11210,  5043,   329,  1903,\n",
            "          6709,   329,   262,  5267,    13,   220,   807,    11,  1584,  3071,\n",
            "            13, 50256]])\n",
            "concatinated_input : Metadata: elections,florida-amendments,infrastructure citizens-lower-taxes-and-stronger-economy None Florida none an Internet ad. Statement: St. Pete Beachs local version of Amendment 4 resulted in seemingly endless lawsuits (that) decimated the citys legal budget and forced the city to raise the property tax rate.[EXP]\n",
            "full_input_length : 206\n",
            "labels : tensor([[ 9171, 14706,    25,  7024,  2318,   441,    12,   672,  1689,  1992,\n",
            "          9486, 43268,   281,  2720,   319,   262,  6288,  5438,    13, 21983,\n",
            "            25,   775,  1053,  1839,  5403,   355,   867,  2585,    13,   775,\n",
            "          1053,  1839,   257,  3744,  2648,   286,   262,  2968,  3015,    13,\n",
            "         50257,  5990,   563,   373,  3376,   611,   345,   691,   804,   379,\n",
            "          1524,  1687,  2494,    11,   475,   465,  2912,  6412,   284,   477,\n",
            "          3119,  5704,    13,  1002,   345,   804,   379,  2472,  3119,  1687,\n",
            "          6426,    11, 21012,  3432,   546,   262,   976,  2033,   287,  3050,\n",
            "           355,   484,   750,   287,  5075,    13,  1002,   345,  4532,   329,\n",
            "         10610,    11,   339,   338,  5699,   357,   270,   338,   546,   860,\n",
            "          1411,  1342,     8,   475,   340,   338,   991,  1290,  1790,   286,\n",
            "           530,    12, 17089,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  5990,   563,   373,  3376,   611,   345,   691,   804,   379,\n",
            "          1524,  1687,  2494,    11,   475,   465,  2912,  6412,   284,   477,\n",
            "          3119,  5704,    13,  1002,   345,   804,   379,  2472,  3119,  1687,\n",
            "          6426,    11, 21012,  3432,   546,   262,   976,  2033,   287,  3050,\n",
            "           355,   484,   750,   287,  5075,    13,  1002,   345,  4532,   329,\n",
            "         10610,    11,   339,   338,  5699,   357,   270,   338,   546,   860,\n",
            "          1411,  1342,     8,   475,   340,   338,   991,  1290,  1790,   286,\n",
            "           530,    12, 17089,    13, 50256]])\n",
            "concatinated_input : Metadata: candidates-biography ted-nugent musician Texas republican an oped column. Statement: Rick Perry has never lost an election and remains the only person to have won the Texas governorship three times in landslide elections.[EXP]\n",
            "full_input_length : 133\n",
            "labels : tensor([[ 9171, 14706,    25,  9358,  6072,    12,   324,  4105, 10106,   286,\n",
            "         10727,  8819, 43268,   257,  2008,  2691, 21983,    25, 28628,   329,\n",
            "           262,  7548,  1575,   286,   257,  2060, 10591,   286, 39042,    11,\n",
            "           356,   423,   257,  7161,  6884,    13, 50257, 16977,    25,   770,\n",
            "          2378,  3544,  6153,  3146,   319,   262,  5057,  4179,  5690,   326,\n",
            "           547,  2810,   284,   514,   416,   262,  5108,   692,  4182,  1923,\n",
            "           706,   428,  2378,   373,  3199,    13, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100, 16977,    25,   770,\n",
            "          2378,  3544,  6153,  3146,   319,   262,  5057,  4179,  5690,   326,\n",
            "           547,  2810,   284,   514,   416,   262,  5108,   692,  4182,  1923,\n",
            "           706,   428,  2378,   373,  3199,    13, 50256]])\n",
            "concatinated_input : Metadata: iraq lindsey-graham U.S. senator South Carolina republican an interview on ABC News' <i> This Week </i> Statement: Joe Biden \"voted against the first Gulf War. He opposed the surge. He wanted to partition Iraq.\"[EXP]\n",
            "full_input_length : 164\n",
            "labels : tensor([[ 9171, 14706,    25,  3773,    11,    79,   917,   896,    11, 42003,\n",
            "         23515, 10484,    12,  2475, 23768,  8829,  2583,  6045,  4844,  5426,\n",
            "          3000,  3502, 21983,    25,  1320,   513,    13,    20,  1411,   357,\n",
            "         24988,   589,   287,   262,  2368,  3860, 12396,     8,  1625,   422,\n",
            "           734,  1243,  1230,  4581,   319, 16210,   329,  1012,  2954,   364,\n",
            "           484,   655,  3888,  5544,    12, 24385,  8295,  4200,   656,   262,\n",
            "          2368,  3860,   290,   262,   717,    12,  2435,  1363, 17872,  1517,\n",
            "            13, 50257, 15354,   262,  1230, 16887,  1637,   880,   393,  2476,\n",
            "           257, 32143,  5496,   318,   407,   262,  2071,   994,    13, 38317,\n",
            "           504,   966,   373,  1598,    11,   290,   356,   466,   407,   892,\n",
            "           339,   373,  2111,   284,   307, 13779,   393, 14169,    13,   679,\n",
            "           925,   465,  2643, 10371,   257,  2260,   290, 34419,  5273,   546,\n",
            "          5704,    11,  4581,   290,   262,  2546,   286,  1230,    13,   887,\n",
            "           465,  2643,  2476, 31321,    11, 11476,   780,   465,  3785,   373,\n",
            "           572,  1377,  3584,   326,   857,   407,  2689,   465,  4045,   966,\n",
            "          1377,   290,   635,   780,   262,  3721,   286,  8914,  1637,   460,\n",
            "           307, 10617, 10338,   329,   584,  4959,    13, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100, 15354,   262,  1230, 16887,  1637,   880,   393,  2476,\n",
            "           257, 32143,  5496,   318,   407,   262,  2071,   994,    13, 38317,\n",
            "           504,   966,   373,  1598,    11,   290,   356,   466,   407,   892,\n",
            "           339,   373,  2111,   284,   307, 13779,   393, 14169,    13,   679,\n",
            "           925,   465,  2643, 10371,   257,  2260,   290, 34419,  5273,   546,\n",
            "          5704,    11,  4581,   290,   262,  2546,   286,  1230,    13,   887,\n",
            "           465,  2643,  2476, 31321,    11, 11476,   780,   465,  3785,   373,\n",
            "           572,  1377,  3584,   326,   857,   407,  2689,   465,  4045,   966,\n",
            "          1377,   290,   635,   780,   262,  3721,   286,  8914,  1637,   460,\n",
            "           307, 10617, 10338,   329,   584,  4959,    13, 50256]])\n",
            "concatinated_input : Metadata: china,military randy-forbes Representative Virginia republican a radio interview. Statement: China is going to have twice the number of submarines we have in just over a decade.[EXP]\n",
            "full_input_length : 169\n",
            "labels : tensor([[ 9171, 14706,    25, 14141,   433, 26100,  1056,    11,    85, 10720,\n",
            "            12, 22105,   629,  1252,    12, 33282,  6045,   968, 13910, 41477,\n",
            "           257,  3249,   351,  4734,   287, 48823,    11,   399,    13,    39,\n",
            "            13, 21983,    25,   314,   373,   262,   749, 20953, 12329,   287,\n",
            "           262,  1578,  1829,  3845,    13, 50257, 26141,  2667,   416,  5690,\n",
            "          3436,    11,  4373,   373,   407,  1464,   262,   749, 20953, 12329,\n",
            "          1141,   465,   640,   287,  2607,    11,   475,   339,   373,  1400,\n",
            "            13,   352,   329,   257,   640,    11,   290,   373,  1969,   329,\n",
            "           262,  1334,   286,   262,   640,    13, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100, 26141,  2667,   416,  5690,\n",
            "          3436,    11,  4373,   373,   407,  1464,   262,   749, 20953, 12329,\n",
            "          1141,   465,   640,   287,  2607,    11,   475,   339,   373,  1400,\n",
            "            13,   352,   329,   257,   640,    11,   290,   373,  1969,   329,\n",
            "           262,  1334,   286,   262,   640,    13, 50256]])\n",
            "concatinated_input : Metadata: military jack-conners None None democrat an opinion column in the Times of Trenton Statement: Almost 88,000 United States service members are still missing and unaccounted for, dating back to World War II.[EXP]\n",
            "full_input_length : 134\n",
            "labels : tensor([[ 9171, 14706,    25,  7024,    11,  2704,   273,  3755,    12,   321,\n",
            "           437,   902,    11, 10745,  6410,  4290,    12, 21037,    12, 19290,\n",
            "           274,    12,   392,    12, 11576,   263,    12, 13926,    88,  6045,\n",
            "          4744,  4844,   281,  4455,   512,    13, 21983,    25,   520,    13,\n",
            "         17542,  8511,    82,  1957,  2196,   286,  8441,   604,  8724,   287,\n",
            "          9775, 13079, 17785,   357,  5562,     8,   875, 15655,   262,  1748,\n",
            "            82,  2742,  4466,   290,  4137,   262,  1748,   284,  5298,   262,\n",
            "          3119,  1687,  2494,    13, 50257, 16549, 47630, 16137,   329,   852,\n",
            "           366,   505,   286,   262,  4094,  4736,   287,   262,  1499,   284,\n",
            "          1057,  1802,  1411,   319,  3424,  2568,    13,   366, 16549,   338,\n",
            "          7607,  2313,  1838,   340,  2128,   588, 16137,   318,  1479,   286,\n",
            "          5655,    12, 19082,  1176,    13,   383,  3950,   220,   262,  1748,\n",
            "           338,  1802,  1411, 29479,   540,  6682, 10504,  3038,   318,   517,\n",
            "          8253,    13,  2893,   612,   389,  9188,   286,   262, 15713,  2568,\n",
            "          3884,  1080,    11,   340,   318,  3058,   262,   691,   835,   284,\n",
            "          4155,   262,  5001,   286,  1176,   422,  4077,  4237,    13,   383,\n",
            "           749,  2274,  3136,   905,   326,  5946,    11, 42534, 16137, 12503,\n",
            "           290,   352,    11, 27800,  1402,  5692,  8277,   287,   262,  1748,\n",
            "            82, 42048, 19015, 43068,  6118,    13,  1320,    82,   407,  1802,\n",
            "          1411,   286,   327, 15020, 37749,  2568,  7008,   220,   663,   546,\n",
            "          9773, 25067,   286, 12503,    13, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100, 16549, 47630, 16137,   329,   852,\n",
            "           366,   505,   286,   262,  4094,  4736,   287,   262,  1499,   284,\n",
            "          1057,  1802,  1411,   319,  3424,  2568,    13,   366, 16549,   338,\n",
            "          7607,  2313,  1838,   340,  2128,   588, 16137,   318,  1479,   286,\n",
            "          5655,    12, 19082,  1176,    13,   383,  3950,   220,   262,  1748,\n",
            "           338,  1802,  1411, 29479,   540,  6682, 10504,  3038,   318,   517,\n",
            "          8253,    13,  2893,   612,   389,  9188,   286,   262, 15713,  2568,\n",
            "          3884,  1080,    11,   340,   318,  3058,   262,   691,   835,   284,\n",
            "          4155,   262,  5001,   286,  1176,   422,  4077,  4237,    13,   383,\n",
            "           749,  2274,  3136,   905,   326,  5946,    11, 42534, 16137, 12503,\n",
            "           290,   352,    11, 27800,  1402,  5692,  8277,   287,   262,  1748,\n",
            "            82, 42048, 19015, 43068,  6118,    13,  1320,    82,   407,  1802,\n",
            "          1411,   286,   327, 15020, 37749,  2568,  7008,   220,   663,   546,\n",
            "          9773, 25067,   286, 12503,    13, 50256]])\n",
            "concatinated_input : Metadata: crime,immigration,public-health battleground-texas None Texas democrat an interview on Bloomberg TV Statement: Says Dan Patrick has called immigration into Texas an invasion and said immigrants coming into Texas bring third-world diseases.[EXP]\n",
            "full_input_length : 192\n",
            "labels : tensor([[ 9171, 14706,    25,  5871,    12,  8482,  4867, 28501,    12,    77,\n",
            "          1018,   298, 21623,  3936, 41477,   281,  1034,   276,  5721,    13,\n",
            "         21983,    25,  8759, 14105,   468,  1239,  2626,   281,  3071,   290,\n",
            "          3793,   262,   691,  1048,   284,   423,  1839,   262,  3936,  1089,\n",
            "         11094,  1115,  1661,   287, 36348,  7024,    13, 50257,  1870,    11,\n",
            "         22772,  1559,   531,    11,   339,   561,   407,  3381, 14105,    82,\n",
            "          9130,  1592,   257, 36348,    13,  9676,    11,   339,   531,    11,\n",
            "           262, 26824,  5014,  1411, 30008,   284,   257,  1282,  7211,   590,\n",
            "            26,  3126,  1411,   286,  4446,  7690,   257,  1180, 18808,   415,\n",
            "            13,  3954,  1011,    25, 14105,   318,   262,  2368,   438,  1662,\n",
            "           717,   438, 21607,  8153,   284,  1592,   302,    12, 14300,  5636,\n",
            "           501,    13,  4930,   286,   465,  7864,   547, 41602, 43340, 29433,\n",
            "          1460,   475,   262, 41188, 36348,  3722,   318,   379,  1551,  1915,\n",
            "         21156,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1870,    11,\n",
            "         22772,  1559,   531,    11,   339,   561,   407,  3381, 14105,    82,\n",
            "          9130,  1592,   257, 36348,    13,  9676,    11,   339,   531,    11,\n",
            "           262, 26824,  5014,  1411, 30008,   284,   257,  1282,  7211,   590,\n",
            "            26,  3126,  1411,   286,  4446,  7690,   257,  1180, 18808,   415,\n",
            "            13,  3954,  1011,    25, 14105,   318,   262,  2368,   438,  1662,\n",
            "           717,   438, 21607,  8153,   284,  1592,   302,    12, 14300,  5636,\n",
            "           501,    13,  4930,   286,   465,  7864,   547, 41602, 43340, 29433,\n",
            "          1460,   475,   262, 41188, 36348,  3722,   318,   379,  1551,  1915,\n",
            "         21156,    13, 50256]])\n",
            "concatinated_input : Metadata: elections,market-regulation michael-moore Documentary filmmaker None none an interview on \"The Colbert Report\" Statement: Goldman Sachs was Barack Obama's \"No. 1 private contributor.\"[EXP]\n",
            "full_input_length : 107\n",
            "labels : tensor([[ 9171, 14706,    25,  4173, 30188,   300,   521,  4397,    12,    70,\n",
            "         13220,   471,    13,    50,    13, 12329,  2520,  5913, 41477,   281,\n",
            "          2720,   319,  9738,  3000,     6,  1279,    72,    29,   770,  6119,\n",
            "          7359,    72,    29, 21983,    25,  5689, 21010,   366,    85,  5191,\n",
            "          1028,   262,   717, 12108,  1810,    13,   679,  6886,   262, 13853,\n",
            "            13,   679,  2227,   284, 18398,  3908,   526, 50257,  2504,  6323,\n",
            "           373,  9772,  6337,    12,  4310,    11,   351, 21010,  6709,   287,\n",
            "          2661,    13,  1550,   262,  1923,  8025,    11, 21010,  1838,   262,\n",
            "          1339,   326,   339,   468,   262, 37088, 35879,   284,   467,  1182,\n",
            "            12,  1462,    12,  2256,   351,  3215,  2766,    13,  1081,   281,\n",
            "          1672,    11,   339,  2173,   284,   257,  9656,  8791,   351, 30655,\n",
            "          3554,  3454,   672, 45561,  4460,   577, 25531,  7043,  3428,   262,\n",
            "         29643,   272,  9976,   287,   543,   339,  1444,  4460,   577, 25531,\n",
            "           257,  1175,  4301,   284,   465,  1986,    13,   887, 21010,   338,\n",
            "           734,  5690,   319,   262, 28001,   286,  1175,   287, 10249,  4084,\n",
            "           905,   339, 19344,  3767,  3034,  9388,   625,   262,   779,   286,\n",
            "          2422,  2700,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2504,  6323,\n",
            "           373,  9772,  6337,    12,  4310,    11,   351, 21010,  6709,   287,\n",
            "          2661,    13,  1550,   262,  1923,  8025,    11, 21010,  1838,   262,\n",
            "          1339,   326,   339,   468,   262, 37088, 35879,   284,   467,  1182,\n",
            "            12,  1462,    12,  2256,   351,  3215,  2766,    13,  1081,   281,\n",
            "          1672,    11,   339,  2173,   284,   257,  9656,  8791,   351, 30655,\n",
            "          3554,  3454,   672, 45561,  4460,   577, 25531,  7043,  3428,   262,\n",
            "         29643,   272,  9976,   287,   543,   339,  1444,  4460,   577, 25531,\n",
            "           257,  1175,  4301,   284,   465,  1986,    13,   887, 21010,   338,\n",
            "           734,  5690,   319,   262, 28001,   286,  1175,   287, 10249,  4084,\n",
            "           905,   339, 19344,  3767,  3034,  9388,   625,   262,   779,   286,\n",
            "          2422,  2700,    13, 50256]])\n",
            "concatinated_input : Metadata: health-care,public-health richard-besser ABC News' Chief Health and Medical Editor.  None none comments on ABC's \"This Week\" Statement: It used to be that the only children at school who werent vaccinated were those who had true medical conditions. Now there are 19 states that allow personal belief exemptions.[EXP]\n",
            "full_input_length : 145\n",
            "labels : tensor([[ 9171, 14706,    25,   442,  1437,    11, 33631,   374, 10757,    12,\n",
            "          1640, 12636, 19920,  6025, 41477,   257,  5243,  2720,    13, 21983,\n",
            "            25,  2807,   318,  1016,   284,   423,  5403,   262,  1271,   286,\n",
            "         34031,   356,   423,   287,   655,   625,   257,  5707,    13, 50257,\n",
            "           464,  4025,   966,   286, 29442,  3793,    11,   996,    13,  1002,\n",
            "           339,   338,  1016,   284,   779,  4466, 22729,   284, 12051,  2263,\n",
            "          2057, 25560,  1497,   422,  3595,   661,    11,   339,   815,   379,\n",
            "           257,  5288,   307,  4684,   284,  1085,   416,  1672,   287, 42833,\n",
            "          5469,   465,   898,  2607,  9307,   357,  2339,   867,   286,   465,\n",
            "          6796,  7810,   508,  2380,   881,  4025, 12815,   423,  1760,   737,\n",
            "           366,  5122,  7955,   220,   220,   220,  7383,   274,  5371, 35829,\n",
            "           286,  1262,  1687,  1637,   329,   257,  1097,   329,  2614,   779,\n",
            "            11,  2282,   339,   318,  1871,  3598,   366, 49770,  3615, 45208,\n",
            "             1,   508,   366,  2777,   298,   281,  2811,   286,   720,  1314,\n",
            "            11,   830,   319,  5006,   329,  2405,    13,   366,  1537, 35829,\n",
            "         38522,   257,  1323,   326,   318,   973,   355,   257,   366, 24896,\n",
            "          2607,   553,   407,   257,  1097,   329,  2241,    13, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "           464,  4025,   966,   286, 29442,  3793,    11,   996,    13,  1002,\n",
            "           339,   338,  1016,   284,   779,  4466, 22729,   284, 12051,  2263,\n",
            "          2057, 25560,  1497,   422,  3595,   661,    11,   339,   815,   379,\n",
            "           257,  5288,   307,  4684,   284,  1085,   416,  1672,   287, 42833,\n",
            "          5469,   465,   898,  2607,  9307,   357,  2339,   867,   286,   465,\n",
            "          6796,  7810,   508,  2380,   881,  4025, 12815,   423,  1760,   737,\n",
            "           366,  5122,  7955,   220,   220,   220,  7383,   274,  5371, 35829,\n",
            "           286,  1262,  1687,  1637,   329,   257,  1097,   329,  2614,   779,\n",
            "            11,  2282,   339,   318,  1871,  3598,   366, 49770,  3615, 45208,\n",
            "             1,   508,   366,  2777,   298,   281,  2811,   286,   720,  1314,\n",
            "            11,   830,   319,  5006,   329,  2405,    13,   366,  1537, 35829,\n",
            "         38522,   257,  1323,   326,   318,   973,   355,   257,   366, 24896,\n",
            "          2607,   553,   407,   257,  1097,   329,  2241,    13, 50256]])\n",
            "concatinated_input : Metadata: county-budget,county-government chris-abele Philanthropist Wisconsin none an interview  Statement: On Milwaukee County governments debt burden[EXP]\n",
            "full_input_length : 52\n",
            "labels : tensor([[ 9171, 14706,    25,  2422, 14509,    12,  1102,  2741,  6045,  6045,\n",
            "         43268,   281,  4459,  5721,   287,   262,  3782,   286, 24269,   261,\n",
            "         21983,    25, 16699,  9193,    11,   830,  1578,  1829,  2139,  1866,\n",
            "           389,   991,  4814,   290, 48422,   276,   329,    11, 10691,   736,\n",
            "           284,  2159,  1810,  2873,    13, 50257,  1537,   287,   691,   530,\n",
            "           286,   883,  9231,   373,  8742,   338, 10330,  3744,   621,   262,\n",
            "         10330,   286,  4049,    13,  1002,   345,   779,   262,   749,  2274,\n",
            "          2482,   422,   262,  2972, 13985,  5745,  1201,  3158,    13,   220,\n",
            "           604,    11,  8742,  7864,   287,   734,    11,  8470,   287,   734,\n",
            "           290, 14754,   287,  3598,    13,   775, 11237,  8742,   338, 12262,\n",
            "           284,  1265,   546,   262,  2482,   475,  1422,   470,  3285,   736,\n",
            "            13,  1406,   618,  8742,  1139,   339, 17825,  2605,   287,  3278,\n",
            "           706,  3278,   706,  3278,    11,   339,   338,  1790,   257,  3278,\n",
            "           393,   734,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  1537,   287,   691,   530,\n",
            "           286,   883,  9231,   373,  8742,   338, 10330,  3744,   621,   262,\n",
            "         10330,   286,  4049,    13,  1002,   345,   779,   262,   749,  2274,\n",
            "          2482,   422,   262,  2972, 13985,  5745,  1201,  3158,    13,   220,\n",
            "           604,    11,  8742,  7864,   287,   734,    11,  8470,   287,   734,\n",
            "           290, 14754,   287,  3598,    13,   775, 11237,  8742,   338, 12262,\n",
            "           284,  1265,   546,   262,  2482,   475,  1422,   470,  3285,   736,\n",
            "            13,  1406,   618,  8742,  1139,   339, 17825,  2605,   287,  3278,\n",
            "           706,  3278,   706,  3278,    11,   339,   338,  1790,   257,  3278,\n",
            "           393,   734,    13, 50256]])\n",
            "concatinated_input : Metadata: elections,ethics barack-obama President Illinois democrat Tampa, Fla. Statement: McCain \"hired some of the biggest lobbyists in Washington to run his campaign.\"[EXP]\n",
            "full_input_length : 103\n",
            "labels : tensor([[ 9171, 14706,    25,  4065,    11, 47620,    11, 11377,    12, 13948,\n",
            "         35057,    12, 16886,   292,  6045,  3936, 43268,   281,  2720,   319,\n",
            "         15689,  3195, 21983,    25, 28628,  6035,  9925,   468,  1444,  6272,\n",
            "           656,  3936,   281, 11796,   290,   531,  7971,  2406,   656,  3936,\n",
            "          2222,  2368,    12,  6894, 10040,    13, 50257, 15948,   531,    11,\n",
            "           366,  5122,  1029,  1524, 21571,  2494,   318,   262,  4511,   319,\n",
            "          1700,    13,   366,  1135,   635,  1043,  1194, 13467,  2746,   326,\n",
            "          7584,   262,  1029,   966,   379, 16450,    13,  7831,    11,   262,\n",
            "          2276,  2565,   287,   262,  3707,  2831,   318,   326,  3965,   389,\n",
            "           319,   262,  4485,   290, 15394,  2440,   621,  1160,   393,   772,\n",
            "           838,   812,  2084,    13,   383,  2643,   318, 12387,  7187,   475,\n",
            "          5667,   503,  1593,  3307,    13, 35074,    11,  3035,   860,    11,\n",
            "          1946,    25,   383,  4238,  2196,   286,   428,  1621, 23175,  5081,\n",
            "           326,   262,  7868,  2732,   550,  3421,   703,   340, 10488,   262,\n",
            "         13475, 20138,   805, 17701,  2288, 14806,    11, 12988,   257,  2805,\n",
            "          2211,  1803,  2650,    13,  2102,    11,   326,  2650,   373, 32578,\n",
            "           257,  1180, 24696,    11,   262, 45624, 47222,   419, 17701,  2288,\n",
            "         14806,    11,   543,  2486,   373,   407, 32578,   287,   465,  4046,\n",
            "            13,   383,  1621,   468,   587, 12328,   284,  4079,   428, 17137,\n",
            "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100, 15948,   531,    11,\n",
            "           366,  5122,  1029,  1524, 21571,  2494,   318,   262,  4511,   319,\n",
            "          1700,    13,   366,  1135,   635,  1043,  1194, 13467,  2746,   326,\n",
            "          7584,   262,  1029,   966,   379, 16450,    13,  7831,    11,   262,\n",
            "          2276,  2565,   287,   262,  3707,  2831,   318,   326,  3965,   389,\n",
            "           319,   262,  4485,   290, 15394,  2440,   621,  1160,   393,   772,\n",
            "           838,   812,  2084,    13,   383,  2643,   318, 12387,  7187,   475,\n",
            "          5667,   503,  1593,  3307,    13, 35074,    11,  3035,   860,    11,\n",
            "          1946,    25,   383,  4238,  2196,   286,   428,  1621, 23175,  5081,\n",
            "           326,   262,  7868,  2732,   550,  3421,   703,   340, 10488,   262,\n",
            "         13475, 20138,   805, 17701,  2288, 14806,    11, 12988,   257,  2805,\n",
            "          2211,  1803,  2650,    13,  2102,    11,   326,  2650,   373, 32578,\n",
            "           257,  1180, 24696,    11,   262, 45624, 47222,   419, 17701,  2288,\n",
            "         14806,    11,   543,  2486,   373,   407, 32578,   287,   465,  4046,\n",
            "            13,   383,  1621,   468,   587, 12328,   284,  4079,   428, 17137,\n",
            "            13, 50256]])\n",
            "concatinated_input : Metadata: crime,history,immigration barack-obama President Illinois democrat a speech at the Democratic National Convention Statement: Illegal immigration and the crime rate are as low as theyve been in decades.[EXP]\n",
            "full_input_length : 44\n",
            "labels : tensor([[ 9171, 14706,    25,  7024,    11, 10728,    12, 27727,   285, 40302,\n",
            "            12,  5908,   382, 16854,   560, 26479,  6045,  4844,   281,  2720,\n",
            "           319,   366,   464, 31368,  6358,     1, 21983,    25, 18740, 25158,\n",
            "           373,  8732,  2486,   338,   366,  2949,    13,   352,  2839, 18920,\n",
            "           526, 50257, 47073,   750,  1104,   326, 19572,    11,   290,  1705,\n",
            "          6685,   905,   326, 30970,   468,  2622,  3224,  1181,  6133,  5054,\n",
            "           625,   262,   812,    13,   887,   262,   512,  8470,   262,   366,\n",
            "         35546,     1,   284,   262,  3119,  1687, 19572,    11,   290,   262,\n",
            "           691, 12066,  3264,  5676,   389,   262,  1748,    11,   262,  1524,\n",
            "          4783,   290,   262,  7968,  1377,   407,   883,  4446,   287,  4312,\n",
            "          5235,   290,  6251, 18452, 14683,    13, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100, 47073,   750,  1104,   326, 19572,    11,   290,  1705,\n",
            "          6685,   905,   326, 30970,   468,  2622,  3224,  1181,  6133,  5054,\n",
            "           625,   262,   812,    13,   887,   262,   512,  8470,   262,   366,\n",
            "         35546,     1,   284,   262,  3119,  1687, 19572,    11,   290,   262,\n",
            "           691, 12066,  3264,  5676,   389,   262,  1748,    11,   262,  1524,\n",
            "          4783,   290,   262,  7968,  1377,   407,   883,  4446,   287,  4312,\n",
            "          5235,   290,  6251, 18452, 14683,    13, 50256]])\n",
            "concatinated_input : Metadata: bipartisanship david-barton President, Wallbuilders Texas republican an open letter Statement: Says state Rep. Jim Keffer, a GOP lieutenant to House Speaker Joe Straus, \"did mail pieces for Democrat Mark Strama to help him defeat\" a Republican.[EXP]\n",
            "full_input_length : 162\n",
            "labels : tensor([[ 9171, 14706,    25,  1535,    12,  6651,    11, 11377,    12, 13948,\n",
            "          5527,   446,    12,    65,   408,   263,  9738,  3000,     6,  5953,\n",
            "          3893,   290,  8366, 12058,    13,   220,  6045,  4844,  3651,   319,\n",
            "          9738,   338,   366,  1212,  6119,     1, 21983,    25,   632,   973,\n",
            "           284,   307,   326,   262,   691,  1751,   379,  1524,   508,   547,\n",
            "           429, 37663,   547,   883,   508,   550,  2081,  3315,  3403,    13,\n",
            "          2735,   612,   389,   678,  2585,   326,  1249,  2614,  4901, 30244,\n",
            "            13, 50257,    39,  2879,    73,  8546,   531,   262,  3415,  3615,\n",
            "          3859,   366,    82,   592, 29489,  7288,   290,   284,  5968,   351,\n",
            "           340,    13,   366,   464,  3936,  6796,  3859, 28928,  5293,  6272,\n",
            "           290,   597,  2742,  3722,   329, 22959,   471,    13,   311,    13,\n",
            "           220,  5085,    11,   543,  5667, 29489,   602,   257,  5885,    13,\n",
            "           887,   262,  3859, 46701, 13189,   355,  1290,   355,   262,  8900,\n",
            "           531,    13,   632,  1595,   470,   869,   329,   390, 26527,  7288,\n",
            "          2877,   994, 15572,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,    39,  2879,    73,  8546,   531,   262,  3415,  3615,\n",
            "          3859,   366,    82,   592, 29489,  7288,   290,   284,  5968,   351,\n",
            "           340,    13,   366,   464,  3936,  6796,  3859, 28928,  5293,  6272,\n",
            "           290,   597,  2742,  3722,   329, 22959,   471,    13,   311,    13,\n",
            "           220,  5085,    11,   543,  5667, 29489,   602,   257,  5885,    13,\n",
            "           887,   262,  3859, 46701, 13189,   355,  1290,   355,   262,  8900,\n",
            "           531,    13,   632,  1595,   470,   869,   329,   390, 26527,  7288,\n",
            "          2877,   994, 15572,    13, 50256]])\n",
            "concatinated_input : Metadata: public-health,taxes nancy-nathanson Representative in the Oregon House Oregon democrat a House floor speech. Statement: Says a pack-a-day smoker who quits because of the tax increase will save about $1,650 a year.[EXP]\n",
            "full_input_length : 105\n",
            "labels : tensor([[ 9171, 14706,    25,  7968,    12, 37315,    11,  9127,    88,    12,\n",
            "         14480,   442,  2442,    12,   397, 11129,  4543, 22178,   396,  9279,\n",
            "          4844,   281,  2720,   220, 21983,    25,  1550, 16629,  3418,  6905,\n",
            "          5057, 10538, 50257,   464,  2486,   512,  5644,   612,   373,   617,\n",
            "         23077, 42689,   994,    11,   475,   356,   836,   470,   766,   340,\n",
            "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,   464,  2486,   512,  5644,   612,   373,   617,\n",
            "         23077, 42689,   994,    11,   475,   356,   836,   470,   766,   340,\n",
            "            13, 50256]])\n",
            "concatinated_input : Metadata: education john-edwards former senator North Carolina democrat Des Moines, Iowa. Statement: Rural schools \"face the highest dropout rates, the lowest college enrollment rates, have the lowest average teacher salaries...\"[EXP]\n",
            "full_input_length : 129\n",
            "labels : tensor([[ 9171, 14706,    25,  7024,    11,  2788,   873,  2318,   441,    12,\n",
            "           672,  1689,  1992,  9486, 43268, 15528,    11, 22026,    13, 21983,\n",
            "            25, 14264,   366,    71,  1202,   617,   286,   262,  4094, 26637,\n",
            "           287,  2669,   284,  1057,   465,  1923,   526, 50257, 16454,    11,\n",
            "           523, 14264,   468,  4084,   900,   257,  2450,   326, 44114,  1459,\n",
            "          2717, 26637,   422,  8263,   257,  1923, 37751,   290,  1923,  2828,\n",
            "           910,   484,   389,   287,  1336, 11846,   351,   326,    13,   887,\n",
            "           262,  2450,  2058,   517,   621,   257,   614,   656,   262,  1923,\n",
            "            13,  9236, 26637,  9657,   416,   262,  1923,   743,   423,   783,\n",
            "           587,  1308,  2004,    11,   475,   262,  1109,   318,   484,   547,\n",
            "          9657,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 16454,    11,\n",
            "           523, 14264,   468,  4084,   900,   257,  2450,   326, 44114,  1459,\n",
            "          2717, 26637,   422,  8263,   257,  1923, 37751,   290,  1923,  2828,\n",
            "           910,   484,   389,   287,  1336, 11846,   351,   326,    13,   887,\n",
            "           262,  2450,  2058,   517,   621,   257,   614,   656,   262,  1923,\n",
            "            13,  9236, 26637,  9657,   416,   262,  1923,   743,   423,   783,\n",
            "           587,  1308,  2004,    11,   475,   262,  1109,   318,   484,   547,\n",
            "          9657,    13, 50256]])\n",
            "concatinated_input : Metadata: crime,criminal-justice cory-booker U.S. senator New Jersey democrat a post on Instagram Statement: We built a new prison every 10 days between 1990 and 2005 to keep up with our mass incarceration explosion of nonviolent offenders.[EXP]\n",
            "full_input_length : 133\n",
            "labels : tensor([[ 9171, 14706,    25,  4065,    11, 23569,    11, 47620,  2318,   441,\n",
            "            12,   672,  1689,  1992,  9486, 43268,   257,  4046,   379,   262,\n",
            "          4390,  2351, 11680, 21983,    25, 42272,  6272,   290,   262,  4065,\n",
            "          2494,   389,   355,  1877,   355,   484,   303,   587,   287,  4647,\n",
            "            13, 50257, 14202, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100, 14202, 50256]])\n",
            "concatinated_input : Metadata: economy,jobs,women eric-bolling Co-host on Fox News Channel's \"The Five\" None none a discussion on Fox News' \"The Five\" Statement: Of all the jobs President Obama claims to have created since he started, only 38.5 percent are women. So 61.5 percent have gone to men.[EXP]\n",
            "full_input_length : 106\n",
            "labels : tensor([[ 9171, 14706,    25, 14141,   433, 26100,  1056, 21970,    12, 16575,\n",
            "           261,  1992,    11,  5007, 50034,  3936, 41477,   281,  1280,  3850,\n",
            "         21983,    25, 28628,  1181,  1432,    13,  5395,  3873, 36761,    11,\n",
            "           257,  6796, 30269,   284,  2097, 14931,  5689, 15195,   385,    11,\n",
            "           366, 20839,  6920,  5207,   329,  9755,  2940,   520, 20058,   284,\n",
            "          1037,   683,  7433,     1,   257,  3415,    13, 50257, 16977,    25,\n",
            "          1550,  1737,   604,    11,   706,   428,  2378,   373,  3199,    11,\n",
            "         20585,  1297,   262, 13117, 26716,   326,  4756,  4744, 10150,   284,\n",
            "         18479, 19747,   468,   587,   366,   525,   805,  1473,   256,  4510,\n",
            "             1,  2233,   284,  4786,   625,   262, 41593,  3056, 19431,    13,\n",
            "         20585,   531,   339,   481,   407,  4574, 19747,  1141,   465,   734,\n",
            "           812,   355, 14931,    13,  1052,  5778,   588,   428,   468,   429,\n",
            "          3022,   287,   616,  4044, 10869,    11,   339,   531,    13,   314,\n",
            "          4719,   547,  1016,   284,   651, 12872,  7429,   685,   292,   284,\n",
            "           262,  2728,    60,   287,   734,   812,    11,   290,   355,   257,\n",
            "          2300,   286,  3381,  7095,    11,  1846,  1760,   287,   734,   812,\n",
            "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 16977,    25,\n",
            "          1550,  1737,   604,    11,   706,   428,  2378,   373,  3199,    11,\n",
            "         20585,  1297,   262, 13117, 26716,   326,  4756,  4744, 10150,   284,\n",
            "         18479, 19747,   468,   587,   366,   525,   805,  1473,   256,  4510,\n",
            "             1,  2233,   284,  4786,   625,   262, 41593,  3056, 19431,    13,\n",
            "         20585,   531,   339,   481,   407,  4574, 19747,  1141,   465,   734,\n",
            "           812,   355, 14931,    13,  1052,  5778,   588,   428,   468,   429,\n",
            "          3022,   287,   616,  4044, 10869,    11,   339,   531,    13,   314,\n",
            "          4719,   547,  1016,   284,   651, 12872,  7429,   685,   292,   284,\n",
            "           262,  2728,    60,   287,   734,   812,    11,   290,   355,   257,\n",
            "          2300,   286,  3381,  7095,    11,  1846,  1760,   287,   734,   812,\n",
            "            13, 50256]])\n",
            "concatinated_input : Metadata: federal-budget marco-rubio U.S. Senator Florida republican a Fox News interview Statement: Forty cents of every dollar being spent by the federal government is being borrowed from my children.[EXP]\n",
            "full_input_length : 176\n",
            "labels : tensor([[ 9171, 14706,    25,  1171,    12, 13948,    11, 19290,   274,   299,\n",
            "          3883,    12,    77,   776, 23103, 19920,   287,   262,  8819,  2097,\n",
            "          8819, 43268,   257,  2097,  4314,  4046,    13, 21983,    25, 28628,\n",
            "           257,  2353,    12,    64,    12,   820, 41644,   508,   627,   896,\n",
            "           780,   286,   262,  1687,  2620,   481,  3613,   546,   720,    16,\n",
            "            11, 17544,   257,   614,    13, 50257,    45, 12305,   531,   326,\n",
            "          5023,  3936,    11, 11565,   468,   257,  2818, 25734,  3884,  7955,\n",
            "            13,  1320,    82,  3376,    11,   996,  3936,   318,  1969,    26,\n",
            "           734,  1688,  2594,  2494,   262,  5749,  2585, 10351,  5057, 25734,\n",
            "            11,   530,  3965,   340, 15923, 28200,   543,   318,   663,  1306,\n",
            "            12, 35323,  7955,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,    45, 12305,   531,   326,\n",
            "          5023,  3936,    11, 11565,   468,   257,  2818, 25734,  3884,  7955,\n",
            "            13,  1320,    82,  3376,    11,   996,  3936,   318,  1969,    26,\n",
            "           734,  1688,  2594,  2494,   262,  5749,  2585, 10351,  5057, 25734,\n",
            "            11,   530,  3965,   340, 15923, 28200,   543,   318,   663,  1306,\n",
            "            12, 35323,  7955,    13, 50256]])\n",
            "concatinated_input : Metadata: state-budget,transportation rick-perry Governor Texas republican a campaign blog entry during the GOP gubernatorial debate Statement: The Trans-Texas Corridor is dead.[EXP]\n",
            "full_input_length : 238\n",
            "labels : tensor([[ 9171, 14706,    25,  3707, 45610,    12,   276,  2017,  1966, 12329,\n",
            "          2258,  5913, 43268,  2935, 36576,    11,  9406,    13, 21983,    25,\n",
            "         36486,  4266,   366,  2550,   262,  4511,  4268,   448,  3965,    11,\n",
            "           262,  9016,  4152, 20753,  3965,    11,   423,   262,  9016,  2811,\n",
            "          4701, 17058,  9313, 50257,  1212,  1227,    11,   339,   531,    11,\n",
            "           339,  3352,   284,  1656,   379,   281,  2556,    13,   220,  2310,\n",
            "          4540, 10041, 12007,   416,   262,  7710,   286, 10204,  3415,  6926,\n",
            "         16741,   287, 20874,  6379,    13,  3954,  1011,    25, 25075, 33500,\n",
            "           750,   407,  1656,   379,  5193, 14216,    11,  3624,   286,   606,\n",
            "           706,   262,  1755,   339,  3414,   465, 24514,    13,   887,   663,\n",
            "           257,   285,   578, 18269,   284,   910,   339, 15828, 26684,   262,\n",
            "          2995,    13,   554,   749,  2663,    11,   465,  1923,   531, 27091,\n",
            "         25075, 33500,   373,   429,  1016,   284,  1656,    13, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  1212,  1227,    11,   339,   531,    11,\n",
            "           339,  3352,   284,  1656,   379,   281,  2556,    13,   220,  2310,\n",
            "          4540, 10041, 12007,   416,   262,  7710,   286, 10204,  3415,  6926,\n",
            "         16741,   287, 20874,  6379,    13,  3954,  1011,    25, 25075, 33500,\n",
            "           750,   407,  1656,   379,  5193, 14216,    11,  3624,   286,   606,\n",
            "           706,   262,  1755,   339,  3414,   465, 24514,    13,   887,   663,\n",
            "           257,   285,   578, 18269,   284,   910,   339, 15828, 26684,   262,\n",
            "          2995,    13,   554,   749,  2663,    11,   465,  1923,   531, 27091,\n",
            "         25075, 33500,   373,   429,  1016,   284,  1656,    13, 50256]])\n",
            "concatinated_input : Metadata: energy jeff-merkley U.S. Senator Oregon democrat a letter to President Barack Obama Statement: Tapping the Strategic Petroleum Reserve would immediately lower gasoline prices[EXP]\n",
            "full_input_length : 101\n",
            "labels : tensor([[ 9171, 14706,    25,  4065,    11, 45955,    12, 31012,   269,   652,\n",
            "            12,  2070,   263,   471,    13,    50,    13, 12329,   968,  8221,\n",
            "         43268,   257,  1281,   319, 10767, 21983,    25,   775,  3170,   257,\n",
            "           649,  3770,   790,   838,  1528,  1022,  6303,   290,  5075,   284,\n",
            "          1394,   510,   351,   674,  2347, 27224, 11278,   286, 42582, 17120,\n",
            "            13, 50257, 10482,   263,   531,    11,   366,  1135,  3170,   257,\n",
            "           649,  3770,   790,   838,  1528,  1022,  6303,   290,  5075,   284,\n",
            "          1394,   510,   351,   674,  2347, 27224, 11278,   286, 42582, 17120,\n",
            "            13,   366, 10482,   263,   318,   826,   326,   257,   649,  3770,\n",
            "           373,  3170,   790,   838,  1528,  1141,   326,  2278,    13,   887,\n",
            "           981,   612,   373,   257,  2383,  4485,   287,   262, 27224,  2494,\n",
            "           286, 42582, 17120,    11,   340,  1718,  1295,   625,   257,  2392,\n",
            "           640,  2278,   621,   262,  1281,  9217,  1377,  1022,  7169,   290,\n",
            "          2211,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100, 10482,   263,   531,    11,   366,  1135,  3170,   257,\n",
            "           649,  3770,   790,   838,  1528,  1022,  6303,   290,  5075,   284,\n",
            "          1394,   510,   351,   674,  2347, 27224, 11278,   286, 42582, 17120,\n",
            "            13,   366, 10482,   263,   318,   826,   326,   257,   649,  3770,\n",
            "           373,  3170,   790,   838,  1528,  1141,   326,  2278,    13,   887,\n",
            "           981,   612,   373,   257,  2383,  4485,   287,   262, 27224,  2494,\n",
            "           286, 42582, 17120,    11,   340,  1718,  1295,   625,   257,  2392,\n",
            "           640,  2278,   621,   262,  1281,  9217,  1377,  1022,  7169,   290,\n",
            "          2211,    13, 50256]])\n",
            "concatinated_input : Metadata: civil-rights lynn-derbyshire faculty member, University of Rhode Island Rhode Island newsmaker The Providence Journal Statement: If you are black or brown, you are nine times more likely to be stopped and frisked in New York City.[EXP]\n",
            "full_input_length : 197\n",
            "labels : tensor([[ 9171, 14706,    25,  3773,    11, 43863,    11, 25878,  1931,   291,\n",
            "            12,    65,   692,   278,  1766,    12,  4774,   319,  5426,  3000,\n",
            "         11102,   338,   366,   464, 10579,     1,  6045,  4844,   257,  5114,\n",
            "           319,  5426,  3000,     6,   366,   464, 10579,     1, 21983,    25,\n",
            "          3226,   477,   262,  3946,  1992,  2486,  3667,   284,   423,  2727,\n",
            "          1201,   339,  2067,    11,   691,  4353,    13,    20,  1411,   389,\n",
            "          1466,    13,  1406,  8454,    13,    20,  1411,   423,  3750,   284,\n",
            "          1450,    13, 50257,    47, 33421,  2077,   326,  1227, 16449,   655,\n",
            "           739,  3933,  1411,    13,   887,   262,  7955,   750,  7074,   326,\n",
            "         18335,  5403,  1626,   262,  1511,  9231,  2077,   287,  2805,    11,\n",
            "          3035,   290,  1737,  3717,    13, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,    47, 33421,  2077,   326,  1227, 16449,   655,\n",
            "           739,  3933,  1411,    13,   887,   262,  7955,   750,  7074,   326,\n",
            "         18335,  5403,  1626,   262,  1511,  9231,  2077,   287,  2805,    11,\n",
            "          3035,   290,  1737,  3717,    13, 50256]])\n",
            "concatinated_input : Metadata: cap-and-trade,climate-change,energy,environment,message-machine-2012 democratic-national-committee None None none a Web ad Statement: Says Mitt Romney has said different things about whether global warming is caused by humans[EXP]\n",
            "full_input_length : 68\n",
            "labels : tensor([[ 9171, 14706,    25,  2717,    12, 37315,  1667,  1073,    12, 25089,\n",
            "           952,   471,    13,    50,    13,  8962,  4744, 41477,   257,  5426,\n",
            "          3000,  2720, 21983,    25, 38223, 16059,   286,   790,  8872,   852,\n",
            "          3377,   416,   262,  2717,  1230,   318,   852, 22546,   422,   616,\n",
            "          1751,    13, 50257,  6170,   531,  3284,   468,   366,    83,  2787,\n",
            "           437,   516,  3146,   286,  4523, 50046,   220,   352,    11,  7410,\n",
            "            11,   416,   262,   835,   220,   810,   484,  9902,    11,   290,\n",
            "           356,  1422,   470,    13,   366, 25262,  2813,   290,  1584,    11,\n",
            "          3284,   468,  9902,   663, 12380, 10039,  4523, 50046,   422,   352,\n",
            "            11, 46096,   284,   352,    11, 41060,    11,   981,   262,  1578,\n",
            "          1829, 11832,   663, 12380, 10039,  4523, 50046,   625,   262,   976,\n",
            "          2278,    13,   887,   326, 24696, 24245,   326,  1887,  4448,  2472,\n",
            "          1271,   286, 50046,    11, 12380,   290,   287,   663, 14072,    11,\n",
            "           318,   546,   262,   976,   355,   262,  1578,  1829,    13,  5747,\n",
            "           423,   546,   604,    11,  4059,  4523, 50046,   287,   511,  2472,\n",
            "         29534,  2915,    13,   843,  6154,   531,   262,  2274, 20240,   287,\n",
            "          1887,  4448, 12380,  3777,   318,  8584,    11,  2138,   621, 29105,\n",
            "           286,   281, 18644,  5182,    13, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  6170,   531,  3284,   468,   366,    83,  2787,\n",
            "           437,   516,  3146,   286,  4523, 50046,   220,   352,    11,  7410,\n",
            "            11,   416,   262,   835,   220,   810,   484,  9902,    11,   290,\n",
            "           356,  1422,   470,    13,   366, 25262,  2813,   290,  1584,    11,\n",
            "          3284,   468,  9902,   663, 12380, 10039,  4523, 50046,   422,   352,\n",
            "            11, 46096,   284,   352,    11, 41060,    11,   981,   262,  1578,\n",
            "          1829, 11832,   663, 12380, 10039,  4523, 50046,   625,   262,   976,\n",
            "          2278,    13,   887,   326, 24696, 24245,   326,  1887,  4448,  2472,\n",
            "          1271,   286, 50046,    11, 12380,   290,   287,   663, 14072,    11,\n",
            "           318,   546,   262,   976,   355,   262,  1578,  1829,    13,  5747,\n",
            "           423,   546,   604,    11,  4059,  4523, 50046,   287,   511,  2472,\n",
            "         29534,  2915,    13,   843,  6154,   531,   262,  2274, 20240,   287,\n",
            "          1887,  4448, 12380,  3777,   318,  8584,    11,  2138,   621, 29105,\n",
            "           286,   281, 18644,  5182,    13, 50256]])\n",
            "concatinated_input : Metadata: public-health,trade doctors-without-borders None None none a press release Statement: Trade deals threaten Indias role as the pharmacy of the developing world for new HIV medicines.[EXP]\n",
            "full_input_length : 59\n",
            "labels : tensor([[ 9171, 14706,    25,  1181,    12, 37315,    11,  7645, 10189,   374,\n",
            "           624,    12,   525,   563, 10807,  3936, 41477,   257,  1923,  4130,\n",
            "          5726,  1141,   262,  6796, 45265,  4384, 21983,    25,   383,  3602,\n",
            "            12, 21607, 48994,   318,  2636,    13, 50257,    43,  1765, 20951,\n",
            "           531,    11,   366,  3673,   257,  2060, 24270,  2330,  1048,   468,\n",
            "           587,  2823,   416,   262,  1644,     1,  1201,   673,   468,  5615,\n",
            "           287,   968,  1971,    13,   770,   373,   257,  6842,   286,  1109,\n",
            "            12,  9122,   284, 46825,   284,   262,  2323,    11,   287,   636,\n",
            "           780,   286,   262,  3092,   286, 11113,   422,   262,   968,  1971,\n",
            "          4287,  2732,    11,   287,   636,   780,   356,   547,  2045,   379,\n",
            "           257,   640,  2278,   326,   599,  3577,   546,  4153,   812,    11,\n",
            "           290,   287,   636,   780,   340,   318,  1598,   326,  5510,    12,\n",
            "         17636,   290, 30714,   547,  4084,  2823,   379,   416,   968,  1971,\n",
            "          1644,   881,   517,   621, 13216,    13,  1439,   326,   531,    11,\n",
            "           281,  3781,   286,  1644,  1366,   905,  1644,  2823,  9264,   286,\n",
            "         24270,  2330,   661,  1141,   262,  1903,  8069,    82,    11,   262,\n",
            "          1903,  1735,   286,  1004,  8176,  4224,    82,  1204,   287,   968,\n",
            "          1971,  2254,    13,  2893,   356,  4601,   356,   550,  1844,  1366,\n",
            "           319,  1644, 17690,   416,  1123,  4970,  3234,   290,  6936,  3722,\n",
            "           329,   262,  7169,    82,   319,    11,   356, 17666,  6646,   761,\n",
            "           340,   284,  5052,  1004,  8176,  4224,    82,  1948,  2643,    13,\n",
            "           632,   318,   845,  7187,   284,   910, 15102,   389,  2823,   379,\n",
            "           517,  1690,   416,  1644,   621, 13216,    13,   887,   326,   318,\n",
            "           407,   644,  1004,  8176,  4224,   531,    13, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,    43,  1765, 20951,\n",
            "           531,    11,   366,  3673,   257,  2060, 24270,  2330,  1048,   468,\n",
            "           587,  2823,   416,   262,  1644,     1,  1201,   673,   468,  5615,\n",
            "           287,   968,  1971,    13,   770,   373,   257,  6842,   286,  1109,\n",
            "            12,  9122,   284, 46825,   284,   262,  2323,    11,   287,   636,\n",
            "           780,   286,   262,  3092,   286, 11113,   422,   262,   968,  1971,\n",
            "          4287,  2732,    11,   287,   636,   780,   356,   547,  2045,   379,\n",
            "           257,   640,  2278,   326,   599,  3577,   546,  4153,   812,    11,\n",
            "           290,   287,   636,   780,   340,   318,  1598,   326,  5510,    12,\n",
            "         17636,   290, 30714,   547,  4084,  2823,   379,   416,   968,  1971,\n",
            "          1644,   881,   517,   621, 13216,    13,  1439,   326,   531,    11,\n",
            "           281,  3781,   286,  1644,  1366,   905,  1644,  2823,  9264,   286,\n",
            "         24270,  2330,   661,  1141,   262,  1903,  8069,    82,    11,   262,\n",
            "          1903,  1735,   286,  1004,  8176,  4224,    82,  1204,   287,   968,\n",
            "          1971,  2254,    13,  2893,   356,  4601,   356,   550,  1844,  1366,\n",
            "           319,  1644, 17690,   416,  1123,  4970,  3234,   290,  6936,  3722,\n",
            "           329,   262,  7169,    82,   319,    11,   356, 17666,  6646,   761,\n",
            "           340,   284,  5052,  1004,  8176,  4224,    82,  1948,  2643,    13,\n",
            "           632,   318,   845,  7187,   284,   910, 15102,   389,  2823,   379,\n",
            "           517,  1690,   416,  1644,   621, 13216,    13,   887,   326,   318,\n",
            "           407,   644,  1004,  8176,  4224,   531,    13, 50256]])\n",
            "concatinated_input : Metadata: candidates-biography,pensions anita-perry First Lady of Texas Texas republican a campaign stop. Statement: My son had to resign his job because of federal regulations that Washington has put on us.[EXP]\n",
            "full_input_length : 112\n",
            "labels : tensor([[ 9171, 14706,    25,  2568, 11223,   487,    12,   647,    74,  1636,\n",
            "           471,    13,    50,    13,  8962,  8819, 43268,   257,  3850,   284,\n",
            "          1992,  8732,  2486, 21983,    25,   309,  5912,   262, 24999, 39970,\n",
            "         12224,   561,  3393,  2793, 21408,  4536, 50257,  4864,    11,   262,\n",
            "          2472,  1271,   286, 32149,   468,  1201,  9292,    13,  6060,   329,\n",
            "           262,   749,  2274,   614,    11,  2321,    11,  2523,  1597, 32149,\n",
            "           379,   546,   644,   484,   547,   287,  5075,    13,   383,  5873,\n",
            "           286,  9611,   326,  4838,   373,  2407,  1029,   287,  3717,    11,\n",
            "           475,   340,   373,   429,   262,  4511,   966,    11,   290,   517,\n",
            "          2904,    11,   340,   318,   287,  1627,   351,  6754,  5538,    13,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  4864,    11,   262,\n",
            "          2472,  1271,   286, 32149,   468,  1201,  9292,    13,  6060,   329,\n",
            "           262,   749,  2274,   614,    11,  2321,    11,  2523,  1597, 32149,\n",
            "           379,   546,   644,   484,   547,   287,  5075,    13,   383,  5873,\n",
            "           286,  9611,   326,  4838,   373,  2407,  1029,   287,  3717,    11,\n",
            "           475,   340,   373,   429,   262,  4511,   966,    11,   290,   517,\n",
            "          2904,    11,   340,   318,   287,  1627,   351,  6754,  5538,    13,\n",
            "         50256]])\n",
            "concatinated_input : Metadata: poverty shaun-donovan Secretary of Housing and Urban Development New York democrat an interview on \"The Daily Show\" Statement: It costs about $40,000 a year for a homeless person to be on the streets.[EXP]\n",
            "full_input_length : 159\n",
            "labels : tensor([[ 9171, 14706,    25,  3026,    12, 28046, 31432,    77,    12,  1082,\n",
            "         48209, 10695, 12829,  2888,    11,  2059,   286, 24545,  5451, 24545,\n",
            "          5451,  1705, 10297,   383, 28319,  4913, 21983,    25,  1002,   345,\n",
            "           389,  2042,   393,  7586,    11,   345,   389,  5193,  1661,   517,\n",
            "          1884,   284,   307,  5025,   290,  1216,  1984,   276,   287,   968,\n",
            "          1971,  2254,    13, 50257, 17636,   329,  9241, 17893,   318,  3376,\n",
            "           326,  3869, 18279,  1023,  1592, 23400,   389, 25069,    11,   290,\n",
            "           340,  3769,   617,   443, 16172,   416,  2282,   326,   471,    13,\n",
            "           311,    13,   220, 14591,   714,   307, 31075,   510,   284,   720,\n",
            "            24,    11,   830,    13,  7831,    11,   663,   407,  1884,   326,\n",
            "          2687,   561,  1414,   326,   881,   583, 18279,   287,  5704,  1377,\n",
            "           772,   611,   262,  8464,   373, 20200,  1576,   284,   423,  5079,\n",
            "          3739,   880,   625,   720, 23734,    11,   830,   290,  6520,   284,\n",
            "         18777,   597,  1597,  9307,   319,   511,  1592, 23400,    13,  4377,\n",
            "         42627,  2861,   511,  8268,   815,   307,  1498,   284,   651,   262,\n",
            "          2494,   286,  1687,   319, 18279,  1592, 23400,   881,  2174,   720,\n",
            "            24,    11,   830,    11,   290,  3863,   772,   284,  6632,    13,\n",
            "         48483, 20673, 24550,    25,  2293,   674,  2708,  4120,    11,  3399,\n",
            "           329,  9241, 17893,  4481,   257, 46472,   282,   284,   674,  2708,\n",
            "           326,   460,   307,  1100,   994,    13, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100, 17636,   329,  9241, 17893,   318,  3376,\n",
            "           326,  3869, 18279,  1023,  1592, 23400,   389, 25069,    11,   290,\n",
            "           340,  3769,   617,   443, 16172,   416,  2282,   326,   471,    13,\n",
            "           311,    13,   220, 14591,   714,   307, 31075,   510,   284,   720,\n",
            "            24,    11,   830,    13,  7831,    11,   663,   407,  1884,   326,\n",
            "          2687,   561,  1414,   326,   881,   583, 18279,   287,  5704,  1377,\n",
            "           772,   611,   262,  8464,   373, 20200,  1576,   284,   423,  5079,\n",
            "          3739,   880,   625,   720, 23734,    11,   830,   290,  6520,   284,\n",
            "         18777,   597,  1597,  9307,   319,   511,  1592, 23400,    13,  4377,\n",
            "         42627,  2861,   511,  8268,   815,   307,  1498,   284,   651,   262,\n",
            "          2494,   286,  1687,   319, 18279,  1592, 23400,   881,  2174,   720,\n",
            "            24,    11,   830,    11,   290,  3863,   772,   284,  6632,    13,\n",
            "         48483, 20673, 24550,    25,  2293,   674,  2708,  4120,    11,  3399,\n",
            "           329,  9241, 17893,  4481,   257, 46472,   282,   284,   674,  2708,\n",
            "           326,   460,   307,  1100,   994,    13, 50256]])\n",
            "concatinated_input : Metadata: education,state-budget,taxes scott-walker Milwaukee County Executive Wisconsin republican a news release Statement: Says that even if his budget is adopted, private schools in the choice program would be getting about half the per-pupil funds that public schools receive[EXP]\n",
            "full_input_length : 191\n",
            "labels : tensor([[ 9171, 14706,    25,  1451,    12,   392,    12, 25351,    11, 42570,\n",
            "            12,  3803,    11, 22554,    11, 38986,    11, 20500,    12, 30243,\n",
            "            12,  6999, 10518,    12, 14648,    12, 26799,  6045,  6045,  4844,\n",
            "           257,  5313,   512, 21983,    25, 28628, 16627,  8431,   468,   531,\n",
            "          1180,  1243,   546,  1771,  3298,  9917,   318,  4073,   416,  5384,\n",
            "         50257,  1544,   655,  9181,   606,   355,  3858,   286,  6958,   326,\n",
            "           815,   407,   307,  8867,   329,  4845,    13, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  1544,   655,  9181,   606,   355,  3858,   286,  6958,   326,\n",
            "           815,   407,   307,  8867,   329,  4845,    13, 50256]])\n",
            "concatinated_input : Metadata: debt,deficit,federal-budget bill-nelson None Florida democrat an interview on Central Florida News 13 Statement: In a budget deal struck in 2011 a trillion dollars of cuts went into effect immediately, and then a special committee was set up ... to get agreement on another $3 trillion of cuts.[EXP]\n",
            "full_input_length : 143\n",
            "labels : tensor([[ 9171, 14706,    25,  1171,    12, 13948,    11, 25351,  7519,    12,\n",
            "         19419,    12,    65,  6361,  6045,  6045,  4844,   257,  1803,  2650,\n",
            "         21983,    25,  9601,  7529, 16180,  1423,  4448,  2597,   355,   262,\n",
            "         32763,   286,   262,  5922,   995,   329,   649, 10498, 23533,    13,\n",
            "         50257,     1,  1858,   318,  2267,   284,  7603,   326, 17849,  4953,\n",
            "          9574,   389,  6692,   351,  2793,  7341,  3965,    13, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,     1,  1858,   318,  2267,   284,  7603,   326, 17849,  4953,\n",
            "          9574,   389,  6692,   351,  2793,  7341,  3965,    13, 50256]])\n",
            "concatinated_input : Metadata: agriculture,energy,gas-prices scott-walker Milwaukee County Executive Wisconsin republican a speech Statement: On support for ethanol mandates[EXP]full_input_length : 65\n",
            "\n",
            "labels : tensor([[ 9171, 14706,    25,  5871,    12,  8482,  4867,    11,    79,  5736,\n",
            "           281,  5350,    12,   525,   563,  3274, 11182,   286,  3936,  3936,\n",
            "         41477,   257,  1923,  2245,    13, 21983,    25,  2011,  3367,   550,\n",
            "           284, 10931,   465,  1693,   780,   286,  2717,  6647,   326,  2669,\n",
            "           468,  1234,   319,   514,    13, 50257,  2396,  8454,    13,   642,\n",
            "          1411,   423,  3750,   284,  1450,    13,   366,   464,  4257, 26500,\n",
            "           284,  1693,  8810,  1141,  1835, 17485, 12112,  5679,   257,  1913,\n",
            "          3912,   286,  4257,  1693,  9089,   287,   262,  1933,   286,   262,\n",
            "         16457,   878,  2486,  1718,  2607,    13,  1406,   262,  5279,  5236,\n",
            "           286,  1693,  6282,   468,   517,   284,   466,   351, 11700,   605,\n",
            "          3034,  5087,   621,  1997,   262,  3662,   750,   393, 42547,   466,\n",
            "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  2396,  8454,    13,   642,\n",
            "          1411,   423,  3750,   284,  1450,    13,   366,   464,  4257, 26500,\n",
            "           284,  1693,  8810,  1141,  1835, 17485, 12112,  5679,   257,  1913,\n",
            "          3912,   286,  4257,  1693,  9089,   287,   262,  1933,   286,   262,\n",
            "         16457,   878,  2486,  1718,  2607,    13,  1406,   262,  5279,  5236,\n",
            "           286,  1693,  6282,   468,   517,   284,   466,   351, 11700,   605,\n",
            "          3034,  5087,   621,  1997,   262,  3662,   750,   393, 42547,   466,\n",
            "            13, 50256]])\n",
            "concatinated_input : Metadata: immigration gary-johnson Presidential candidate New Mexico libertarian an interview. Statement: Undocumented workers crossing the border right now is at a 12-year low.[EXP]\n",
            "full_input_length : 194\n",
            "labels : tensor([[ 9171, 14706,    25,  8098,   427,  1942,    12,  9099, 22590,  4986,\n",
            "           286, 16797,   290, 14665,  7712,   968,  1971, 43268,   281,  2720,\n",
            "           319,   366,   464,  6714,  5438,     1, 21983,    25,   632,  3484,\n",
            "           546,   720,  1821,    11,   830,   257,   614,   329,   257, 10463,\n",
            "          1048,   284,   307,   319,   262,  6483,    13, 50257,  2396,   284,\n",
            "         26871,    25,   311,   676,   318,  3376,   326,   262,  1099,   318,\n",
            "           991,   319,   262,  3835,    13,   887,   340,   338,   587,   220,\n",
            "          9514,   290,   257,  1966, 10636,  6136,   220,  2276,  1139,   339,\n",
            "          1595,   470,  1064,   340, 12765,    13,   632,   338,  1593,   284,\n",
            "          3465,   994,   326,   356,   389,   407, 14837,   257,  2742, 15593,\n",
            "           319,   220,  5108,   692,   388,   338,  4028,   475,  6974,  2282,\n",
            "          1771,   311,   676,   318,  3376,   326,   366, 31135,   220,  5133,\n",
            "          1769,  2421,  5108,   692,   388,   284,  5725,   351,  1866,   286,\n",
            "          4744,   338, 15757,   220, 22635,     1,   878, 12180,   262,  6050,\n",
            "            13,  1119,   466,    11,  3584,   612,   338,  4753,  4384,   220,\n",
            "           546,   703,   881,   262,  1099,   991,  8991,    13, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2396,   284,\n",
            "         26871,    25,   311,   676,   318,  3376,   326,   262,  1099,   318,\n",
            "           991,   319,   262,  3835,    13,   887,   340,   338,   587,   220,\n",
            "          9514,   290,   257,  1966, 10636,  6136,   220,  2276,  1139,   339,\n",
            "          1595,   470,  1064,   340, 12765,    13,   632,   338,  1593,   284,\n",
            "          3465,   994,   326,   356,   389,   407, 14837,   257,  2742, 15593,\n",
            "           319,   220,  5108,   692,   388,   338,  4028,   475,  6974,  2282,\n",
            "          1771,   311,   676,   318,  3376,   326,   366, 31135,   220,  5133,\n",
            "          1769,  2421,  5108,   692,   388,   284,  5725,   351,  1866,   286,\n",
            "          4744,   338, 15757,   220, 22635,     1,   878, 12180,   262,  6050,\n",
            "            13,  1119,   466,    11,  3584,   612,   338,  4753,  4384,   220,\n",
            "           546,   703,   881,   262,  1099,   991,  8991,    13, 50256]])\n",
            "concatinated_input : Metadata: education,labor,state-budget scott-walker Milwaukee County Executive Wisconsin republican a television interview Statement: Says he never called teachers thugs and has said nothing but great things about them during the fight over his curbs on unions[EXP]\n",
            "full_input_length : 99\n",
            "labels : tensor([[ 9171, 14706,    25,  3707,    11,  5219,    12, 37315,    11, 19290,\n",
            "           274,   629,  1252,    12, 20783, 16629,  3418, 10390,  9279, 41477,\n",
            "           257,  1705,  2650, 21983,    25, 28628,   326,   772,   611,   465,\n",
            "          4466,   318,  8197,    11,  2839,  4266,   287,   262,  3572,  1430,\n",
            "           561,   307,  1972,   546,  2063,   262,   583,    12,    79,   929,\n",
            "           346,  5153,   326,  1171,  4266,  3328, 50257,  5124, 10321,   428,\n",
            "           614,   973,   546,   720,    20,    13,   807,  1510,   286,   262,\n",
            "          5153,   720,  1238,    13,   718,  1510,   284, 32411,  9651,  1660,\n",
            "          9024,   326,  4306,   561,   423,  3220,   517,   621,   262,   513,\n",
            "            13,   718,  1411,  4391,   326,  1718,  1245,  2901,   352,    11,\n",
            "           367,  4669, 21116,   531,    13,  7793,    72, 29054,  5763,   689,\n",
            "         13388,    82,  1642,   262,  1266,   286,   262,  1964, 26688,  2156,\n",
            "           339, 19552,   422, 20131,    11,   475,   597,  1245,   262,  5466,\n",
            "           286,   262,  5638,  2097,   481,   423,   319,  1660,  3965,   318,\n",
            "          1016,   284,   307,   257,  7009,  4268,   287,   257,   845,  1588,\n",
            "         19236,    13,  7735,    11,   262,  1109,   326,   883, 15740,   481,\n",
            "          2380,   257,   530,    12,  2435,    12,  8807, 35547,   698, 34115,\n",
            "           597,  1245,   772,   517,    13, 12911,   422,   262,  5638, 34336,\n",
            "          5466,   481,   423,   645,  2694,   284, 32411,  1660,  3965,    13,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  5124, 10321,   428,\n",
            "           614,   973,   546,   720,    20,    13,   807,  1510,   286,   262,\n",
            "          5153,   720,  1238,    13,   718,  1510,   284, 32411,  9651,  1660,\n",
            "          9024,   326,  4306,   561,   423,  3220,   517,   621,   262,   513,\n",
            "            13,   718,  1411,  4391,   326,  1718,  1245,  2901,   352,    11,\n",
            "           367,  4669, 21116,   531,    13,  7793,    72, 29054,  5763,   689,\n",
            "         13388,    82,  1642,   262,  1266,   286,   262,  1964, 26688,  2156,\n",
            "           339, 19552,   422, 20131,    11,   475,   597,  1245,   262,  5466,\n",
            "           286,   262,  5638,  2097,   481,   423,   319,  1660,  3965,   318,\n",
            "          1016,   284,   307,   257,  7009,  4268,   287,   257,   845,  1588,\n",
            "         19236,    13,  7735,    11,   262,  1109,   326,   883, 15740,   481,\n",
            "          2380,   257,   530,    12,  2435,    12,  8807, 35547,   698, 34115,\n",
            "           597,  1245,   772,   517,    13, 12911,   422,   262,  5638, 34336,\n",
            "          5466,   481,   423,   645,  2694,   284, 32411,  1660,  3965,    13,\n",
            "         50256]])\n",
            "concatinated_input : Metadata: retirement,state-budget hetty-rosenstein Union leader New Jersey none a press release from the Communication Workers of America Statement: The average state pension, including managers, is $23,000 a year; and just $14,000 for local government workers.[EXP]\n",
            "full_input_length : 175\n",
            "labels : tensor([[ 9171, 14706,    25,  5057,    11,  4299,  3628,    11,    69,  2110,\n",
            "            12, 37315,  2855,    12,    77, 10151,  6045,  4744, 43268,   281,\n",
            "          2720,   319,  5694,  4744,  3000,  1511, 21983,    25,   554,   257,\n",
            "          4466,  1730,  7425,   287,  2813,   257, 12989,  5054,   286,  6630,\n",
            "          1816,   656,  1245,  3393,    11,   290,   788,   257,  2041,  5583,\n",
            "           373,   900,   510,  2644,   284,   651,  4381,   319,  1194,   720,\n",
            "            18, 12989,   286,  6630,    13, 50257,    32,  6333,  3053,  1139,\n",
            "          2486,  6149,  1936,  6215, 16651,   656, 32134, 20818,  7308,  1978,\n",
            "           287,  1946,    11, 13519,   262,   717,   640,   428,   550,   587,\n",
            "          1760,  1201,  2159,  1810,  2873,    13,   887,   262,  9851, 50063,\n",
            "          5091,   287,  2321,    11,   257,  2092, 36115,  3022,   379,   262,\n",
            "          2779,   287,  8309,    11,   290,   340,   373,  1760,   523,   326,\n",
            "         19014,  8213,   714,   307,   351,   511,  4172,   329,   262,  6786,\n",
            "          9912,  1377,   407,   329, 42079,  3840,   326, 15459,   257,  2324,\n",
            "          2526,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,    32,  6333,  3053,  1139,\n",
            "          2486,  6149,  1936,  6215, 16651,   656, 32134, 20818,  7308,  1978,\n",
            "           287,  1946,    11, 13519,   262,   717,   640,   428,   550,   587,\n",
            "          1760,  1201,  2159,  1810,  2873,    13,   887,   262,  9851, 50063,\n",
            "          5091,   287,  2321,    11,   257,  2092, 36115,  3022,   379,   262,\n",
            "          2779,   287,  8309,    11,   290,   340,   373,  1760,   523,   326,\n",
            "         19014,  8213,   714,   307,   351,   511,  4172,   329,   262,  6786,\n",
            "          9912,  1377,   407,   329, 42079,  3840,   326, 15459,   257,  2324,\n",
            "          2526,    13, 50256]])\n",
            "concatinated_input : Metadata: economy hillary-clinton Presidential candidate New York democrat remarks at the 2015 New Hampshire Democratic Party State Convention Statement: Under Republicans, recessions happen four times as frequently as under Democrats.[EXP]\n",
            "full_input_length : 127\n",
            "labels : tensor([[ 9171, 14706,    25, 14510,    11, 22554,    11, 22649,    12,  1050,\n",
            "          1063,   629,  1252,    12, 20783, 16629,  3418, 10390,  9279, 41477,\n",
            "           257,  4046, 21983,    25,  1550,  1104,   329, 28829, 33568, 50257,\n",
            "          1026, 15565,   326,  1693,  9089,   481,   307,   530,   286,   262,\n",
            "           749,  2383,  3048,   286,   262,  1099,    13,   383,  9546,  5644,\n",
            "           257,  4858,  7794,   287,  7184,    11,   475,   262,  1366,  1595,\n",
            "           470,  1104,   326,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          1026, 15565,   326,  1693,  9089,   481,   307,   530,   286,   262,\n",
            "           749,  2383,  3048,   286,   262,  1099,    13,   383,  9546,  5644,\n",
            "           257,  4858,  7794,   287,  7184,    11,   475,   262,  1366,  1595,\n",
            "           470,  1104,   326,    13, 50256]])\n",
            "concatinated_input : Metadata: corrections-and-updates,education,income,workers facebook-posts Social media posting None none a meme on social media Statement: In 1978, a student who worked a minimum-wage summer job could afford to pay a years full tuition at the 4-year public university of their choice.[EXP]\n",
            "full_input_length : 144\n",
            "labels : tensor([[ 9171, 14706,    25,  6272,   308,   560,    12, 30686,  1559, 17471,\n",
            "          4540,   968,  5828, 19466,   281,  2720,    13, 21983,    25, 13794,\n",
            "         17664,  3259, 12538,   262,  4865,   826,   783,   318,   379,   257,\n",
            "          1105,    12,  1941,  1877,    13, 50257, 34349,  8770,   531, 15031,\n",
            "          4884,   366,  3132,    11,   830, 33499,    13,  1320,   338,   379,\n",
            "          1551,   530, 27860,   583,  1048,  5556,   838,    11,   830,   329,\n",
            "          9692,    13,   383,  1748,   318,   991, 16997,   691,   319,  4979,\n",
            "          8587,    26,   612,   338,  9826,   645,  2831,   612,    13,   366,\n",
            "         34349,  8770,   373,  3729,  3376,   287,  2282,   326,   262, 27264,\n",
            "           286, 15031,  4884,   281, 42364,   453,  1029,  1271,   286, 33499,\n",
            "           290, 33785,   319,   326,  6426,   284,  1037,  1814,  1748,  1230,\n",
            "            13,  1320,   338,  4855,   416,   262,  2732,   286,  4796,   989,\n",
            "            11,   475,   465,  1271,   373,   517,   621,  4274,   262,  3376,\n",
            "          3785,    13,  2399,  2912,   546,   262,  1748, 16997,   691,   572,\n",
            "           286,  8587,   318,  5688,  8718, 45693,    13, 15031,   468,   587,\n",
            "          6464,   546,  1511,  1411,   286,   663,  6426,   422, 17176,   290,\n",
            "          1171,  3747,    11,   543,  3769,   262,  1748,   351,   720,    17,\n",
            "            13,   642,  1510,   287,  6426,    13,  1320,   338,   257,  1256,\n",
            "            11,   475,  3729,   407,   262,   691,  2723,   286,  3739,   329,\n",
            "           262,  1748,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100, 34349,  8770,   531, 15031,\n",
            "          4884,   366,  3132,    11,   830, 33499,    13,  1320,   338,   379,\n",
            "          1551,   530, 27860,   583,  1048,  5556,   838,    11,   830,   329,\n",
            "          9692,    13,   383,  1748,   318,   991, 16997,   691,   319,  4979,\n",
            "          8587,    26,   612,   338,  9826,   645,  2831,   612,    13,   366,\n",
            "         34349,  8770,   373,  3729,  3376,   287,  2282,   326,   262, 27264,\n",
            "           286, 15031,  4884,   281, 42364,   453,  1029,  1271,   286, 33499,\n",
            "           290, 33785,   319,   326,  6426,   284,  1037,  1814,  1748,  1230,\n",
            "            13,  1320,   338,  4855,   416,   262,  2732,   286,  4796,   989,\n",
            "            11,   475,   465,  1271,   373,   517,   621,  4274,   262,  3376,\n",
            "          3785,    13,  2399,  2912,   546,   262,  1748, 16997,   691,   572,\n",
            "           286,  8587,   318,  5688,  8718, 45693,    13, 15031,   468,   587,\n",
            "          6464,   546,  1511,  1411,   286,   663,  6426,   422, 17176,   290,\n",
            "          1171,  3747,    11,   543,  3769,   262,  1748,   351,   720,    17,\n",
            "            13,   642,  1510,   287,  6426,    13,  1320,   338,   257,  1256,\n",
            "            11,   475,  3729,   407,   262,   691,  2723,   286,  3739,   329,\n",
            "           262,  1748,    13, 50256]])\n",
            "concatinated_input : Metadata: candidates-biography,energy,government-regulation,voting-record gloria-romero-roses managing partner at Nexus Homes Florida democrat a mailer Statement: Says Joe Garcia voted to raise our utility rates.[EXP]\n",
            "full_input_length : 97\n",
            "labels : tensor([[ 9171, 14706,    25,  3707,    11,    75,  4820,    11,  5219,    12,\n",
            "         37315,   629,  1252,    12, 20783, 16629,  3418, 10390,  9279, 41477,\n",
            "           257,  5581,  2720, 21983,    25, 28628,   339,  1239,  1444,  7799,\n",
            "         35052,   290,   468,   531,  2147,   475,  1049,  1243,   546,   606,\n",
            "          1141,   262,  1907,   625,   465,  1090,  1443,   319, 11936, 50257,\n",
            "         26747,   531,   326,  2605,  4855,   290,  4477,   366,  1462,  1104,\n",
            "         25441,    13,   366,  1722,  7705,   286,  1181,    11,  2605,  4855,\n",
            "           290, 13722, 25441,  1088,   262,   995,    13,  1081,   257,  1584,\n",
            "          4540,    11,   607,  1104,  2058,   351,  3403,   884,   355,  1957,\n",
            "          3572,    11,  7387,  6142,  9001,   290, 12910,    13, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         26747,   531,   326,  2605,  4855,   290,  4477,   366,  1462,  1104,\n",
            "         25441,    13,   366,  1722,  7705,   286,  1181,    11,  2605,  4855,\n",
            "           290, 13722, 25441,  1088,   262,   995,    13,  1081,   257,  1584,\n",
            "          4540,    11,   607,  1104,  2058,   351,  3403,   884,   355,  1957,\n",
            "          3572,    11,  7387,  6142,  9001,   290, 12910,    13, 50256]])\n",
            "concatinated_input : Metadata: iraq barack-obama President Illinois democrat a news conference Statement: In Iraq, civilian deaths, incidents of bombings, etc., remain very low relative to what was going on last year.[EXP]\n",
            "full_input_length : 173\n",
            "labels : tensor([[ 9171, 14706,    25, 10737,    11,  5219,    12, 37315,   339, 42852,\n",
            "            12,  4951, 37975,  4479,  3554,   968,  8221,  4844,   257,  1803,\n",
            "          2650,   422,   262, 26117, 16847,   286,  2253, 21983,    25,   383,\n",
            "          2811,  1181, 13553,    11,  1390, 11663,    11,   318,   720,  1954,\n",
            "            11,   830,   257,   614,    26,   290,   655,   720,  1415,    11,\n",
            "           830,   329,  1957,  1230,  3259,    13, 50257,   464,  3517, 14115,\n",
            "           338,  7000,   991,   779,  1729,    12, 36617,  7545,   287,  1465,\n",
            "         50133,  1124,    13, 15416,   373,   319,   262,  1317,   618,   339,\n",
            "          4367,   326,   287,   262,   471,    13,   509,    13,   837, 14115,\n",
            "           338,  3544,   691,  1479,    12,  9521,  9653,    11,   981,   484,\n",
            "           389,   407,   973,   287,   262,   471,    13,   311,    13,   887,\n",
            "           618, 15416,   531, 14115,   338,   287,  4492,   318,   366, 22366,\n",
            "           588,     1,   262,   471,    13,   311,    13,   220,   530,    11,\n",
            "           339, 15565,   262,  4788,   287,  4492, 15772,   284, 13788,   517,\n",
            "         24000,  2057, 18369,    13,  2893,   262,   471,    13,   509,    13,\n",
            "           220,  4788,   423,  7428, 13463,   422,  5044,  2489,  2628,    11,\n",
            "          6154,   910,   340,   338,   257,  7539,   284,   910,   484, 15772,\n",
            "           284,  1365, 16633,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,   464,  3517, 14115,\n",
            "           338,  7000,   991,   779,  1729,    12, 36617,  7545,   287,  1465,\n",
            "         50133,  1124,    13, 15416,   373,   319,   262,  1317,   618,   339,\n",
            "          4367,   326,   287,   262,   471,    13,   509,    13,   837, 14115,\n",
            "           338,  3544,   691,  1479,    12,  9521,  9653,    11,   981,   484,\n",
            "           389,   407,   973,   287,   262,   471,    13,   311,    13,   887,\n",
            "           618, 15416,   531, 14115,   338,   287,  4492,   318,   366, 22366,\n",
            "           588,     1,   262,   471,    13,   311,    13,   220,   530,    11,\n",
            "           339, 15565,   262,  4788,   287,  4492, 15772,   284, 13788,   517,\n",
            "         24000,  2057, 18369,    13,  2893,   262,   471,    13,   509,    13,\n",
            "           220,  4788,   423,  7428, 13463,   422,  5044,  2489,  2628,    11,\n",
            "          6154,   910,   340,   338,   257,  7539,   284,   910,   484, 15772,\n",
            "           284,  1365, 16633,    13, 50256]])\n",
            "concatinated_input : Metadata: economy,jobs robin-vos Wisconsin Assembly speaker Wisconsin republican a television interview Statement: Right-to-work states have higher rates of income growth.[EXP]\n",
            "full_input_length : 103\n",
            "labels : tensor([[ 9171, 14706,    25,  3773, 12788,   560,    12, 37821, 17471,  4540,\n",
            "           968,  1971, 43268, 10252,   379,   262,  1853,   968, 13910,  4390,\n",
            "          3615,  1812, 11680, 21983,    25,  4698,  4734,    11,   664,  6202,\n",
            "          1645,  1440,  1661,   355,  6777,   355,   739,  4956,    13, 50257,\n",
            "           818,  1109,    11,   612,   318,   645,  9815,  1366,   319, 21571,\n",
            "           290,  4268,   448,  3965,   422,  2839,  4266,    11,   543,  1848,\n",
            "           329,   546,   807,   284,   860,  1411,   286,  1029,  1524,  2444,\n",
            "            13, 12168,   286,   674, 13905,  6154,   531,   326,   340,   338,\n",
            "          2219,   329,  7602,  8347,   284, 14267,   262, 14601,   618,   875,\n",
            "         14992,  1605,  4268,   448,  3965,    13,   775,  1833,   262,   761,\n",
            "           329, 21654,   287,   257, 30681,  2720,    11,   475, 18625,   714,\n",
            "           423,  3538,   531,   262,  4268,   448,  2494,   366, 11261,   307,\n",
            "           355,  1029,   355,  1679,  1411,    13, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "           818,  1109,    11,   612,   318,   645,  9815,  1366,   319, 21571,\n",
            "           290,  4268,   448,  3965,   422,  2839,  4266,    11,   543,  1848,\n",
            "           329,   546,   807,   284,   860,  1411,   286,  1029,  1524,  2444,\n",
            "            13, 12168,   286,   674, 13905,  6154,   531,   326,   340,   338,\n",
            "          2219,   329,  7602,  8347,   284, 14267,   262, 14601,   618,   875,\n",
            "         14992,  1605,  4268,   448,  3965,    13,   775,  1833,   262,   761,\n",
            "           329, 21654,   287,   257, 30681,  2720,    11,   475, 18625,   714,\n",
            "           423,  3538,   531,   262,  4268,   448,  2494,   366, 11261,   307,\n",
            "           355,  1029,   355,  1679,  1411,    13, 50256]])\n",
            "concatinated_input : Metadata: health-care bernie-s U.S. Senator Vermont independent the Dec. 19 Democratic presidential debate Statement: The United States spends almost three times per capita what they spend in the U.K. on health care and 50 percent more than they pay in France.[EXP]\n",
            "full_input_length : 160\n",
            "labels : tensor([[ 9171, 14706,    25, 26251,    12,   392,    12,   929, 19581,    11,\n",
            "         40796,    11, 12519,    11, 22896, 23960,    12, 24875,  5483,  2056,\n",
            "         10754,  6045,  4844,   257, 25336,   319,  1919,  2056, 21983,    25,\n",
            "           554, 15524,    11,   257,  3710,   508,  3111,   257,  5288,    12,\n",
            "         21482,  3931,  1693,   714,  5368,   284,  1414,   257,   812,  1336,\n",
            "         18385,   379,   262,   604,    12,  1941,  1171,  6403,   286,   511,\n",
            "          3572,    13, 50257, 34487, 33209,  1271,   857,  1656,   287,   262,\n",
            "         38260,   989,   339, 10288,   287,   262,  1034,    12,   276,    11,\n",
            "           475,   663,   407,   262,  4165,  8636,  1377,   663,   530,   286,\n",
            "          1440,  5559, 13858,    11,   290,  3538,   262,   530,   351,   262,\n",
            "          4094, 10070,    13,  1406,   262,  1271,   318, 23612,    12, 41891,\n",
            "            13,   843,   339, 29801, 24245,   262,  7746,   326,   860,  1510,\n",
            "           661,   508,  1422,   470,   423,  9749,  5197,   481,   651,   340,\n",
            "           780,   286,   262,  1099,    11,   379,  1551,  1864,   284, 38260,\n",
            "            82,  7746,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100, 34487, 33209,  1271,   857,  1656,   287,   262,\n",
            "         38260,   989,   339, 10288,   287,   262,  1034,    12,   276,    11,\n",
            "           475,   663,   407,   262,  4165,  8636,  1377,   663,   530,   286,\n",
            "          1440,  5559, 13858,    11,   290,  3538,   262,   530,   351,   262,\n",
            "          4094, 10070,    13,  1406,   262,  1271,   318, 23612,    12, 41891,\n",
            "            13,   843,   339, 29801, 24245,   262,  7746,   326,   860,  1510,\n",
            "           661,   508,  1422,   470,   423,  9749,  5197,   481,   651,   340,\n",
            "           780,   286,   262,  1099,    11,   379,  1551,  1864,   284, 38260,\n",
            "            82,  7746,    13, 50256]])\n",
            "concatinated_input : Metadata: foreign-policy,public-health tim-ziemer None None none an article posted on Medium Statement: In Africa, a child dies every minute because of (malaria).[EXP]\n",
            "full_input_length : 75\n",
            "labels : tensor([[ 9171, 14706,    25,  5871,    12,  8482,  4867,    11, 22554,    11,\n",
            "         14480,    12, 27727,    11,    85, 10720,    12, 22105,  1278,  7661,\n",
            "            12,   398,  3529,    12,  4951,   274, 11149,  5212,   379, 16756,\n",
            "         34698,  4744, 43268,   257,  6920,   263, 21983,    25, 28628,  5689,\n",
            "         18555,  7052,   284,  5298,   674, 10361,  3965,    13, 50257,    52,\n",
            "            13,   311,    13,   220,  2311,    13,   220,  3619, 16105,    11,\n",
            "           360,    12,    49,    13,   314,    13,   837,   531,    11,   286,\n",
            "          2258,  4969,    25,   366,  2990,   423,   257,  8904,  5055,  5428,\n",
            "            11,   530,   286,   262,  4387,    11,  3729,   262,  4387,   583,\n",
            "         21344,    11,   287,   262,   995,    13, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,    52,\n",
            "            13,   311,    13,   220,  2311,    13,   220,  3619, 16105,    11,\n",
            "           360,    12,    49,    13,   314,    13,   837,   531,    11,   286,\n",
            "          2258,  4969,    25,   366,  2990,   423,   257,  8904,  5055,  5428,\n",
            "            11,   530,   286,   262,  4387,    11,  3729,   262,  4387,   583,\n",
            "         21344,    11,   287,   262,   995,    13, 50256]])\n",
            "concatinated_input : Metadata: health-care,pundits bob-beckel Co-host on Fox News Channel's \"The Five\" Washington, D.C. democrat Sean Hannity's show on Fox News Statement: There were more people at the Air and Space Museum than at a rally against the health care bills.[EXP]\n",
            "full_input_length : 103\n",
            "labels : tensor([[ 9171, 14706,    25,  4173, 30188,  2318,   441,    12,   672,  1689,\n",
            "          1992,  9486, 43268,   257,  1705,  4495, 21983,    25,   554,  3908,\n",
            "            11, 11107,  7040,    11, 10207,   286, 26579,    11,  3503,  1539,\n",
            "          3520,   845,  1877,  3585,   284,   644,   373,  1016,   319,   938,\n",
            "           614,    13, 50257, 28100,  4250,   287,  2661,   286,   465,  5150,\n",
            "          3958,   319,  7045,  8218,   262,  1499,    11,  1301,   531,   326,\n",
            "           257, 21805,  4992, 12481,   550,  1043,   366,  2079,  1411,   286,\n",
            "           262,   661,   287,  8037,     1,  1104, 38627,  1099,    13, 21805,\n",
            "           531,   833,  8142, 22627,   373,   366, 11664,   306,  7187,   553,\n",
            "         10820,   326,   262,  9546,   366, 10365,  3314,     1,   373,   833,\n",
            "          8142,   898,    13,   775,   750,   407,  1011,   257, 12046,   319,\n",
            "           465,   779,   286,   262, 43441,    13,  1318,   318,   617, 47128,\n",
            "           287,   262,  8037,  2482,    11,  2158,    11,   355,  7045, 20840,\n",
            "           423,  1180, 22582,   319,  2972,  7612,   286, 38627,  1099,   290,\n",
            "           703,   340, 10783,   284,   307,  9177,    13,   843,  3016,  2319,\n",
            "          1411,   286, 14502,   287,  8037,   531,   326, 38627,  1099,   815,\n",
            "           691,  4174,   284,   584,  7045,    11,   407,  1729,    12, 17067,\n",
            "          4290,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100, 28100,  4250,   287,  2661,   286,   465,  5150,\n",
            "          3958,   319,  7045,  8218,   262,  1499,    11,  1301,   531,   326,\n",
            "           257, 21805,  4992, 12481,   550,  1043,   366,  2079,  1411,   286,\n",
            "           262,   661,   287,  8037,     1,  1104, 38627,  1099,    13, 21805,\n",
            "           531,   833,  8142, 22627,   373,   366, 11664,   306,  7187,   553,\n",
            "         10820,   326,   262,  9546,   366, 10365,  3314,     1,   373,   833,\n",
            "          8142,   898,    13,   775,   750,   407,  1011,   257, 12046,   319,\n",
            "           465,   779,   286,   262, 43441,    13,  1318,   318,   617, 47128,\n",
            "           287,   262,  8037,  2482,    11,  2158,    11,   355,  7045, 20840,\n",
            "           423,  1180, 22582,   319,  2972,  7612,   286, 38627,  1099,   290,\n",
            "           703,   340, 10783,   284,   307,  9177,    13,   843,  3016,  2319,\n",
            "          1411,   286, 14502,   287,  8037,   531,   326, 38627,  1099,   815,\n",
            "           691,  4174,   284,   584,  7045,    11,   407,  1729,    12, 17067,\n",
            "          4290,    13, 50256]])\n",
            "concatinated_input : Metadata: energy,environment,oil-spill alex-sink None Florida democrat remarks to reporters. Statement: House Republicans who complained they didnt have enough time to consider a constitutional ban against oil drilling shoved through a proposal in just a few days to open state waters to oil drilling.[EXP]\n",
            "full_input_length : 140\n",
            "labels : tensor([[ 9171, 14706,    25,  3773,    11, 43863,  3857,   259,    12,    85,\n",
            "           418,  9279, 10006, 10834,  9279, 41477,   257,  5581,  2720, 21983,\n",
            "            25,  6498,    12,  1462,    12,  1818,  2585,   423,  2440,  3965,\n",
            "           286,  3739,  3349,    13, 50257,   464,  2050,   290,   663,  1772,\n",
            "          2897,  6409,   290,  2726,  1275,  3508,   326,   787,  1598,   262,\n",
            "          4317,  1411,  3785,  1377,   981, 42789,   286,   257,  1688,  4268,\n",
            "          1377,   318,   407,   257, 17347,  3785,    13,   843,   262,  2050,\n",
            "          1838,  1598,   326,   407,   477,  3777, 23246,   416,  4773,   389,\n",
            "            11,   355,   376,  6213,  5907,  2643, 17142,    11,   973,   366,\n",
            "           259,  6741,    13,   366,  9627,   389,  1593,  3307,   326,   547,\n",
            "          4814,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,   464,  2050,   290,   663,  1772,\n",
            "          2897,  6409,   290,  2726,  1275,  3508,   326,   787,  1598,   262,\n",
            "          4317,  1411,  3785,  1377,   981, 42789,   286,   257,  1688,  4268,\n",
            "          1377,   318,   407,   257, 17347,  3785,    13,   843,   262,  2050,\n",
            "          1838,  1598,   326,   407,   477,  3777, 23246,   416,  4773,   389,\n",
            "            11,   355,   376,  6213,  5907,  2643, 17142,    11,   973,   366,\n",
            "           259,  6741,    13,   366,  9627,   389,  1593,  3307,   326,   547,\n",
            "          4814,    13, 50256]])\n",
            "concatinated_input : Metadata: health-care americas-health-insurance-plans Trade group representing the health insurance industry Washington, D.C. none CNN's Lou Dobbs Tonight Statement: Every survey shows strong satisfaction with private coverage.[EXP]\n",
            "full_input_length : 131\n",
            "labels : tensor([[ 9171, 14706,    25,  1535,    12,  6651,   275,  1142,   494,    12,\n",
            "            82,   471,    13,    50,    13,  8962, 16033,  4795,   262,  4280,\n",
            "            13,   678,  4390,  4787,  4384, 21983,    25,   383,  1578,  1829,\n",
            "         16887,  2048,  1115,  1661,   583, 21344,   644,   484,  4341,   287,\n",
            "           262,   471,    13,    42,    13,   319,  1535,  1337,   290,  2026,\n",
            "          1411,   517,   621,   484,  1414,   287,  4881,    13, 50257,  1537,\n",
            "           326,  4384,   318,  3675,   262,  8354,   286,   428,  2708,    13,\n",
            "          1550,   262,  2176,  1808, 28370,  4376,  1377,  1771,  6809,   287,\n",
            "          5027,  4223,   261,   481,   366,  1186,   557,  1642,   379,  1551,\n",
            "          2026,  1411,   517,   621,   484,   561,  1683,   651,   503,   286,\n",
            "          5483,  4765,     1,  1377,   262,  3280,   318,    11,   366,   270,\n",
            "          8338,    13,   366,  4821,   284,  3640,  3199,   257,  8667,   812,\n",
            "          2084,    11,   617,   481,    11,   290,   617, 28329,    13,   843,\n",
            "           262, 19360,  1909,   329,   262,  5027,  4223,   261,  3352,  2494,\n",
            "           286,  1441,  1377,   981,   407, 40139,  1016,  2651,  1377,   318,\n",
            "           517,   866, 12945,   621,   340,   373,   287,  7358,    13, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1537,\n",
            "           326,  4384,   318,  3675,   262,  8354,   286,   428,  2708,    13,\n",
            "          1550,   262,  2176,  1808, 28370,  4376,  1377,  1771,  6809,   287,\n",
            "          5027,  4223,   261,   481,   366,  1186,   557,  1642,   379,  1551,\n",
            "          2026,  1411,   517,   621,   484,   561,  1683,   651,   503,   286,\n",
            "          5483,  4765,     1,  1377,   262,  3280,   318,    11,   366,   270,\n",
            "          8338,    13,   366,  4821,   284,  3640,  3199,   257,  8667,   812,\n",
            "          2084,    11,   617,   481,    11,   290,   617, 28329,    13,   843,\n",
            "           262, 19360,  1909,   329,   262,  5027,  4223,   261,  3352,  2494,\n",
            "           286,  1441,  1377,   981,   407, 40139,  1016,  2651,  1377,   318,\n",
            "           517,   866, 12945,   621,   340,   373,   287,  7358,    13, 50256]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "`.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-272-d4d5f8807686>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     train_and_test_CEG(\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mmodel_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpt2_small_config1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mtrain_and_test_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_and_test_config_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-242-7137d5856615>\u001b[0m in \u001b[0;36mtrain_and_test_CEG\u001b[0;34m(model_config, train_and_test_config, batch_size, base_time, set_type)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_result_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         return call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m         ckpt_path = self._checkpoint_connector._select_ckpt_path(\n\u001b[0m\u001b[1;32m    792\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_provided\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\u001b[0m in \u001b[0;36m_select_ckpt_path\u001b[0;34m(self, state_fn, ckpt_path, model_provided, model_connected)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             ckpt_path = self._parse_ckpt_path(\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0mstate_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\u001b[0m in \u001b[0;36m_parse_ckpt_path\u001b[0;34m(self, state_fn, ckpt_path, model_provided, model_connected)\u001b[0m\n\u001b[1;32m    169\u001b[0m                         \u001b[0;34mf\" Please pass an exact checkpoint path to `.{fn}(ckpt_path=...)`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                     )\n\u001b[0;32m--> 171\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    172\u001b[0m                     \u001b[0;34mf'`.{fn}(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: `.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# statementのみ\n",
        "# GPT2-small\n",
        "base_time = '2024-01-24 13:17 s only'\n",
        "\n",
        "gpt2_small_config2 = {\n",
        "    'MODEL_NAME' : 'gpt2',\n",
        "    'TOKENIZER_NAME' : 'gpt2',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/small',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'small'],\n",
        "    'INPUT_FLIP' : 0\n",
        "\n",
        "}\n",
        "\n",
        "# GPT2-medium\n",
        "gpt2_medium_config2 = {\n",
        "    'MODEL_NAME' : 'gpt2-medium',\n",
        "    'TOKENIZER_NAME' : 'gpt2-medium',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/medium',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'medium'],\n",
        "    'INPUT_FLIP' : 0\n",
        "}\n",
        "\n",
        "# gpt2-small\n",
        "# true\n",
        "for batch_size in [1, 2]:\n",
        "    train_and_test_CEG(\n",
        "        model_config=gpt2_small_config2,\n",
        "        train_and_test_config=train_and_test_config_true,\n",
        "        batch_size=batch_size,\n",
        "        base_time=base_time,\n",
        "        set_type='true')\n",
        "# # false\n",
        "for batch_size in [1, 2]:\n",
        "    train_and_test_CEG(\n",
        "        model_config=gpt2_small_config2,\n",
        "        train_and_test_config=train_and_test_config_false,\n",
        "        batch_size=batch_size,\n",
        "        base_time=base_time,\n",
        "        set_type='false')\n",
        "\n",
        "# gpt2-medium\n",
        "# # # true\n",
        "for batch_size in [1, 2]:\n",
        "    train_and_test_CEG(\n",
        "        model_config=gpt2_medium_config2,\n",
        "        train_and_test_config=train_and_test_config_true,\n",
        "        batch_size=batch_size,\n",
        "        base_time=base_time,\n",
        "        set_type='true')\n",
        "# # false\n",
        "for batch_size in [1, 2]:\n",
        "    train_and_test_CEG(\n",
        "        model_config=gpt2_medium_config2,\n",
        "        train_and_test_config=train_and_test_config_false,\n",
        "        batch_size=batch_size,\n",
        "        base_time=base_time,\n",
        "        set_type='false')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0b5b875c76fd400c9d78e1aff0732e5f",
            "e288f2c151ca419d9cf4198d1d4849e2",
            "713a07cdc7644e4a8ea6677e2f218632",
            "4a3cf79ff8aa4bdeaf205c5275dd3304",
            "c7a1094731594f0f871ad1ffe5833189",
            "76851d531d1640f39ac092513421f84a",
            "9473b58e0e2548c296576bfe6d5e450c",
            "07922ef591cd4edab36b910915ada873",
            "56c6af83517047abae2175f9fb8f2aed",
            "42a79fbc356d466990485fd812e55920",
            "0b608772164d46bfbbca458c06dab360",
            "3477e8a0433a4d289206f0ad98611c4d",
            "56657abfb5cc44eb85d9d74f6ece77de",
            "d4709f5f2c2346a493f8c37ebbbe191b",
            "b88efb638a664133922996911e38dd3d",
            "27f7399f836046e694e5f4c5517b2ed2",
            "8c897186058e4367b5c149c537752283",
            "c8e0ef82fb634235a8b276a6950b5c43",
            "d80f8dd02f274b8d86534c2a994bdf20",
            "30c49dd30213446eba7029e2fca4068e",
            "aaf9ff85e31448d88922a33deb019502",
            "a8de57c923134ae3a98897fc5c2fa084"
          ]
        },
        "id": "onVArZXnoOtX",
        "outputId": "a3f1e12d-0430-41f6-ef24-004b6431b8a8"
      },
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- training and testing has just started (batch_size : 1) -----------\n",
            "total_training_steps : 11504 , warmup_steps : 2300 \n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : gpt2\n",
            "self.tokenizer : GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['[EXP]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t50257: AddedToken(\"[EXP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : 50258\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : <|endoftext|>\n",
            "self.eos_token_id : 50256\n",
            "self.pad_token : <|endoftext|>\n",
            "self.pad_token_id : 50256\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : 50257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: lightning_logs/2024-01-24 13:17 s only/gpt2/small/batch_size:1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- model information ----------\n",
            "self.MODEL_NAME : gpt2\n",
            "dirpath : /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/src/models/gpt2/small/2024-01-24 13:17 s only/true_batch_size=1\n",
            "checkpoint_callback : <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7ff2e348dde0>\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : gpt2\n",
            "self.tokenizer : GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['[EXP]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t50257: AddedToken(\"[EXP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : 50258\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : <|endoftext|>\n",
            "self.eos_token_id : 50256\n",
            "self.pad_token : <|endoftext|>\n",
            "self.pad_token_id : 50256\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : 50257\n",
            "train_dataset size : 5752\n",
            "\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : gpt2\n",
            "self.tokenizer : GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['[EXP]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t50257: AddedToken(\"[EXP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : 50258\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : <|endoftext|>\n",
            "self.eos_token_id : 50256\n",
            "self.pad_token : <|endoftext|>\n",
            "self.pad_token_id : 50256\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : 50257\n",
            "test_dataset size : 714\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | GPT2LMHeadModel  | 163 M \n",
            "1 | softmax   | Softmax          | 0     \n",
            "2 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "163 M     Trainable params\n",
            "0         Non-trainable params\n",
            "163 M     Total params\n",
            "652.155   Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : gpt2\n",
            "self.tokenizer : GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['[EXP]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t50257: AddedToken(\"[EXP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : 50258\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : <|endoftext|>\n",
            "self.eos_token_id : 50256\n",
            "self.pad_token : <|endoftext|>\n",
            "self.pad_token_id : 50256\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : 50257\n",
            "val_dataset size : 668\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b5b875c76fd400c9d78e1aff0732e5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "concatinated_input : Statement: Says nearly half of Oregons children are poor.[EXP]\n",
            "full_input_length : 118\n",
            "concatinated_input : Statement: On attacks by Republicans that various programs in the economic stimulus plan are not stimulative, \"If you add all that stuff up, it accounts for less than 1 percent of the overall package.\"[EXP]\n",
            "full_input_length : 111concatinated_input : Statement: Worldwide credit card transactions, the credit card fraud rate is 0.04 percent, compared to almost 8 percent, 9 percent, 10 percent of Medicare fraud.[EXP]\n",
            "\n",
            "full_input_length : 138\n",
            "concatinated_input : Statement: Says Tennessee is providing millions of dollars to virtual school company for results at the bottom of the bottom.[EXP]\n",
            "full_input_length : 174concatinated_input : Statement: In the month of January, Canada created more new jobs than we did.[EXP]\n",
            "\n",
            "full_input_length : 137\n",
            "concatinated_input : Statement: Says Donald Trump started his career back in 1973 being sued by the Justice Department for racial discrimination because he would not rent apartments in one of his developments to African-Americans.[EXP]concatinated_input : Statement: Under his leadership, more people in Wisconsin have access to health care.[EXP]\n",
            "\n",
            "full_input_length : 154\n",
            "full_input_length : 153\n",
            "concatinated_input : Statement: Bill White has a long history of trying to limit or even disenfranchise military voters.[EXP]\n",
            "concatinated_input : Statement: Says Donald Trumps only economic agenda is imposing massive taxes on the American people with a 40 percent tax hike of a giant tariff.[EXP]full_input_length : 126\n",
            "\n",
            "full_input_length : 87\n",
            "concatinated_input : Statement: The loan guarantee program that helped Solyndra was a program that was supported by President Bush.[EXP]concatinated_input : Statement: John McCains chief economic adviser during the 08 race estimated that Trumps promises would cause America to lose 3.5 million jobs.[EXP]\n",
            "\n",
            "full_input_length : 121full_input_length : 200\n",
            "\n",
            "concatinated_input : Statement: Bill McCollumhas \"recovered $200 million in Medicaid fraud.\"[EXP]concatinated_input : Statement: State revenue projections have missed the mark month after month.[EXP]\n",
            "\n",
            "full_input_length : 155\n",
            "full_input_length : 85\n",
            "concatinated_input : Statement: Over 3 million Americans are employed in the growing green-collar workforce, which is more than the number of people working in the fossil fuel industry.[EXP]concatinated_input : Statement: The median income of a middle class family went down $2,100 from 2001 to 2007.[EXP]\n",
            "\n",
            "full_input_length : 110full_input_length : 121\n",
            "\n",
            "concatinated_input : Statement: Because of the steps we took, there are about 2 million Americans working right now who would otherwise be unemployed.[EXP]concatinated_input : Statement: Rick Perry has advocated abandoning Social Security, scuttling Medicaid and ending the federal income tax.[EXP]\n",
            "\n",
            "full_input_length : 109full_input_length : 102\n",
            "\n",
            "concatinated_input : Statement: Says that In 2009, I saved ratepayers around $500 million by persuading the Council to pursue a less expensive compliance mechanism if the City is required to treat Bull Run drinking water.[EXP]concatinated_input : Statement: Two thirds to three quarters of people without [health] insurance in Rhode Island work.[EXP]\n",
            "\n",
            "full_input_length : 90full_input_length : 114\n",
            "\n",
            "concatinated_input : Statement: The military has spent $500 million enforcing the Dont Ask, Dont Tell policy regarding gays and lesbians in the military.[EXP]concatinated_input : Statement: Congress has spent 66 of the first 100 days of this term in recess.[EXP]\n",
            "\n",
            "full_input_length : 86full_input_length : 129\n",
            "\n",
            "concatinated_input : Statement: If you dont buy cigarettes at your local supermarket, your grocery bill wont go up a dime. The same is true of the sugary drink tax. If passed, you can avoid paying the tax by not buying sugary drinks.[EXP]concatinated_input : Statement: Were. . . keeping and creating jobs in our state. From American Greetings, to Wendys, to Diebold, weve gone to their doorsteps to keep jobs right here in Ohio.[EXP]\n",
            "\n",
            "full_input_length : 161full_input_length : 407\n",
            "\n",
            "concatinated_input : Statement: Georgia has had ʺmore bank failures than any other state.ʺ[EXP]concatinated_input : Statement: From 2003 to 2006, Sesame Street made more than $211 million from toy and consumer product sales.[EXP]\n",
            "\n",
            "full_input_length : 78full_input_length : 147\n",
            "\n",
            "concatinated_input : Statement: (John) Kasich was the architect who balanced the budget, cut spending, created a surplus, igniting record job creation.[EXP]concatinated_input : Statement: Thom Tillis cut almost $500 million from education.[EXP]\n",
            "full_input_length : 74\n",
            "\n",
            "full_input_length : 122\n",
            "concatinated_input : Statement: Says that President Obama said in 2008 that his proposed greenhouse gas regulations will bankrupt anyone who wants to build a new coal-fired power plant.[EXP]\n",
            "full_input_length : 143concatinated_input : Statement: We are poised to get rid of over 1,000 more regulations in 2012.[EXP]\n",
            "\n",
            "full_input_length : 100\n",
            "concatinated_input : Statement: There has been $5 trillion in debt added over the last four years.[EXP]\n",
            "full_input_length : 74\n",
            "concatinated_input : Statement: Administrative employees at colleges and universitieshave more than doubled over the last 25 years, outpacing the growth of students by more than 2 to 1.[EXP]\n",
            "concatinated_input : Statement: Says that under Gov. Rick Perry, Texas Department of Public Safety troopers have had standing orders not to inquire into the immigration status of people unless theyre under arrest.[EXP]full_input_length : 99\n",
            "\n",
            "full_input_length : 79\n",
            "concatinated_input : Statement: The cost of the food stamp program is at an all-time high.[EXP]\n",
            "concatinated_input : Statement: Democrats already agreed to a deal that Republicans wanted, that Eric Cantor said would be a win.[EXP]full_input_length : 45\n",
            "\n",
            "full_input_length : 187\n",
            "concatinated_input : Statement: Says opponent Mary Burke says she supports Obamacare unequivocally and wants to expand it.[EXP]concatinated_input : Statement: Eight of the nine justices in the Supreme Court decision (on campaign finance) said that not only is it constitutional for Congress to require disclosure of the special interest money, but they recommend we do it.[EXP]\n",
            "\n",
            "full_input_length : 54full_input_length : 135\n",
            "\n",
            "concatinated_input : Statement: Says Essex County residents suffer the second highest property taxes in the nation.[EXP]\n",
            "concatinated_input : Statement: The reality is, we are holding some of the most dangerous terrorists in the world right now in our federal prisons, including the mastermind of the 1993 World Trade Center bombing, the 'shoe bomber,' the 'Unabomber,' and many others.[EXP]full_input_length : 35\n",
            "\n",
            "full_input_length : 169\n",
            "concatinated_input : Statement: Ken Lanci is a lifelong Clevelander[EXP]concatinated_input : Statement: Under the new and little known 'global justice' initiative, the Obama administration has ordered FBI agents to \"read Miranda Rights to high-value terrorist detainees captured on the battlefield.\"[EXP]\n",
            "\n",
            "full_input_length : 70full_input_length : 75\n",
            "\n",
            "concatinated_input : Statement: Says Wisconsin Gov. Scott Walker slashed pensions and benefits for public employees.[EXP]concatinated_input : Statement: Wholly domestic communication between you and your wife can go to New York to London and back and get caught up in the (NSA) database.[EXP]\n",
            "full_input_length : 163\n",
            "\n",
            "full_input_length : 102\n",
            "concatinated_input : Statement: Says for every dollar the state spent on audits last year, it delivered $64 in cost savings.[EXP]\n",
            "full_input_length : 83\n",
            "concatinated_input : Statement: When I was governor, not only did test scores improve we also narrowed the achievement gap.[EXP]\n",
            "full_input_length : 155\n",
            "concatinated_input : Statement: Adding ethanol to gas raises food costs.[EXP]\n",
            "concatinated_input : Statement: Georgia is nearly 50 percent Democratic and (the Republican majority) diminished our voting strength to 32 percent through gerrymandered maps.[EXP]\n",
            "full_input_length : 232full_input_length : 79\n",
            "concatinated_input : Statement: You can import as many hemp products into this country as you want...but we cant grow it.[EXP]\n",
            "\n",
            "full_input_length : 83\n",
            "concatinated_input : Statement: Says a Republican hasnt won [an election] for a presidency in New Jersey since 1988.[EXP]concatinated_input : Statement: Mitt Romney has proposed cutting his own taxes while raising them on 18 million working families.[EXP]\n",
            "\n",
            "full_input_length : 101full_input_length : 140\n",
            "\n",
            "concatinated_input : Statement: 40 percent of illegal immigrants had a visa and then became illegal, mostly because they changed jobs.[EXP]\n",
            "full_input_length : 77concatinated_input : Statement: Says Pennsylvania charges a top income tax rate of 3 percent and Delaware has no state income tax at all.[EXP]\n",
            "\n",
            "full_input_length : 55\n",
            "concatinated_input : Statement: Says Ted Cruz slurred Republican senators including John Cornyn as graybeards and spineless jellyfish.[EXP]\n",
            "concatinated_input : Statement: Jeff Greene can buy anything ... he owns two mansions.[EXP]full_input_length : 102\n",
            "full_input_length : 98\n",
            "\n",
            "concatinated_input : Statement: Since we last debated in Las Vegas, nearly 3,000 people have been killed by guns.[EXP]\n",
            "full_input_length : 158concatinated_input : Statement: Despite having their budget increased by over 40 percent since 2009 pending claims for benefits with the (Department of Veterans Affairs) have increased from 391,000 to 890,000 under the Obama Administration.[EXP]\n",
            "\n",
            "full_input_length : 98\n",
            "concatinated_input : Statement: 94 percent of winning candidates in 2010 had more money than their opponents.[EXP]concatinated_input : Statement: I am going to be on the ballot in all 50 states. There is no other third-party candidate thats going to come close to achieving that.[EXP]\n",
            "\n",
            "full_input_length : 161full_input_length : 82\n",
            "\n",
            "concatinated_input : Statement: Says the federal agency in charge of Medicare and Medicaid will disburse $803 billion in benefits this year, making it larger than all but 15 of the worlds economies.[EXP]concatinated_input : Statement: After World War II, we tried, convicted, and, in some cases, executed Japanese soldiers for war crimes that included charges of waterboarding.[EXP]\n",
            "\n",
            "full_input_length : 94full_input_length : 148\n",
            "\n",
            "concatinated_input : Statement: Marco Rubio \"supported $800,000 for AstroTurf for a field where he played flag football.\"[EXP]\n",
            "concatinated_input : Statement: Ken Bucks (District Attorneys) office? His spending skyrocketed by 40 percent.[EXP]\n",
            "labels : tensor([[48682,    25, 28628,  ..., 50256, 50256, 50256],\n",
            "        [48682,    25,  1550,  ..., 50256, 50256, 50256],\n",
            "        [48682,    25, 28628,  ..., 50256, 50256, 50256],\n",
            "        ...,\n",
            "        [48682,    25,  4323,  ..., 50256, 50256, 50256],\n",
            "        [48682,    25,   775,  ..., 50256, 50256, 50256],\n",
            "        [48682,    25, 30048,  ..., 50256, 50256, 50256]])\n",
            "labels : tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
            "        [-100, -100, -100,  ..., -100, -100, -100],\n",
            "        [-100, -100, -100,  ..., -100, -100, -100],\n",
            "        ...,\n",
            "        [-100, -100, -100,  ..., -100, -100, -100],\n",
            "        [-100, -100, -100,  ..., -100, -100, -100],\n",
            "        [-100, -100, -100,  ..., -100, -100, -100]])\n",
            "full_input_length : 99\n",
            "full_input_length : 90\n",
            "concatinated_input : Statement: The dollar has fallen over 44% since Bernanke began pumping money into the system beginning back in 2002.[EXP]\n",
            "concatinated_input : Statement: McCain \"said he was 'stumped' when asked whether contraceptives help stop the spread of HIV.\"[EXP]\n",
            "full_input_length : 139full_input_length : 116\n",
            "\n",
            "concatinated_input : Statement: Congress will begin its recess without having allocated one penny to fight Zika.[EXP]\n",
            "full_input_length : 115\n",
            "concatinated_input : Statement: On the propriety of budget reconciliation.[EXP]\n",
            "full_input_length : 95\n",
            "concatinated_input : Statement: Says legislation pending in the House would effectively limit or eliminate time-and-a-half for people who work overtime.[EXP]\n",
            "full_input_length : 100\n",
            "concatinated_input : Statement: The cost for renovating the headquarters of the U.N. has doubled from the original estimate.[EXP]\n",
            "full_input_length : 143\n",
            "concatinated_input : Statement: Mitt Romney is proposing a tax plan that would give millionaires another tax break and raises taxes on middle class families by up to $2,000 a year.[EXP]\n",
            "full_input_length : 229\n",
            "concatinated_input : Statement: There have been three people tried and convicted by the last administration in military courts. Two are walking the street right now.[EXP]\n",
            "full_input_length : 146\n",
            "concatinated_input : Statement: The NCAA will get billions from the mens basketball tournament. Players get a trophy.[EXP]\n",
            "full_input_length : 186\n",
            "concatinated_input : Statement: The budget currently being debated significantly decreases the use of one-time resources.[EXP]\n",
            "full_input_length : 143\n",
            "concatinated_input : Statement: Americans work way more than an average of industrialized countries around the world.[EXP]\n",
            "full_input_length : 111\n",
            "concatinated_input : Statement: Says Ted Strickland in this campaign bragged about his A-plus rating with the NRA. ... He has said he has a mixed and spotty record on this issue and that he can be criticized for it. Those are his words, not mine. So I dont know where he is on this issue.[EXP]\n",
            "full_input_length : 132\n",
            "concatinated_input : Statement: Says new Medicare billing guidelines have nine codes for (injuries by) turkeys.[EXP]\n",
            "full_input_length : 93\n",
            "concatinated_input : Statement: One man opposed a flawed strategy in Iraq. One man had the courage to call for change. One man didn't play politics with the truth.[EXP]\n",
            "full_input_length : 76\n",
            "concatinated_input : Statement: I never had a filibuster-proof Senate.[EXP]\n",
            "full_input_length : 100\n",
            "concatinated_input : Statement: Says 100,000 are on waiting list to attend Texas charter schools.[EXP]\n",
            "full_input_length : 166\n",
            "concatinated_input : Statement: The new Ukrainian government introduced a law abolishing the use of languages other than Ukrainian in official circumstances.[EXP]\n",
            "full_input_length : 165\n",
            "concatinated_input : Statement: Nearly half of African-American children under the age of 6 are living in abject poverty.[EXP]\n",
            "full_input_length : 128\n",
            "concatinated_input : Statement: During the last five years, Iran has perpetrated terror attacks in 25 different countries on five continents.[EXP]\n",
            "full_input_length : 121\n",
            "concatinated_input : Statement: President Obama has left the U.S. with the lowest number of active-duty troops since before World War II, putting the nation at risk.[EXP]\n",
            "full_input_length : 93\n",
            "concatinated_input : Statement: Every student paying out-of-state tuition actually covers more than the cost of instruction.[EXP]\n",
            "full_input_length : 147\n",
            "concatinated_input : Statement: The Georgia Department of Education has implemented a new policy beginning in August that states that public schools will no longer accept credits from home school entities or non-traditional education centers.[EXP]\n",
            "full_input_length : 69\n",
            "concatinated_input : Statement: The Milwaukee County sheriffs department plays only a limited role as a traditional law enforcement agency and in 2009 reported far fewer crimes to the FBI than the University of Wisconsin-Milwaukee police did.[EXP]\n",
            "full_input_length : 141\n",
            "concatinated_input : Statement: Unlike virtually every other campaign, we dont have a super PAC.[EXP]\n",
            "full_input_length : 101\n",
            "concatinated_input : Statement: Within just a few years, immigration as a share of national population is set to break all historical records.[EXP]\n",
            "full_input_length : 118\n",
            "concatinated_input : Statement: Ed Gillespies firm even lobbied for five foreign governments, including a dictator now awaiting trial for war crimes.[EXP]\n",
            "full_input_length : 184\n",
            "concatinated_input : Statement: We have a system now where in 40 states, the highest-paid public employee is the state universitys head football or basketball coach.[EXP]\n",
            "full_input_length : 141\n",
            "concatinated_input : Statement: Because of violence spreading from Mexico, youve got bullets hitting the city hall in El Paso. Youve got bombs exploding in El Paso.[EXP]\n",
            "full_input_length : 132\n",
            "concatinated_input : Statement: Says there are concrete examples of University of Texas job applicants or prospective applicants and students as well as invited speakers changing their minds because of handguns being allowed in campus buildings and classrooms.[EXP]\n",
            "full_input_length : 161\n",
            "concatinated_input : Statement: TSA WILL ACCEPT DRIVERS PRIV CARDS FOR ID AT THE AIRPORT[EXP]\n",
            "full_input_length : 102\n",
            "concatinated_input : Statement: Forty percent of people in this country, illegally, are overstaying visas.[EXP]\n",
            "full_input_length : 119\n",
            "concatinated_input : Statement: Says Lee Fisher wanted a $1.1 billion tax increase which could have driven countless jobs out of state.[EXP]\n",
            "full_input_length : 53\n",
            "concatinated_input : Statement: Ilana Shafran Mandels stake in Forest City Enterprises constitutes significantly less than 1 percent of the companys shares and any implication of a conflict of interest is legally incorrect.[EXP]\n",
            "full_input_length : 125\n",
            "concatinated_input : Statement: The state budget has increased almost $800 million a year in each of the last two years.[EXP]labels : tensor([[48682,    25, 33140,  ..., 50256, 50256, 50256],\n",
            "        [48682,    25,   554,  ..., 50256, 50256, 50256],\n",
            "        [48682,    25,  4698,  ..., 50256, 50256, 50256],\n",
            "        ...,\n",
            "        [48682,    25, 28628,  ..., 50256, 50256, 50256],\n",
            "        [48682,    25,  1318,  ..., 50256, 50256, 50256],\n",
            "        [48682,    25, 28628,  ..., 50256, 50256, 50256]])\n",
            "labels : tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
            "        [-100, -100, -100,  ..., -100, -100, -100],\n",
            "        [-100, -100, -100,  ..., -100, -100, -100],\n",
            "        ...,\n",
            "        [-100, -100, -100,  ..., -100, -100, -100],\n",
            "        [-100, -100, -100,  ..., -100, -100, -100],\n",
            "        [-100, -100, -100,  ..., -100, -100, -100]])\n",
            "\n",
            "full_input_length : 137\n",
            "concatinated_input : Statement: In the past 10 years, our (Austin) water rates have increased by 100 percent and we now have the highest water cost of the top 10 cities in Texas.[EXP]\n",
            "full_input_length : 109\n",
            "concatinated_input : Statement: Says Mitch McConnell credits Republicans for recent economic improvements even though they took control of the Senate only days ago.[EXP]\n",
            "full_input_length : 50\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3477e8a0433a4d289206f0ad98611c4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "concatinated_input : Statement: I believe in tax cuts. I believe in being a supply-sider. I cut the income tax I think it was 24 percent. We got 42 percent more revenues.[EXP]\n",
            "full_input_length : 184\n",
            "concatinated_input : Statement: Spanish was the first European language spoken in this country.[EXP]\n",
            "full_input_length : 262\n",
            "concatinated_input : Statement: Since I took office, weve created 76,800 jobs.[EXP]\n",
            "full_input_length : 105\n",
            "concatinated_input : Statement: Irans regime is responsible for more than 1,000 American casualties during the Iraq war and has plotted a terrorist attack here in our nations capital.[EXP]\n",
            "full_input_length : 159\n",
            "labels : tensor([[48682,    25, 28628, 20472, 18184, 10824,  4734,   329,  2274,  3034,\n",
            "          8561,   772,   996,   484,  1718,  1630,   286,   262,  3845,   691,\n",
            "          1528,  2084,    13, 50257, 38708,   468, 11434,   262,  5466,   286,\n",
            "           262, 13016,    11,  2282,   484,   717,  2622,   262,  8281,   286,\n",
            "           262, 20815,    13,   383, 13016,   389,   890,  3750,    13, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100, 38708,   468, 11434,   262,  5466,   286,\n",
            "           262, 13016,    11,  2282,   484,   717,  2622,   262,  8281,   286,\n",
            "           262, 20815,    13,   383, 13016,   389,   890,  3750,    13, 50256]])\n",
            "concatinated_input : Statement: Florida high schools are four out of the top 10 in the entire United States.[EXP]\n",
            "full_input_length : 149\n",
            "labels : tensor([[48682,    25,   314,  1975,   287,  1687,  6630,    13,   314,  1975,\n",
            "           287,   852,   257,  5127,    12,    82,  1304,    13,   314,  2005,\n",
            "           262,  3739,  1687,   314,   892,   340,   373,  1987,  1411,    13,\n",
            "           775,  1392,  5433,  1411,   517, 13089,    13, 50257,     1,  1890,\n",
            "           607,    11,   262, 23082,   286,   428,   966,   318,   326,  2717,\n",
            "          1042,  1377,   262,   471,    13,   311,    13,   220,  7965,   338,\n",
            "          1598, 46925,   341,   286,  5635,  1022,   262,  2717,   290,  1181,\n",
            "          6905,  1377,   468,  1464,   587,   257,  7531,  4843,   286,   674,\n",
            "          7965,   290,   286,   674,  2260, 15012,    13, 13248,   306,  5475,\n",
            "           422,   597,  2450,    12,  3106, 37990,   351,   262,  2717, 11409,\n",
            "          1099,    11, 24497,   318,  5364,   284,  4744,   338,  8087, 10582,\n",
            "           780,   673,  7224,   340,   355,  3306,   284, 12201,   674,  2717,\n",
            "           396,  1080,   286,  1230,    13,   366,  4677,  6648,   319,  5426,\n",
            "          3000,    11, 12812,    72,  1392,   572,  2610,   287, 11142,   257,\n",
            "          2742,  4427,   284,   262,  2717,  1535,  1337,  1099,   416,  2282,\n",
            "           262,  1578,  1829,   468,   262,   366,  8807,  1080,   286,  2717,\n",
            "          1042,   287,   262,   995,    13,   366,   464,  1578,  1829,   743,\n",
            "           423,   587,   262,   717,    11,   475,   340,   338,  1290,   422,\n",
            "           262,   691,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,     1,  1890,\n",
            "           607,    11,   262, 23082,   286,   428,   966,   318,   326,  2717,\n",
            "          1042,  1377,   262,   471,    13,   311,    13,   220,  7965,   338,\n",
            "          1598, 46925,   341,   286,  5635,  1022,   262,  2717,   290,  1181,\n",
            "          6905,  1377,   468,  1464,   587,   257,  7531,  4843,   286,   674,\n",
            "          7965,   290,   286,   674,  2260, 15012,    13, 13248,   306,  5475,\n",
            "           422,   597,  2450,    12,  3106, 37990,   351,   262,  2717, 11409,\n",
            "          1099,    11, 24497,   318,  5364,   284,  4744,   338,  8087, 10582,\n",
            "           780,   673,  7224,   340,   355,  3306,   284, 12201,   674,  2717,\n",
            "           396,  1080,   286,  1230,    13,   366,  4677,  6648,   319,  5426,\n",
            "          3000,    11, 12812,    72,  1392,   572,  2610,   287, 11142,   257,\n",
            "          2742,  4427,   284,   262,  2717,  1535,  1337,  1099,   416,  2282,\n",
            "           262,  1578,  1829,   468,   262,   366,  8807,  1080,   286,  2717,\n",
            "          1042,   287,   262,   995,    13,   366,   464,  1578,  1829,   743,\n",
            "           423,   587,   262,   717,    11,   475,   340,   338,  1290,   422,\n",
            "           262,   691,    13, 50256]])\n",
            "concatinated_input : Statement: Floridas proposed amendment for medical marijuana would allow people who alleged minor ailments such as muscle spasms, neck pain, back pain and even menstrual cramps (to qualify) for government-sanctioned pot-smoking.[EXP]\n",
            "full_input_length : 191\n",
            "labels : tensor([[48682,    25,  7897,   373,   262,   717,  3427,  3303,  9635,   287,\n",
            "           428,  1499,    13, 50257, 18102, 14931, 20320,  4705,   494, 18798,\n",
            "           531,    11,   366,  1858,  4398,   470,   587,  1687,  6630,   329,\n",
            "           262,  5527,     1,   287, 24545,  5451,    13,   679,  1568,  7368,\n",
            "           257,   640,  5739,    25,  7236,    13,   383,  3050,  1687,  2458,\n",
            "          1377,   543,  5710,   262,  1353,  1687,  2494,   416,  1440,  5873,\n",
            "          2173,  1377,   318,  1690, 11987,   355,   257,  1687,  2005,   329,\n",
            "           262,  5527,    11,   475,   262, 11574,  1682,  3432,   517,    13,\n",
            "          1119,   635,  3432,   517,   706,   262,  3717,  1487,    11,   543,\n",
            "          1444,   329, 36587,  3139,  8810,   355,  8850,  3739,    13,   887,\n",
            "           356,   691,   550,   284,   467,   736,   284,  4793,   284,  1064,\n",
            "          1687,  5520,   326, 11673,  1028,   465,   966,    13,  4900,   645,\n",
            "          2392,   287,  1245,    11,   340, 10893,   281,  1683,    12, 21037,\n",
            "          1451,   319,  1181,  5704,   326,  2204,  2175,   517,   290,   517,\n",
            "          5527,   661,   355,   262,  1687,  1451,  5710,  2793,   290,  2793,\n",
            "            13,   843,   262,  1946,  1487,   287,   262,  7964,  1687,  4001,\n",
            "           326, 49311, 42252,  6304,   470, 31075,   355,  7272,    13, 14674,\n",
            "            11,   356,  1975,   262,  5527, 28719,  4414,   422,   428,  1487,\n",
            "           772,   996,   477, 40862,    11,   279,   559, 19276,   290, 42676,\n",
            "         12936,    11,   651,   262,   976,  2270,    13,   843,  1061,   514,\n",
            "           319,  3009,    25,  2488, 34470, 29660,   380,    13,  1267,     7,\n",
            "         42779,   507,    25, 11646,  5426,   373,  2097, 22171, 10540,   618,\n",
            "           339,  6619,   284,   262,  8894,  1044,  5136,   546,   262,  4793,\n",
            "          1687,  2458,    13,   383,  4238,  2196,   286,   428,  2378, 23175,\n",
            "           531,   339,   373,  2097, 14931,    13,  3759,  1879,    66, 29864,\n",
            "           373,   262,  8153,   618,   262,  3050,  1687,  2458,   547,  3804,\n",
            "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100, 18102, 14931, 20320,  4705,   494, 18798,\n",
            "           531,    11,   366,  1858,  4398,   470,   587,  1687,  6630,   329,\n",
            "           262,  5527,     1,   287, 24545,  5451,    13,   679,  1568,  7368,\n",
            "           257,   640,  5739,    25,  7236,    13,   383,  3050,  1687,  2458,\n",
            "          1377,   543,  5710,   262,  1353,  1687,  2494,   416,  1440,  5873,\n",
            "          2173,  1377,   318,  1690, 11987,   355,   257,  1687,  2005,   329,\n",
            "           262,  5527,    11,   475,   262, 11574,  1682,  3432,   517,    13,\n",
            "          1119,   635,  3432,   517,   706,   262,  3717,  1487,    11,   543,\n",
            "          1444,   329, 36587,  3139,  8810,   355,  8850,  3739,    13,   887,\n",
            "           356,   691,   550,   284,   467,   736,   284,  4793,   284,  1064,\n",
            "          1687,  5520,   326, 11673,  1028,   465,   966,    13,  4900,   645,\n",
            "          2392,   287,  1245,    11,   340, 10893,   281,  1683,    12, 21037,\n",
            "          1451,   319,  1181,  5704,   326,  2204,  2175,   517,   290,   517,\n",
            "          5527,   661,   355,   262,  1687,  1451,  5710,  2793,   290,  2793,\n",
            "            13,   843,   262,  1946,  1487,   287,   262,  7964,  1687,  4001,\n",
            "           326, 49311, 42252,  6304,   470, 31075,   355,  7272,    13, 14674,\n",
            "            11,   356,  1975,   262,  5527, 28719,  4414,   422,   428,  1487,\n",
            "           772,   996,   477, 40862,    11,   279,   559, 19276,   290, 42676,\n",
            "         12936,    11,   651,   262,   976,  2270,    13,   843,  1061,   514,\n",
            "           319,  3009,    25,  2488, 34470, 29660,   380,    13,  1267,     7,\n",
            "         42779,   507,    25, 11646,  5426,   373,  2097, 22171, 10540,   618,\n",
            "           339,  6619,   284,   262,  8894,  1044,  5136,   546,   262,  4793,\n",
            "          1687,  2458,    13,   383,  4238,  2196,   286,   428,  2378, 23175,\n",
            "           531,   339,   373,  2097, 14931,    13,  3759,  1879,    66, 29864,\n",
            "           373,   262,  8153,   618,   262,  3050,  1687,  2458,   547,  3804,\n",
            "            13, 50256]])\n",
            "concatinated_input : Statement: Proposed cuts in the House farm bill mean 2 million less people on food stamps, 210,000 children will not receive school lunches or breakfasts.[EXP]\n",
            "full_input_length : 129\n",
            "labels : tensor([[48682,    25,  4619,   314,  1718,  2607,    11,   356,   303,  2727,\n",
            "          8684,    11,  7410,  3946,    13, 50257, 11158,    11,   339,   531,\n",
            "            11,   867,  4165,  1337, 17206,  3588,   470,  4379,   649,  3871,\n",
            "           379,   477,    11,  1771,   484,   389,   319, 13594,   393,   423,\n",
            "          2839,  5096,    13,   554,   262,   717,   636,   286,   465,  2643,\n",
            "            11, 10009,   318,   287,   262, 40598,    25,  4377,  3003,   422,\n",
            "          1315,   284,  1248,  1510,   649,  3871,   714, 14627,   287, 13594,\n",
            "           611,   262,  1535,  1337, 18708,   318,  1234,   656,  1099,    13,\n",
            "           775,   635,  1043,   326, 10009,   338, 10238,   966,    11,   326,\n",
            "           617, 13594,  3871,  1244,   423,  5876,  4917,  7519,    11,   318,\n",
            "           257,  4938,  2328,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100, 11158,    11,   339,   531,\n",
            "            11,   867,  4165,  1337, 17206,  3588,   470,  4379,   649,  3871,\n",
            "           379,   477,    11,  1771,   484,   389,   319, 13594,   393,   423,\n",
            "          2839,  5096,    13,   554,   262,   717,   636,   286,   465,  2643,\n",
            "            11, 10009,   318,   287,   262, 40598,    25,  4377,  3003,   422,\n",
            "          1315,   284,  1248,  1510,   649,  3871,   714, 14627,   287, 13594,\n",
            "           611,   262,  1535,  1337, 18708,   318,  1234,   656,  1099,    13,\n",
            "           775,   635,  1043,   326, 10009,   338, 10238,   966,    11,   326,\n",
            "           617, 13594,  3871,  1244,   423,  5876,  4917,  7519,    11,   318,\n",
            "           257,  4938,  2328,    13, 50256]])\n",
            "concatinated_input : Statement: Senator Obama has, in fact, never had a serious Republican challenger.[EXP]\n",
            "full_input_length : 99\n",
            "labels : tensor([[48682,    25,  5686,   504,  7142,   318,  4497,   329,   517,   621,\n",
            "           352,    11,   830,  1605, 18499,  1141,   262,  3908,  1175,   290,\n",
            "           468, 37515,   257,  7417,  1368,   994,   287,   674,  7027,  3139,\n",
            "            13, 50257,  5122, 10681,  2494,   318,   625,   767,  1411,   290,\n",
            "          3957,    13,   775, 37788,   761,   281,  3034,  7628,  5301,   220,\n",
            "           290,   356,   761,   340,  3393,    13,   366,  2202,  2365,    13,\n",
            "           220,  2242,    11,  2486,  1138,   351,  3415,  5531,   284,   651,\n",
            "           511,  5128,   319,   262,  5150, 19819,  5301,    26,   290,   339,\n",
            "          4987,   284,  1826,   351,   262,  3415, 40100,   428,  1285,    11,\n",
            "           996,   867,  4734, 39247,   276,   379,   262, 13052,   340,   481,\n",
            "          1255,   287,  1997,  1969,   284,   257, 20953,  1410,    13,  1881,\n",
            "           460,  7267,  1771,   262, 25615,   286,   262,  3034,  4902, 31405,\n",
            "            82,   262,   761,   329, 18988,  9110,   832, 11702, 18921,    11,\n",
            "           475, 32342,   318,   826,    25,  1318,   423,   587,   645, 18921,\n",
            "           319,   262,  4858, 19819,  5301,   326,  3162,   318,  2938,   284,\n",
            "          3015,   319,   355,  1903,   355,   428,  1285,    13, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  5122, 10681,  2494,   318,   625,   767,  1411,   290,\n",
            "          3957,    13,   775, 37788,   761,   281,  3034,  7628,  5301,   220,\n",
            "           290,   356,   761,   340,  3393,    13,   366,  2202,  2365,    13,\n",
            "           220,  2242,    11,  2486,  1138,   351,  3415,  5531,   284,   651,\n",
            "           511,  5128,   319,   262,  5150, 19819,  5301,    26,   290,   339,\n",
            "          4987,   284,  1826,   351,   262,  3415, 40100,   428,  1285,    11,\n",
            "           996,   867,  4734, 39247,   276,   379,   262, 13052,   340,   481,\n",
            "          1255,   287,  1997,  1969,   284,   257, 20953,  1410,    13,  1881,\n",
            "           460,  7267,  1771,   262, 25615,   286,   262,  3034,  4902, 31405,\n",
            "            82,   262,   761,   329, 18988,  9110,   832, 11702, 18921,    11,\n",
            "           475, 32342,   318,   826,    25,  1318,   423,   587,   645, 18921,\n",
            "           319,   262,  4858, 19819,  5301,   326,  3162,   318,  2938,   284,\n",
            "          3015,   319,   355,  1903,   355,   428,  1285,    13, 50256]])\n",
            "concatinated_input : Statement: Says Russ Feingold broke his 1992 promise to always get the majority of funding from Wisconsin residents.[EXP]\n",
            "full_input_length : 136\n",
            "labels : tensor([[48682,    25,  4744,  1029,  4266,   389,  1440,   503,   286,   262,\n",
            "          1353,   838,   287,   262,  2104,  1578,  1829,    13, 50257, 41811,\n",
            "           531,    25,   366,   818,   262,   614,  2211,    11,   262,  2486,\n",
            "          3662,  2716, 14436,    11,   830,  4301,  5293, 16269,    13,  1119,\n",
            "          2716, 28817, 38313,   220,   661,   351, 19625, 19131,    11,   508,\n",
            "           389,   994, 15572,    13,   366,  1135,   766,   703,  8742,  4251,\n",
            "           465,  5538,    13,   887, 23358,  1139, 27191,  3925,   351,  5123,\n",
            "         19131,   547,  2716,   290,  7724,  1411,   286,   883, 10050,   547,\n",
            "         13677,   448,   286,   663,  1630,    13,  4418,    11,  8257,    11,\n",
            "           830,   286,   262,   661,  8742,  3417,   355,  2716,   547,  1239,\n",
            "          1682, 14847,   416,   262,  4086,    13,   554,  3090,    11,   428,\n",
            "          2643,   318,  4814,  9204,  4732,    13,  2329,   355,   356,   531,\n",
            "           287, 17217,  4176,    82, 14305,    11,   663, 35010,   284,  1950,\n",
            "           262,  3662,   468,  1336,  2551,    12,  8601,  4934,    13,  3078,\n",
            "          5370,   290,  2717,  3657,   711,  1593,  9176,    13, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 41811,\n",
            "           531,    25,   366,   818,   262,   614,  2211,    11,   262,  2486,\n",
            "          3662,  2716, 14436,    11,   830,  4301,  5293, 16269,    13,  1119,\n",
            "          2716, 28817, 38313,   220,   661,   351, 19625, 19131,    11,   508,\n",
            "           389,   994, 15572,    13,   366,  1135,   766,   703,  8742,  4251,\n",
            "           465,  5538,    13,   887, 23358,  1139, 27191,  3925,   351,  5123,\n",
            "         19131,   547,  2716,   290,  7724,  1411,   286,   883, 10050,   547,\n",
            "         13677,   448,   286,   663,  1630,    13,  4418,    11,  8257,    11,\n",
            "           830,   286,   262,   661,  8742,  3417,   355,  2716,   547,  1239,\n",
            "          1682, 14847,   416,   262,  4086,    13,   554,  3090,    11,   428,\n",
            "          2643,   318,  4814,  9204,  4732,    13,  2329,   355,   356,   531,\n",
            "           287, 17217,  4176,    82, 14305,    11,   663, 35010,   284,  1950,\n",
            "           262,  3662,   468,  1336,  2551,    12,  8601,  4934,    13,  3078,\n",
            "          5370,   290,  2717,  3657,   711,  1593,  9176,    13, 50256]])\n",
            "concatinated_input : Statement: While in the Illinois Senate, Barack Obama passed \"tax cuts for hard-working families.\"[EXP]\n",
            "full_input_length : 124\n",
            "labels : tensor([[48682,    25,  4432, 24496,  5150, 11326,   329,  3315,  5727,   561,\n",
            "          1249,   661,   508,  4260,  4159, 42549,   884,   355,  8280,   599,\n",
            "         34432,    11,  7393,  2356,    11,   736,  2356,   290,   772, 37230,\n",
            "          1067,  9430,   357,  1462, 12780,     8,   329,  1230,    12, 12807,\n",
            "           596,   276,  1787,    12, 48783,    13, 50257,  1870,   356,  1053,\n",
            "          1541,  1775,   326,   357, 30464,   391,   318,     8,   407,  1016,\n",
            "           284,  2245,   262,   895,  4127,   290,  3434,   422,   465,  7681,\n",
            "          2491,   523,    12,  7174,   642,  1983,  2628,    11,   508,   481,\n",
            "          4341,  5242,   290,  5242,   286,  5054,   287, 15822, 10976,    13,\n",
            "           366, 18932,  1771,   484, 19189,   281,  4381,   351, 14264,    11,\n",
            "           262,  2486,  1923,  6235,   514,   284,   220,   220,   220,   220,\n",
            "           257,  1705,  1848,   220,   220,   220,   326,   531,  5811, 41971,\n",
            "            11,   281,  6136,   329,  2486,    11,  1138,   351, 14264,  9326,\n",
            "           284,  2112,   703,  1111,  9964,   714,  8076,   287,   262,  1171,\n",
            "         15435,  1080,    13,   887,   326,  1848,   373, 17548,    88,   290,\n",
            "          1422,   470,  2128,   284,   514,   588,   262,   366, 49639, 14748,\n",
            "             1,   326,  2486,   550,  8072,    13,   383,  1109,   318,   326,\n",
            "          2486,   531,   339,   561, 10660,  1171, 15435,    11,   475,  3066,\n",
            "           340,  2492,   470,   287,   262,  1923,   338, 16106,  1393,    13,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  1870,   356,  1053,\n",
            "          1541,  1775,   326,   357, 30464,   391,   318,     8,   407,  1016,\n",
            "           284,  2245,   262,   895,  4127,   290,  3434,   422,   465,  7681,\n",
            "          2491,   523,    12,  7174,   642,  1983,  2628,    11,   508,   481,\n",
            "          4341,  5242,   290,  5242,   286,  5054,   287, 15822, 10976,    13,\n",
            "           366, 18932,  1771,   484, 19189,   281,  4381,   351, 14264,    11,\n",
            "           262,  2486,  1923,  6235,   514,   284,   220,   220,   220,   220,\n",
            "           257,  1705,  1848,   220,   220,   220,   326,   531,  5811, 41971,\n",
            "            11,   281,  6136,   329,  2486,    11,  1138,   351, 14264,  9326,\n",
            "           284,  2112,   703,  1111,  9964,   714,  8076,   287,   262,  1171,\n",
            "         15435,  1080,    13,   887,   326,  1848,   373, 17548,    88,   290,\n",
            "          1422,   470,  2128,   284,   514,   588,   262,   366, 49639, 14748,\n",
            "             1,   326,  2486,   550,  8072,    13,   383,  1109,   318,   326,\n",
            "          2486,   531,   339,   561, 10660,  1171, 15435,    11,   475,  3066,\n",
            "           340,  2492,   470,   287,   262,  1923,   338, 16106,  1393,    13,\n",
            "         50256]])\n",
            "concatinated_input : Statement: The states that are doing better are the ones that have no state income tax.[EXP]\n",
            "full_input_length : 61\n",
            "labels : tensor([[48682,    25,  8772,  1335,  6630,   287,   262,  2097,  5318,  2855,\n",
            "          1612,   362,  1510,  1342,   661,   319,  2057, 25560,    11, 20064,\n",
            "            11,   830,  1751,   481,   407,  3328,  1524, 14678,  2052,   393,\n",
            "          2270,    69,  5773,    13, 50257,  1320,   373,  3940,   416,   257,\n",
            "          2068,   366,  2704,   541,     1,   736,   284,  1104,   286,  2035,\n",
            "         14218,   393,  7748,  2742,  3722,   287,   262,  4894,   286,  5581,\n",
            "          9299,    13,  3827,   262,   812,  5511,   468,   531,   339, 19344,\n",
            "         14218,   393,  2742, 27308,    11, 21135, 30913,   284, 11628,   326,\n",
            "           714,   307,  3177,  1626,   257, 10595,  4975,  3626,    13,  1406,\n",
            "           379,  1661,   339,   468, 18079,  1111,    11,   393,  2035,   530,\n",
            "            13,   383,   411,   645,  4719,    11,   996,    11,   326,   262,\n",
            "         24298,  5511,   287,   262,  1492,   550,   257,  1180,  4459,   422,\n",
            "           262, 24298,  5511,   319,   262,  1492,  4205,    13, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  1320,   373,  3940,   416,   257,\n",
            "          2068,   366,  2704,   541,     1,   736,   284,  1104,   286,  2035,\n",
            "         14218,   393,  7748,  2742,  3722,   287,   262,  4894,   286,  5581,\n",
            "          9299,    13,  3827,   262,   812,  5511,   468,   531,   339, 19344,\n",
            "         14218,   393,  2742, 27308,    11, 21135, 30913,   284, 11628,   326,\n",
            "           714,   307,  3177,  1626,   257, 10595,  4975,  3626,    13,  1406,\n",
            "           379,  1661,   339,   468, 18079,  1111,    11,   393,  2035,   530,\n",
            "            13,   383,   411,   645,  4719,    11,   996,    11,   326,   262,\n",
            "         24298,  5511,   287,   262,  1492,   550,   257,  1180,  4459,   422,\n",
            "           262, 24298,  5511,   319,   262,  1492,  4205,    13, 50256]])\n",
            "concatinated_input : Statement: Ohio Republicans made significant gains during [Chris McNultys] time at the ORP, including the extremely successful re-election of President Bush in Ohio.[EXP]\n",
            "full_input_length : 126\n",
            "labels : tensor([[48682,    25,  8962,  2486,   468,    11,   287,  1109,    11,  1239,\n",
            "           550,   257,  2726,  3415, 32127,    13, 50257,   464,   411,   645,\n",
            "          4719,   326, 16210,   329,  1012,  2954,   364,   373,   636,   286,\n",
            "           262, 19819,   290,   326,   262,  1430, 29657,   262,  4220,  3951,\n",
            "           286, 24666,   469,   297,    82, 15737,  5748,    13,   887,   356,\n",
            "         18548,  1295,   257,  8872,  2033,   319,   262,  4414,   284, 24666,\n",
            "           695,    13,   383,   512, 31238,  5644,   326,   465, 15737,  5748,\n",
            "           720, 39710,    11,   830,   287,  3405,   689,   547,  3892, 10177,\n",
            "           290,   262,   411,  1738,   284,  1975, 24666, 19187,  4036,  4461,\n",
            "           422,  1012,  2954,   364,   373, 15394,  4833,    13, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,   464,   411,   645,\n",
            "          4719,   326, 16210,   329,  1012,  2954,   364,   373,   636,   286,\n",
            "           262, 19819,   290,   326,   262,  1430, 29657,   262,  4220,  3951,\n",
            "           286, 24666,   469,   297,    82, 15737,  5748,    13,   887,   356,\n",
            "         18548,  1295,   257,  8872,  2033,   319,   262,  4414,   284, 24666,\n",
            "           695,    13,   383,   512, 31238,  5644,   326,   465, 15737,  5748,\n",
            "           720, 39710,    11,   830,   287,  3405,   689,   547,  3892, 10177,\n",
            "           290,   262,   411,  1738,   284,  1975, 24666, 19187,  4036,  4461,\n",
            "           422,  1012,  2954,   364,   373, 15394,  4833,    13, 50256]])\n",
            "concatinated_input : Statement: Live cats have holes drilled into their skulls, posts put into their heads and coils put into their eyes, and some have had their ears cut off or are intentionally deafened or starved at UW-Madison labs that do research to improve hearing in humans.[EXP]\n",
            "full_input_length : 149\n",
            "labels : tensor([[48682,    25, 28628,  1887,  5452,   278,   727,  6265,   465,  9768,\n",
            "          6991,   284,  1464,   651,   262,  3741,   286,  4918,   422,  9279,\n",
            "          5085,    13, 50257, 41811,   531,   326,   366, 12518,   357, 23672,\n",
            "          1940,     8, 12823,  1625,   287,    11,   422, 15524,   284, 14489,\n",
            "            11,  3034,  3349, 16449,  1342,   621,   352,  1411,   257,   614,\n",
            "            13,  8742,  6825, 11545,   584,  1440,    12,  1941,  9574,   326,\n",
            "          4197,   262,  9987,    13,   554,  3090,    11,   465, 26863,  1377,\n",
            "           326,  4956,   389,  5688,   284,  8138,   329,   883,  3403,  1377,\n",
            "         46701,  1302,   510,   284, 12219, 15794,    11,  3573,   618,   339,\n",
            "         15009,   262,  2278,  3726,   287,  4793,   357,  4758,  3017,  1115,\n",
            "           812,  9944,   739,  5511,     8,   290,  4343,   357,  4758,  3017,\n",
            "           734,   812,  9944,   739,  5511,   737,   770,  2925,   319,  1353,\n",
            "           286,  2276, 13479,   546,   703,   881,  8138,   284,  8333, 19033,\n",
            "           329,  3595,  3034,  3403,    13, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100, 41811,   531,   326,   366, 12518,   357, 23672,\n",
            "          1940,     8, 12823,  1625,   287,    11,   422, 15524,   284, 14489,\n",
            "            11,  3034,  3349, 16449,  1342,   621,   352,  1411,   257,   614,\n",
            "            13,  8742,  6825, 11545,   584,  1440,    12,  1941,  9574,   326,\n",
            "          4197,   262,  9987,    13,   554,  3090,    11,   465, 26863,  1377,\n",
            "           326,  4956,   389,  5688,   284,  8138,   329,   883,  3403,  1377,\n",
            "         46701,  1302,   510,   284, 12219, 15794,    11,  3573,   618,   339,\n",
            "         15009,   262,  2278,  3726,   287,  4793,   357,  4758,  3017,  1115,\n",
            "           812,  9944,   739,  5511,     8,   290,  4343,   357,  4758,  3017,\n",
            "           734,   812,  9944,   739,  5511,   737,   770,  2925,   319,  1353,\n",
            "           286,  2276, 13479,   546,   703,   881,  8138,   284,  8333, 19033,\n",
            "           329,  3595,  3034,  3403,    13, 50256]])\n",
            "concatinated_input : Statement: Texas population is projected to double in the next 50 years or so, but our basic amount of water will remain about where it is now.[EXP]\n",
            "full_input_length : 150\n",
            "labels : tensor([[48682,    25,  2893,   287,   262,  9486,  3845,    11,  8732,  2486,\n",
            "          3804,   366, 19290,  6630,   329,  1327,    12, 16090,  4172,   526,\n",
            "         50257, 10294,    11,  2048,   477,  1402,  5692,  1377,   883,   351,\n",
            "          7380,   621,  2026,  4409,  1377,   389, 13068,   422, 12970,    11,\n",
            "          1771,   484,  2897,  5096,   393,   407,    13,  2773,  1402,  5692,\n",
            "           389,  4025,   621,  2026,  4409,   290,   714,  1986, 17176,   611,\n",
            "           484,   836,   470,  2897,  4409,  5096,    13,   887,   257,  5909,\n",
            "          3741,   286,   471,    13,   311,    13,   220,  9611,   389,  4833,\n",
            "           621,  2026,  4409,   290,   389, 13068,   422,   262,  1535,  5096,\n",
            "          5359,    13,   383, 11847,   338,   512,   318, 18404,    11,   290,\n",
            "          1595,   470,  1848,   329,   597,   286,   262,  3967,  8617,   326,\n",
            "           836,   470,   366,  6098,  1530,     1,  1402,  1597,   475,  1682,\n",
            "          1037,   606,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100, 10294,    11,  2048,   477,  1402,  5692,  1377,   883,   351,\n",
            "          7380,   621,  2026,  4409,  1377,   389, 13068,   422, 12970,    11,\n",
            "          1771,   484,  2897,  5096,   393,   407,    13,  2773,  1402,  5692,\n",
            "           389,  4025,   621,  2026,  4409,   290,   714,  1986, 17176,   611,\n",
            "           484,   836,   470,  2897,  4409,  5096,    13,   887,   257,  5909,\n",
            "          3741,   286,   471,    13,   311,    13,   220,  9611,   389,  4833,\n",
            "           621,  2026,  4409,   290,   389, 13068,   422,   262,  1535,  5096,\n",
            "          5359,    13,   383, 11847,   338,   512,   318, 18404,    11,   290,\n",
            "          1595,   470,  1848,   329,   597,   286,   262,  3967,  8617,   326,\n",
            "           836,   470,   366,  6098,  1530,     1,  1402,  1597,   475,  1682,\n",
            "          1037,   606,    13, 50256]])\n",
            "concatinated_input : Statement: Ohios Planned Parenthood operations received millions of taxpayer dollars via federal grants in 2010 and 2011.[EXP]\n",
            "full_input_length : 42\n",
            "labels : tensor([[48682,    25,   383,  2585,   326,   389,  1804,  1365,   389,   262,\n",
            "          3392,   326,   423,   645,  1181,  3739,  1687,    13, 50257,  1537,\n",
            "           314,  1101,  5597,  1909,  1011,   319,   477,   401,   364,   220,\n",
            "           477,   401,   364,   220,   287,   257,  2276,  3071,    13,   843,\n",
            "          4361,    11,   314,   423,  3066,   284,   307,   257,  4540,   329,\n",
            "           302,    12, 14300,   287,  3050,   287,   262,  4390,  4165,    13,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1537,\n",
            "           314,  1101,  5597,  1909,  1011,   319,   477,   401,   364,   220,\n",
            "           477,   401,   364,   220,   287,   257,  2276,  3071,    13,   843,\n",
            "          4361,    11,   314,   423,  3066,   284,   307,   257,  4540,   329,\n",
            "           302,    12, 14300,   287,  3050,   287,   262,  4390,  4165,    13,\n",
            "         50256]])\n",
            "concatinated_input : Statement: Says President Barack Obama promised a pathway to citizenship to undocumented immigrants and didnt deliver jack squat on any of it.[EXP]\n",
            "full_input_length : 108\n",
            "labels : tensor([[48682,    25,  6835,  4734,   925,  2383,  8810,  1141,   685, 15645,\n",
            "         22586,   586,   893,    60,   640,   379,   262,  6375,    47,    11,\n",
            "          1390,   262,  4457,  4388,   302,    12, 14300,   286,  1992,  5511,\n",
            "           287,  6835,    13, 50257, 41811,   531,    11,   366,   464, 10193,\n",
            "           468,  3088,   284,  8160,   257,   279, 24500,   393,   257, 37664,\n",
            "         30115,   319,   534,  5318,   284,   307, 20436,   540, 10150,   290,\n",
            "          4145,  2426,   284,  4858,  6142,  6647,    13,   366,   464, 14724,\n",
            "          1722,  1660,  3896,  5734, 36833,   279,  4185,   829,    13,   632,\n",
            "           691,  8991,   284,   288,  9249,   326,   389, 12006,   503,   286,\n",
            "         15190,   393,   288,  9249,   326,  2163,   588, 15190,   290,   714,\n",
            "          3283, 12231,   284, 33218, 10150,    13,  8742,   267, 24883,   326,\n",
            "           262,  1660,  3896,   373,  1234,   319,  1745,   416,   262,  8028,\n",
            "           287,  1853, 13310, 19284,    13, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256]])\n",
            "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100, 41811,   531,    11,   366,   464, 10193,\n",
            "           468,  3088,   284,  8160,   257,   279, 24500,   393,   257, 37664,\n",
            "         30115,   319,   534,  5318,   284,   307, 20436,   540, 10150,   290,\n",
            "          4145,  2426,   284,  4858,  6142,  6647,    13,   366,   464, 14724,\n",
            "          1722,  1660,  3896,  5734, 36833,   279,  4185,   829,    13,   632,\n",
            "           691,  8991,   284,   288,  9249,   326,   389, 12006,   503,   286,\n",
            "         15190,   393,   288,  9249,   326,  2163,   588, 15190,   290,   714,\n",
            "          3283, 12231,   284, 33218, 10150,    13,  8742,   267, 24883,   326,\n",
            "           262,  1660,  3896,   373,  1234,   319,  1745,   416,   262,  8028,\n",
            "           287,  1853, 13310, 19284,    13, 50256]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "`.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-274-eac7cc480f7c>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     train_and_test_CEG(\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mmodel_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpt2_small_config2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mtrain_and_test_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_and_test_config_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-242-7137d5856615>\u001b[0m in \u001b[0;36mtrain_and_test_CEG\u001b[0;34m(model_config, train_and_test_config, batch_size, base_time, set_type)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_result_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         return call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m         ckpt_path = self._checkpoint_connector._select_ckpt_path(\n\u001b[0m\u001b[1;32m    792\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_provided\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\u001b[0m in \u001b[0;36m_select_ckpt_path\u001b[0;34m(self, state_fn, ckpt_path, model_provided, model_connected)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             ckpt_path = self._parse_ckpt_path(\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0mstate_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\u001b[0m in \u001b[0;36m_parse_ckpt_path\u001b[0;34m(self, state_fn, ckpt_path, model_provided, model_connected)\u001b[0m\n\u001b[1;32m    169\u001b[0m                         \u001b[0;34mf\" Please pass an exact checkpoint path to `.{fn}(ckpt_path=...)`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                     )\n\u001b[0;32m--> 171\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    172\u001b[0m                     \u001b[0;34mf'`.{fn}(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: `.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 通しで推論"
      ],
      "metadata": {
        "id": "rrkUASPId8Yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データのロード\n",
        "train_data_true = read_tsv(train_and_test_config_true[\"TRAINING_DATA_PATH\"])\n",
        "train_data_false = read_tsv(train_and_test_config_false[\"TRAINING_DATA_PATH\"])"
      ],
      "metadata": {
        "id": "H_nePVdlkeOz"
      },
      "execution_count": 508,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base_time = get_base_time()\n",
        "# print(base_time)"
      ],
      "metadata": {
        "id": "JlGa761SkEK5"
      },
      "execution_count": 509,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# # # 推論の実行\n",
        "# true_gpt2_bs1        = predict_CEG(gpt2_small_config_for_prediction, 'gpt2', train_data_true[:5], ckpt_dict_true['gpt2-small-bs1'], 'train', 'true', 1, time)\n",
        "# true_gpt2_medium_bs1 = predict_CEG(gpt2_medium_config_for_prediction, 'gpt2-medium', train_data_true[:5], ckpt_dict_true['gpt2-medium-bs1'], 'train', 'true', 1, time)\n",
        "# true_t5_small_bs1    = predict_CEG(t5_small_config_for_prediction, 't5-small', train_data_true[:5], ckpt_dict_true['t5-small-bs1'], 'train', 'true', 1, time)\n",
        "# true_t5_base_bs1     = predict_CEG(t5_base_config_for_prediction, 't5-base', train_data_true[:5], ckpt_dict_true['t5-base-bs1'], 'train', 'true', 1, time)\n",
        "# true_bart_base_bs1   = predict_CEG(bart_base_config_for_prediction, 'facebook/bart-base', train_data_true[:5], ckpt_dict_true['bart-base-bs1'], 'train', 'true', 1, time)\n",
        "\n",
        "# true_gpt2_bs2        = predict_CEG(gpt2_small_config_for_prediction, 'gpt2', train_data_true[:5], ckpt_dict_true['gpt2-small-bs2'], 'train', 'true', 1, time)\n",
        "# true_gpt2_medium_bs2 = predict_CEG(gpt2_medium_config_for_prediction, 'gpt2-medium', train_data_true[:5], ckpt_dict_true['gpt2-medium-bs2'], 'train', 'true', 1, time)\n",
        "# true_t5_small_bs2    = predict_CEG(t5_small_config_for_prediction, 't5-small', train_data_true[:5], ckpt_dict_true['t5-small-bs2'], 'train', 'true', 1, time)\n",
        "# true_t5_base_bs2     = predict_CEG(t5_base_config_for_prediction, 't5-base', train_data_true[:5], ckpt_dict_true['t5-base-bs2'], 'train', 'true', 1, time)\n",
        "# true_bart_base_bs2   = predict_CEG(bart_base_config_for_prediction, 'facebook/bart-base', train_data_true[:5], ckpt_dict_true['bart-base-bs2'], 'train', 'true', 1, time)\n",
        "\n",
        "# false_gpt2_bs1        = predict_CEG(gpt2_small_config_for_prediction, 'gpt2', train_data_false[:5], ckpt_dict_false['gpt2-small-bs1'], 'train', 'false', 1, time)\n",
        "# false_gpt2_medium_bs1 = predict_CEG(gpt2_medium_config_for_prediction, 'gpt2-medium', train_data_false[:5], ckpt_dict_false['gpt2-medium-bs1'], 'train', 'false', 1, time)\n",
        "# false_t5_small_bs1    = predict_CEG(t5_small_config_for_prediction, 't5-small', train_data_false[:5], ckpt_dict_false['t5-small-bs1'], 'train', 'false', 1, time)\n",
        "# false_t5_base_bs1     = predict_CEG(t5_base_config_for_prediction, 't5-base', train_data_false[:5], ckpt_dict_false['t5-base-bs1'], 'train', 'false', 1, time)\n",
        "# false_bart_base_bs1   = predict_CEG(bart_base_config_for_prediction, 'facebook/bart-base', train_data_false[:5], ckpt_dict_false['bart-base-bs1'], 'train', 'false', 1, time)\n",
        "\n",
        "# false_gpt2_bs2        = predict_CEG(gpt2_small_config_for_prediction, 'gpt2', train_data_false[:5], ckpt_dict_false['gpt2-small-bs2'], 'train', 'false', 1, time)\n",
        "# false_gpt2_medium_bs2 = predict_CEG(gpt2_medium_config_for_prediction, 'gpt2-medium', train_data_false[:5], ckpt_dict_false['gpt2-medium-bs2'], 'train', 'false', 1, time)\n",
        "# false_t5_small_bs2    = predict_CEG(t5_small_config_for_prediction, 't5-small', train_data_false[:5], ckpt_dict_false['t5-small-bs2'], 'train', 'false', 1, time)\n",
        "# false_t5_base_bs2     = predict_CEG(t5_base_config_for_prediction, 't5-base', train_data_false[:5], ckpt_dict_false['t5-base-bs2'], 'train', 'false', 1, time)\n",
        "# false_bart_base_bs2   = predict_CEG(bart_base_config_for_prediction, 'facebook/bart-base', train_data_false[:5], ckpt_dict_false['bart-base-bs2'], 'train', 'false', 1, time)\n"
      ],
      "metadata": {
        "id": "PPOmjg-RhqIl"
      },
      "execution_count": 510,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ちまちま推論\n",
        "- 23: gpt2 入力入れ替え, gpt2 statementのみ"
      ],
      "metadata": {
        "id": "702E7sGAkbyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データのロード\n",
        "train_data_true = read_tsv(train_and_test_config_true[\"TRAINING_DATA_PATH\"])\n",
        "train_data_false = read_tsv(train_and_test_config_false[\"TRAINING_DATA_PATH\"])"
      ],
      "metadata": {
        "id": "BIXqI-eiksB8"
      },
      "execution_count": 511,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base_time = get_base_time()\n",
        "# print(base_time)"
      ],
      "metadata": {
        "id": "EEDlN_9SksCZ"
      },
      "execution_count": 427,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flip\n",
        "\n",
        "time = '2024-01-24 13:17 flip'\n",
        "\n",
        "# config_dict\n",
        "ckpt_dict_true = {\n",
        "    'gpt2-small-bs1' : f'{time}/true_batch_size=1',\n",
        "    'gpt2-small-bs2' : f'{time}/true_batch_size=2',\n",
        "    'gpt2-medium-bs1' : f'{time}/true_batch_size=1',\n",
        "    'gpt2-medium-bs2' : f'{time}/true_batch_size=2',\n",
        "    't5-small-bs1' : f'{time}/true_batch_size=1',\n",
        "    't5-small-bs2' : f'{time}/true_batch_size=2',\n",
        "    't5-base-bs1' : f'{time}/true_batch_size=1',\n",
        "    't5-base-bs2' : f'{time}/true_batch_size=2',\n",
        "    'bart-base-bs1' : f'{time}/true_batch_size=1',\n",
        "    'bart-base-bs2' : f'{time}/true_batch_size=2',\n",
        "}\n",
        "\n",
        "ckpt_dict_false = {\n",
        "    'gpt2-small-bs1' : f'{time}/false_batch_size=1',\n",
        "    'gpt2-small-bs2' : f'{time}/false_batch_size=2',\n",
        "    'gpt2-medium-bs1' : f'{time}/false_batch_size=1',\n",
        "    'gpt2-medium-bs2' : f'{time}/false_batch_size=2',\n",
        "    't5-small-bs1' : f'{time}/false_batch_size=1',\n",
        "    't5-small-bs2' : f'{time}/false_batch_size=2',\n",
        "    't5-base-bs1' : f'{time}/false_batch_size=1',\n",
        "    't5-base-bs2' : f'{time}/false_batch_size=2',\n",
        "    'bart-base-bs1' : f'{time}/false_batch_size=1',\n",
        "    'bart-base-bs2' : f'{time}/false_batch_size=2',\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# GPT2-small\n",
        "\n",
        "predict_CEG(gpt2_small_config_for_prediction, 'gpt2', train_data_true[:5], ckpt_dict_true['gpt2-small-bs1'], 'train', 'true', 1, time)\n",
        "predict_CEG(gpt2_medium_config_for_prediction, 'gpt2-medium', train_data_true[:5], ckpt_dict_true['gpt2-medium-bs1'], 'train', 'true', 1, time)\n",
        "\n",
        "predict_CEG(gpt2_small_config_for_prediction, 'gpt2', train_data_true[:5], ckpt_dict_true['gpt2-small-bs2'], 'train', 'true', 1, time)\n",
        "predict_CEG(gpt2_medium_config_for_prediction, 'gpt2-medium', train_data_true[:5], ckpt_dict_true['gpt2-medium-bs2'], 'train', 'true', 1, time)\n",
        "\n",
        "predict_CEG(gpt2_small_config_for_prediction, 'gpt2', train_data_false[:5], ckpt_dict_false['gpt2-small-bs1'], 'train', 'false', 1, time)\n",
        "predict_CEG(gpt2_medium_config_for_prediction, 'gpt2-medium', train_data_false[:5], ckpt_dict_false['gpt2-medium-bs1'], 'train', 'false', 1, time)\n",
        "\n",
        "predict_CEG(gpt2_small_config_for_prediction, 'gpt2', train_data_false[:5], ckpt_dict_false['gpt2-small-bs2'], 'train', 'false', 1, time)\n",
        "predict_CEG(gpt2_medium_config_for_prediction, 'gpt2-medium', train_data_false[:5], ckpt_dict_false['gpt2-medium-bs2'], 'train', 'false', 1, time)\n"
      ],
      "metadata": {
        "id": "UB5iW9uul6P0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "85b528cc3d674592a1d75b31df5f4cac",
            "d1fd46f03cc941acac149df9b6e1f466",
            "0b3c2f1cd33442adadbe1a9c79a88c72",
            "0f2c615dcad14c7a875ac1a2f76e948a",
            "9d5fb1289f46455ea81bdf57654db58c",
            "88efbe05fe8c4212b749d4ebb5d1d0ca",
            "0ab491ed66244e3887ec7ed421857a02",
            "2e1783b4b3bc4402be734a20a1622ae2",
            "579eec66b0784d54be5b53c162a6fd65",
            "87780854080f48fbb3a5b3e4889446e1",
            "2e9824bcc2e84880ab97614fa7ea0e23",
            "ed14a584911343688055c02d91acc62a",
            "8b0077a65e2a4f45bb43d4285e9181d6",
            "f87889a3083a46478bdde8f31917ff3c",
            "6059c2b73962419f8f17445330154a13",
            "4bfe0c276b234be3a487d9c5b15f2b58",
            "bd630fa908b54bd795102509c12ae023",
            "d89c003bcc5f4d6c95a81001ee7774dd",
            "fbb5310502594faaaf73988e537b8ab8",
            "4ae6737e33004c7bad880aac673e1fca",
            "963b266b4119478b9c1e53dda98e29f0",
            "0cc44e80862742629edd7b8d26b308f6",
            "d9c9022492664083975b929c9977e4a4",
            "b50c150288df40e3b2190f0d4cf1109f",
            "d40e091cba7846758d4ee302feb6a4cc",
            "59ec0f063f72469a952f7fc394f8487c",
            "9b19254422f34ed49cc1d0f4292169fd",
            "0256a74ed52f4519924140971863dcbe",
            "0dfc2605cf8846c0bc0aafd14de62458",
            "3a0b8d597ebe42d3ae555e03b8de2d87",
            "119e12dfd40c4c57998927f0be7df301",
            "9ba523e8fc7f4bc9a75e11932f0ea704",
            "ea13fcf1c3664ef5bab28150b770fdf2",
            "fc4822d09dc84cdebf95a55442af36c8",
            "9f5a6f6a67984e679050a752323cc70d",
            "7100ac2213a74096b4ae9d3c9fe4f04f",
            "c6e04b90778d43f9a690ee82b1f02971",
            "e2ac3c96f3e342b9aba6875325d6fee8",
            "69d3de89868d49049cd148f8d871ce29",
            "1db8da5fe1f34ddeaa7de77d60803ef8",
            "56b1e0019001442e9b754fe2ab6fd700",
            "f10d452bf3cc4662838ae9d9eea7e30f",
            "4cff07948ca54d8ca1d7691b67204c10",
            "b9089834ca1e40dd92be3668a54b8933"
          ]
        },
        "outputId": "18e2b9b1-8fca-40d4-e766-bb5da7f93b4f"
      },
      "execution_count": 512,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text : None<|endoftext|>\n",
            "inputs_len : 24\n",
            "generated text : None<|endoftext|>\n",
            "inputs_len : 14\n",
            "generated text : <|endoftext|>\n",
            "inputs_len : 35\n",
            "generated text : None<|endoftext|>\n",
            "inputs_len : 36\n",
            "generated text : None<|endoftext|>\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip and delete/gpt2_2024-01-20 20:37_true_batch_size=1_true.tsv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85b528cc3d674592a1d75b31df5f4cac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed14a584911343688055c02d91acc62a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9c9022492664083975b929c9977e4a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc4822d09dc84cdebf95a55442af36c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer : AutoTokenizer.from_pretrained(gpt2-medium)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text : . It started when natural gas was[EXP]. It started when natural gas was imported from China. It started when natural gas was imported from China. It started when natural gas was started in China. it started in China. it started in China. it started in China. it started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in started in started\n",
            "inputs_len : 24\n",
            "generated text : \"McCain says he[EXP]\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McC\n",
            "inputs_len : 14\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 35\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 36\n",
            "generated text : .<|endoftext|>\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip and delete/gpt2-medium_2024-01-20 20:37_true_batch_size=1_true.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text :  the the the...... the..... the................................................................................................................\n",
            "inputs_len : 24\n",
            "generated text :  the the the the the.... the the the.... the the.... the.........................................................................................................\n",
            "inputs_len : 14\n",
            "generated text : , the the the............................................................................................................................\n",
            "inputs_len : 35\n",
            "generated text :  the the the. the the the.... the the the.... the the... the the... the...................................................................................................\n",
            "inputs_len : 36\n",
            "generated text :  the the the the.... the the.... the the..... the..........................................................................................................\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip and delete/gpt2_2024-01-20 20:37_true_batch_size=2_true.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2-medium)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text :  the[EXP] the first time around.[EXP] the[EXP] the[EXP] the[EXP] the second time[EXP] the[EXP] the first time[EXP] the second time the first time the second time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the\n",
            "inputs_len : 24\n",
            "generated text : \"[EXP]\"[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP] is the first time[EXP] is the first time[EXP], \"The\" is the \"The\" is the \"The\" is the \"The\" is the \"The \"The \"The \"The \"The \"The \"The \"The \"The \"The \"The \"The \"The \"The \"The \"The \"The \"The \"\n",
            "inputs_len : 14\n",
            "generated text :  the[EXP] that was[EXP] was[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I am not only[EXP] that I am that I am that I am that I am that I am that I am that I am that I am that I am that I am that I am that I am that I am that I am that I am that I am that I am that I am that I\n",
            "inputs_len : 35\n",
            "generated text :  that span of time.[EXP] that span of time,[EXP] that span of time, the Bears have[EXP] that span of time[EXP] that span of that span of time of that span of time of that span of that time of that span of that time of that span of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of\n",
            "inputs_len : 36\n",
            "generated text : .[EXP].[EXP].[EXP].[EXP]. I[EXP]. I[EXP], I[EXP], I[EXP], I[EXP], I[EXP] that's[EXP],[EXP],[EXP],[EXP],[EXP],[EXP],[EXP],[EXP],[EXP],[EXP],[EXP],[EXP],[EXP], I[EXP],[EXP]. I[EXP],[EXP], I[EXP], I[EXP],[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP]\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip and delete/gpt2-medium_2024-01-20 20:37_true_batch_size=2_true.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text : None of the, the, the, the, the, the, the, the, the,, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the\n",
            "inputs_len : 16\n",
            "generated text : None of the the the the the the<|endoftext|>\n",
            "inputs_len : 18\n",
            "generated text : None of the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the,\n",
            "inputs_len : 28\n",
            "generated text : None<|endoftext|>\n",
            "inputs_len : 23\n",
            "generated text : None<|endoftext|>\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip and delete/gpt2_2024-01-20 20:37_false_batch_size=1_false.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2-medium)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text :  abortion[EXP] abortion[EXP] abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion\n",
            "inputs_len : 16\n",
            "generated text : , by[EXP], by, by, by, by, by, by, by, by, by, by, by, by, by, by, by, by, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a,\n",
            "inputs_len : 18\n",
            "generated text :  his[EXP] he has[EXP] he has[EXP] he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he[EXP] he has he has he has he has he has he has he has he has he[EXP] he has he has he has he has he has he has he has he has he[EXP]\n",
            "inputs_len : 28\n",
            "generated text :  the rate of growth of[EXP] the[EXP] the rate of growth of growth of growth of[EXP] that growth of growth of[EXP] that growth[EXP] that growth[EXP] that growth[EXP] that we[EXP] that[EXP] that[EXP] that[EXP] that we[EXP] that we[EXP] that[EXP] that we[EXP] that we[EXP] that we[EXP] that we[EXP] that we[EXP] that we[EXP] is[EXP] that we are[EXP] that we are[EXP] that we are[EXP] that we are[EXP] that we are[EXP] that we are that we are that we are that we are that we are that we are that we are that we are that we are that we are that we are that we are that we\n",
            "inputs_len : 23\n",
            "generated text :  the[EXP] the[EXP] is[EXP] the[EXP] is[EXP] the[EXP] is[EXP] is[EXP] is[EXP] is[EXP] is[EXP] is[EXP] is[EXP] is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip and delete/gpt2-medium_2024-01-20 20:37_false_batch_size=1_false.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text : . said. said. said. said. said. said. said. said. said. said.[EXP]. said. said. said. said. said. said. said. said.[EXP][EXP][EXP] said[EXP] said said said said said said said said said said said said said said said said[EXP] said said said said said[EXP] said[EXP][EXP][EXP] said said[EXP][EXP] said[EXP][EXP][EXP][EXP] said said said said[EXP] said[EXP][EXP][EXP][EXP] said[EXP] said said said said said[EXP][EXP][EXP][EXP][EXP][EXP] said[EXP] said said[EXP] said said said said said[EXP] said[EXP][EXP] said[EXP][EXP][EXP][EXP][EXP][EXP] said said[EXP][EXP]\n",
            "inputs_len : 16\n",
            "generated text :  said in a statement. \"We are committed[EXP] to make sure that women are treated with care that is consistent[EXP] with their own health care needs, and that they are treated with care that is consistent with their own health care needs, and that they are treated with care that is consistent[EXP] with their own health care needs, and that they are treated with care that is consistent with their own health care needs, and that they are treated with care that is consistent[EXP] with their own health care needs, and that they are treated[EXP] with care that is consistent with their own health care needs, and that they are treated with care that[EXP] is\n",
            "inputs_len : 18\n",
            "generated text :  has been in the district for years. he has been in the district for years. he has been in the district for years. he has been in the district for years. he has been in the district for years. he has been in the district for years[EXP] he has been in the district for years. he has been in the district for years. he has been in the district for years. he has been in the district for years. he has been in the district for years. he has been in the district for years. he has been in the district for years. he has been in the district for years. he has been\n",
            "inputs_len : 28\n",
            "generated text :  said that we would increase taxes on the middle class. that we would increase taxes on the middle class. that we would increase taxes on the middle class. that we would increase taxes on the middle class. that we would increase taxes on the middle class. that we would increase taxes on the middle class. that we would increase taxes on the middle class[EXP] that we would increase taxes on[EXP] middle class. that we would increase taxes[EXP] on the middle class. that we would increase taxes on the middle class. that we would increase taxes on the middle class. that we would increase taxes on the middle class. that we would increase taxes on\n",
            "inputs_len : 23\n",
            "generated text :  has already been waived or otherwise suspended.                                                                                                                        \n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip and delete/gpt2_2024-01-20 20:37_false_batch_size=2_false.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2-medium)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text : , says the group supports third[EXP] the abortions on demand. says the group supports third the abortions on[EXP]. says the group supports third the abortions on demand. says the group supports third the abortions on demand. says the group supports third the abortions on[EXP]. says the group supports third the abortions on demand. says the group supports third the abortions on demand. says the group supports third the abortions on demand. says the group supports third the abortions[EXP]. says the group supports third the abortions on demand. says the group supports third the abortions on demand. says[EXP]. says the group supports third the abortions on demand. says the group supports\n",
            "inputs_len : 16\n",
            "generated text : . Health care reform legislation is likely[EXP] is likely to mandate free sex change procedures. Health care reform legislation is likely to mandate free sex change procedures. Health care reform legislation is likely[EXP]. Health care reform legislation is likely. Health care reform legislation is likely. Health care reform legislation is likely. Health care reform legislation is likely. Health care reform legislation is likely. Health care reform legislation is likely. Health care reform legislation is likely. Health care reform legislation is likely. Health care reform legislation is likely. Health care reform legislation[EXP]. Health care reform legislation is likely. Health care reform legislation is likely[EXP]. Health care[EXP]. Health care\n",
            "inputs_len : 18\n",
            "generated text : . He has[EXP]. he has. he has. he has. he has. he has. he has[EXP]. he has. he[EXP]. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he[EXP]. he. he. he[EXP]. he. he. he. he. he. he. he. he. he.\n",
            "inputs_len : 28\n",
            "generated text :  of our government.[EXP] the rate of growth of growth of our government has been reduced. The rate of growth of our government has been reduced. The rate of our government has been reduced. The rate of our government has been reduced. The rate of our government has been reduced. The rate of our government has been reduced. The rate of our government has been reduced. The rate of our[EXP] has been reduced[EXP] has[EXP] the rate of our government has been reduced[EXP] has been reduced[EXP] has been reduced. has been reduced. has[EXP] has[EXP] has[EXP] the rate of our government has been reduced. has[EXP] has been reduced. has\n",
            "inputs_len : 23\n",
            "generated text :  that[EXP] is[EXP] the[EXP] the Affordable Care Act[EXP].<|endoftext|>\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip and delete/gpt2-medium_2024-01-20 20:37_false_batch_size=2_false.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# statementのみ\n",
        "\n",
        "time = '2024-01-24 13:17 s only'\n",
        "\n",
        "# config_dict\n",
        "ckpt_dict_true = {\n",
        "    'gpt2-small-bs1' : f'{time}/true_batch_size=1',\n",
        "    'gpt2-small-bs2' : f'{time}/true_batch_size=2',\n",
        "    'gpt2-medium-bs1' : f'{time}/true_batch_size=1',\n",
        "    'gpt2-medium-bs2' : f'{time}/true_batch_size=2',\n",
        "    't5-small-bs1' : f'{time}/true_batch_size=1',\n",
        "    't5-small-bs2' : f'{time}/true_batch_size=2',\n",
        "    't5-base-bs1' : f'{time}/true_batch_size=1',\n",
        "    't5-base-bs2' : f'{time}/true_batch_size=2',\n",
        "    'bart-base-bs1' : f'{time}/true_batch_size=1',\n",
        "    'bart-base-bs2' : f'{time}/true_batch_size=2',\n",
        "}\n",
        "\n",
        "ckpt_dict_false = {\n",
        "    'gpt2-small-bs1' : f'{time}/false_batch_size=1',\n",
        "    'gpt2-small-bs2' : f'{time}/false_batch_size=2',\n",
        "    'gpt2-medium-bs1' : f'{time}/false_batch_size=1',\n",
        "    'gpt2-medium-bs2' : f'{time}/false_batch_size=2',\n",
        "    't5-small-bs1' : f'{time}/false_batch_size=1',\n",
        "    't5-small-bs2' : f'{time}/false_batch_size=2',\n",
        "    't5-base-bs1' : f'{time}/false_batch_size=1',\n",
        "    't5-base-bs2' : f'{time}/false_batch_size=2',\n",
        "    'bart-base-bs1' : f'{time}/false_batch_size=1',\n",
        "    'bart-base-bs2' : f'{time}/false_batch_size=2',\n",
        "}\n",
        "\n",
        "gpt2_small_config_for_prediction2 = {\n",
        "    'MODEL_NAME' : 'gpt2',\n",
        "    'TOKENIZER_NAME' : 'gpt2',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/small',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'small'],\n",
        "    'INPUT_FLIP' : 0\n",
        "}\n",
        "\n",
        "# GPT2-medium\n",
        "gpt2_medium_config_for_prediction2 = {\n",
        "    'MODEL_NAME' : 'gpt2-medium',\n",
        "    'TOKENIZER_NAME' : 'gpt2-medium',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/medium',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'medium'],\n",
        "    'INPUT_FLIP' : 0\n",
        "}\n",
        "\n",
        "true_gpt2_bs1        = predict_CEG(gpt2_small_config_for_prediction2, 'gpt2', train_data_true[:5], ckpt_dict_true['gpt2-small-bs1'], 'train', 'true', 1, time)\n",
        "true_gpt2_medium_bs1 = predict_CEG(gpt2_medium_config_for_prediction2, 'gpt2-medium', train_data_true[:5], ckpt_dict_true['gpt2-medium-bs1'], 'train', 'true', 1, time)\n",
        "\n",
        "true_gpt2_bs2        = predict_CEG(gpt2_small_config_for_prediction2, 'gpt2', train_data_true[:5], ckpt_dict_true['gpt2-small-bs2'], 'train', 'true', 1, time)\n",
        "true_gpt2_medium_bs2 = predict_CEG(gpt2_medium_config_for_prediction2, 'gpt2-medium', train_data_true[:5], ckpt_dict_true['gpt2-medium-bs2'], 'train', 'true', 1, time)\n",
        "\n",
        "false_gpt2_bs1        = predict_CEG(gpt2_small_config_for_prediction2, 'gpt2', train_data_false[:5], ckpt_dict_false['gpt2-small-bs1'], 'train', 'false', 1, time)\n",
        "false_gpt2_medium_bs1 = predict_CEG(gpt2_medium_config_for_prediction2, 'gpt2-medium', train_data_false[:5], ckpt_dict_false['gpt2-medium-bs1'], 'train', 'false', 1, time)\n",
        "\n",
        "false_gpt2_bs2        = predict_CEG(gpt2_small_config_for_prediction2, 'gpt2', train_data_false[:5], ckpt_dict_false['gpt2-small-bs2'], 'train', 'false', 1, time)\n",
        "false_gpt2_medium_bs2 = predict_CEG(gpt2_medium_config_for_prediction2, 'gpt2-medium', train_data_false[:5], ckpt_dict_false['gpt2-medium-bs2'], 'train', 'false', 1, time)\n"
      ],
      "metadata": {
        "id": "JYUqAAKnocUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72daf8c7-7695-4312-ca43-caf9318bdf98"
      },
      "execution_count": 513,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text : None<|endoftext|>\n",
            "inputs_len : 24\n",
            "generated text : None<|endoftext|>\n",
            "inputs_len : 14\n",
            "generated text : <|endoftext|>\n",
            "inputs_len : 35\n",
            "generated text : None<|endoftext|>\n",
            "inputs_len : 36\n",
            "generated text : None<|endoftext|>\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip and delete/gpt2_2024-01-20 20:37_true_batch_size=1_true.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2-medium)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text : . It started when natural gas was[EXP]. It started when natural gas was imported from China. It started when natural gas was imported from China. It started when natural gas was started in China. it started in China. it started in China. it started in China. it started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in China. started in started in started\n",
            "inputs_len : 24\n",
            "generated text : \"McCain says he[EXP]\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McCain says\"McC\n",
            "inputs_len : 14\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 35\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 36\n",
            "generated text : .<|endoftext|>\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip and delete/gpt2-medium_2024-01-20 20:37_true_batch_size=1_true.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text :  the the the...... the..... the................................................................................................................\n",
            "inputs_len : 24\n",
            "generated text :  the the the the the.... the the the.... the the.... the.........................................................................................................\n",
            "inputs_len : 14\n",
            "generated text : , the the the............................................................................................................................\n",
            "inputs_len : 35\n",
            "generated text :  the the the. the the the.... the the the.... the the... the the... the...................................................................................................\n",
            "inputs_len : 36\n",
            "generated text :  the the the the.... the the.... the the..... the..........................................................................................................\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip and delete/gpt2_2024-01-20 20:37_true_batch_size=2_true.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2-medium)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text :  the[EXP] the first time around.[EXP] the[EXP] the[EXP] the[EXP] the second time[EXP] the[EXP] the first time[EXP] the second time the first time the second time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the time the\n",
            "inputs_len : 24\n",
            "generated text : \"[EXP]\"[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP]The[EXP] is the first time[EXP] is the first time[EXP], \"The\" is the \"The\" is the \"The\" is the \"The\" is the \"The \"The \"The \"The \"The \"The \"The \"The \"The \"The \"The \"The \"The \"The \"The \"The \"The \"The \"\n",
            "inputs_len : 14\n",
            "generated text :  the[EXP] that was[EXP] was[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I[EXP] that I am not only[EXP] that I am that I am that I am that I am that I am that I am that I am that I am that I am that I am that I am that I am that I am that I am that I am that I am that I am that I am that I\n",
            "inputs_len : 35\n",
            "generated text :  that span of time.[EXP] that span of time,[EXP] that span of time, the Bears have[EXP] that span of time[EXP] that span of that span of time of that span of time of that span of that time of that span of that time of that span of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of that of\n",
            "inputs_len : 36\n",
            "generated text : .[EXP].[EXP].[EXP].[EXP]. I[EXP]. I[EXP], I[EXP], I[EXP], I[EXP], I[EXP] that's[EXP],[EXP],[EXP],[EXP],[EXP],[EXP],[EXP],[EXP],[EXP],[EXP],[EXP],[EXP],[EXP], I[EXP],[EXP]. I[EXP],[EXP], I[EXP], I[EXP],[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP], I[EXP]\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip and delete/gpt2-medium_2024-01-20 20:37_true_batch_size=2_true.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text : None of the, the, the, the, the, the, the, the, the,, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the\n",
            "inputs_len : 16\n",
            "generated text : None of the the the the the the<|endoftext|>\n",
            "inputs_len : 18\n",
            "generated text : None of the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the,\n",
            "inputs_len : 28\n",
            "generated text : None<|endoftext|>\n",
            "inputs_len : 23\n",
            "generated text : None<|endoftext|>\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip and delete/gpt2_2024-01-20 20:37_false_batch_size=1_false.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2-medium)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text :  abortion[EXP] abortion[EXP] abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion abortion\n",
            "inputs_len : 16\n",
            "generated text : , by[EXP], by, by, by, by, by, by, by, by, by, by, by, by, by, by, by, by, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a,\n",
            "inputs_len : 18\n",
            "generated text :  his[EXP] he has[EXP] he has[EXP] he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he has he[EXP] he has he has he has he has he has he has he has he has he[EXP] he has he has he has he has he has he has he has he has he[EXP]\n",
            "inputs_len : 28\n",
            "generated text :  the rate of growth of[EXP] the[EXP] the rate of growth of growth of growth of[EXP] that growth of growth of[EXP] that growth[EXP] that growth[EXP] that growth[EXP] that we[EXP] that[EXP] that[EXP] that[EXP] that we[EXP] that we[EXP] that[EXP] that we[EXP] that we[EXP] that we[EXP] that we[EXP] that we[EXP] that we[EXP] is[EXP] that we are[EXP] that we are[EXP] that we are[EXP] that we are[EXP] that we are[EXP] that we are that we are that we are that we are that we are that we are that we are that we are that we are that we are that we are that we are that we\n",
            "inputs_len : 23\n",
            "generated text :  the[EXP] the[EXP] is[EXP] the[EXP] is[EXP] the[EXP] is[EXP] is[EXP] is[EXP] is[EXP] is[EXP] is[EXP] is[EXP] is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip and delete/gpt2-medium_2024-01-20 20:37_false_batch_size=1_false.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text : . said. said. said. said. said. said. said. said. said. said.[EXP]. said. said. said. said. said. said. said. said.[EXP][EXP][EXP] said[EXP] said said said said said said said said said said said said said said said said[EXP] said said said said said[EXP] said[EXP][EXP][EXP] said said[EXP][EXP] said[EXP][EXP][EXP][EXP] said said said said[EXP] said[EXP][EXP][EXP][EXP] said[EXP] said said said said said[EXP][EXP][EXP][EXP][EXP][EXP] said[EXP] said said[EXP] said said said said said[EXP] said[EXP][EXP] said[EXP][EXP][EXP][EXP][EXP][EXP] said said[EXP][EXP]\n",
            "inputs_len : 16\n",
            "generated text :  said in a statement. \"We are committed[EXP] to make sure that women are treated with care that is consistent[EXP] with their own health care needs, and that they are treated with care that is consistent with their own health care needs, and that they are treated with care that is consistent[EXP] with their own health care needs, and that they are treated with care that is consistent with their own health care needs, and that they are treated with care that is consistent[EXP] with their own health care needs, and that they are treated[EXP] with care that is consistent with their own health care needs, and that they are treated with care that[EXP] is\n",
            "inputs_len : 18\n",
            "generated text :  has been in the district for years. he has been in the district for years. he has been in the district for years. he has been in the district for years. he has been in the district for years. he has been in the district for years[EXP] he has been in the district for years. he has been in the district for years. he has been in the district for years. he has been in the district for years. he has been in the district for years. he has been in the district for years. he has been in the district for years. he has been in the district for years. he has been\n",
            "inputs_len : 28\n",
            "generated text :  said that we would increase taxes on the middle class. that we would increase taxes on the middle class. that we would increase taxes on the middle class. that we would increase taxes on the middle class. that we would increase taxes on the middle class. that we would increase taxes on the middle class. that we would increase taxes on the middle class[EXP] that we would increase taxes on[EXP] middle class. that we would increase taxes[EXP] on the middle class. that we would increase taxes on the middle class. that we would increase taxes on the middle class. that we would increase taxes on the middle class. that we would increase taxes on\n",
            "inputs_len : 23\n",
            "generated text :  has already been waived or otherwise suspended.                                                                                                                        \n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip and delete/gpt2_2024-01-20 20:37_false_batch_size=2_false.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2-medium)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text : , says the group supports third[EXP] the abortions on demand. says the group supports third the abortions on[EXP]. says the group supports third the abortions on demand. says the group supports third the abortions on demand. says the group supports third the abortions on[EXP]. says the group supports third the abortions on demand. says the group supports third the abortions on demand. says the group supports third the abortions on demand. says the group supports third the abortions[EXP]. says the group supports third the abortions on demand. says the group supports third the abortions on demand. says[EXP]. says the group supports third the abortions on demand. says the group supports\n",
            "inputs_len : 16\n",
            "generated text : . Health care reform legislation is likely[EXP] is likely to mandate free sex change procedures. Health care reform legislation is likely to mandate free sex change procedures. Health care reform legislation is likely[EXP]. Health care reform legislation is likely. Health care reform legislation is likely. Health care reform legislation is likely. Health care reform legislation is likely. Health care reform legislation is likely. Health care reform legislation is likely. Health care reform legislation is likely. Health care reform legislation is likely. Health care reform legislation is likely. Health care reform legislation[EXP]. Health care reform legislation is likely. Health care reform legislation is likely[EXP]. Health care[EXP]. Health care\n",
            "inputs_len : 18\n",
            "generated text : . He has[EXP]. he has. he has. he has. he has. he has. he has[EXP]. he has. he[EXP]. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he. he[EXP]. he. he. he[EXP]. he. he. he. he. he. he. he. he. he.\n",
            "inputs_len : 28\n",
            "generated text :  of our government.[EXP] the rate of growth of growth of our government has been reduced. The rate of growth of our government has been reduced. The rate of our government has been reduced. The rate of our government has been reduced. The rate of our government has been reduced. The rate of our government has been reduced. The rate of our government has been reduced. The rate of our[EXP] has been reduced[EXP] has[EXP] the rate of our government has been reduced[EXP] has been reduced[EXP] has been reduced. has been reduced. has[EXP] has[EXP] has[EXP] the rate of our government has been reduced. has[EXP] has been reduced. has\n",
            "inputs_len : 23\n",
            "generated text :  that[EXP] is[EXP] the[EXP] the Affordable Care Act[EXP].<|endoftext|>\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip and delete/gpt2-medium_2024-01-20 20:37_false_batch_size=2_false.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 以降メモ"
      ],
      "metadata": {
        "id": "ljIKzJBCXzur"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIcmjm_ShvOG"
      },
      "source": [
        "# GPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebbsKHDktRwJ"
      },
      "source": [
        "## 推論"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9VQsfvyI7Gw"
      },
      "outputs": [],
      "source": [
        "## modelのロード\n",
        "my_model = CEG.load_from_checkpoint(gpt2_small_config['SAVED_MODEL_PATH']+'/QTag-epoch=20-val_loss=7.58.ckpt')\n",
        "my_model.model.save_pretrained('./model_transformers/gpt2_small')\n",
        "my_model_from_pretrained = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/gpt2_small')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUo_aStv4pZA"
      },
      "outputs": [],
      "source": [
        "my_model_from_pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAR1j9UE6y_W"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, AutoConfig, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/gpt2_small')\n",
        "\n",
        "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
        "input_length = len(inputs[\"input_ids\"][0])\n",
        "print(f\"inputs_len : {input_length}\")\n",
        "\n",
        "# greedy_search\n",
        "# outputs = model.generate(**inputs, max_new_tokens=15, return_dict_in_generate=True, output_scores=True)\n",
        "# beam_search\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=5,\n",
        "    num_beams=4,\n",
        "    num_return_sequences=1,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        ")\n",
        "print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "tokens_list = outputs.sequences[0]\n",
        "print(tokens_list)\n",
        "full_text = tokenizer.decode(tokens_list)\n",
        "generated = tokenizer.decode(generated_tokens[0])\n",
        "\n",
        "print(f\"full text : {full_text}\\n\")\n",
        "print(f\"generated text: {generated}\\n\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2_lpCRDuYqo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "outputId": "0835a782-3cdd-4b57-c697-8f0be46a34b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there is no information. you need to call get_tokenizer() first.\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : gpt2\n",
            "self.tokenizer : GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['[EXP]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t50257: AddedToken(\"[EXP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : 50258\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : <|endoftext|>\n",
            "self.eos_token_id : 50256\n",
            "self.pad_token : <|endoftext|>\n",
            "self.pad_token_id : 50256\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : 50257\n",
            "inputs_len : 33\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'my_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-b994aee1b337>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"inputs_len : {input_length}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m outputs = my_model.generate(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'my_model' is not defined"
          ]
        }
      ],
      "source": [
        "## tokenzierの準備\n",
        "my_prepare_tokenizer = Prepare_Tokenizer(gpt2_small_config)\n",
        "my_prepare_tokenizer.get_tokenizer_info()\n",
        "my_tokenizer = my_prepare_tokenizer.get_tokenizer()\n",
        "my_prepare_tokenizer.get_tokenizer_info()\n",
        "\n",
        "# データの準備\n",
        "inputs = my_tokenizer([\"Statement: Building a wall on the U.S.-Mexico border will take literally years. Metadata: immigration rick-perry Governor Texas republican Radio interview[EXP]\"], return_tensors=\"pt\")\n",
        "input_length = len(inputs[\"input_ids\"][0])\n",
        "print(f\"inputs_len : {input_length}\")\n",
        "\n",
        "outputs = my_model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=5,\n",
        "    num_beams=4,\n",
        "    num_return_sequences=1,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        ")\n",
        "print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "tokens_list = outputs.sequences[0]\n",
        "print(tokens_list)\n",
        "full_text = my_tokenizer.decode(tokens_list)\n",
        "generated = my_tokenizer.decode(generated_tokens[0])\n",
        "\n",
        "print(f\"full text : {full_text}\\n\")\n",
        "print(f\"generated text: {generated}\\n\")\n",
        "print(f\"len(generated) : {len(my_tokenizer.tokenize(generated))}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9On7S6T12TbR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bai7yul1huXr"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, AutoConfig, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
        "input_length = len(inputs[\"input_ids\"][0])\n",
        "print(f\"inputs_len : {input_length}\")\n",
        "\n",
        "# greedy_search\n",
        "# outputs = model.generate(**inputs, max_new_tokens=15, return_dict_in_generate=True, output_scores=True)\n",
        "# beam_search\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=5,\n",
        "    num_beams=4,\n",
        "    num_return_sequences=1,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        ")\n",
        "print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "tokens_list = outputs.sequences[0]\n",
        "print(tokens_list)\n",
        "full_text = tokenizer.decode(tokens_list)\n",
        "generated = tokenizer.decode(generated_tokens[0])\n",
        "\n",
        "print(f\"full text : {full_text}\\n\")\n",
        "print(f\"generated text: {generated}\\n\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRkLjp0h73VM"
      },
      "source": [
        "# T5デモ\n",
        "- `<extra_id_0>`や`</s>`が出力に含まれてしまっている\n",
        "- lm_head.weightsを初期化してもあまり効果が無い\n",
        "- https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py\n",
        "- https://www.kaggle.com/code/kreeshrajani/fine-tune-t5-for-conversational-model\n",
        "- https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Tokenizer.sp_model_kwargs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CSsvMsOyROy"
      },
      "source": [
        "## 公式サイトより"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WZvyBATyUEI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# training\n",
        "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
        "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
        "outputs = model(input_ids=input_ids, labels=labels)\n",
        "loss = outputs.loss\n",
        "logits = outputs.logits\n",
        "\n",
        "# inference\n",
        "input_ids = tokenizer(\n",
        "    \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
        ").input_ids  # Batch size 1\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "# studies have shown that owning a dog is good for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncxz-bFVuQmS"
      },
      "source": [
        "## fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2IFfkHWg5jg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05177341-3a13-4ab9-9705-42012a744a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before\n",
            "['</s>', '<unk>', '<pad>', '<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']\n",
            "[1, 2, 0, 32099, 32098, 32097, 32096, 32095, 32094, 32093, 32092, 32091, 32090, 32089, 32088, 32087, 32086, 32085, 32084, 32083, 32082, 32081, 32080, 32079, 32078, 32077, 32076, 32075, 32074, 32073, 32072, 32071, 32070, 32069, 32068, 32067, 32066, 32065, 32064, 32063, 32062, 32061, 32060, 32059, 32058, 32057, 32056, 32055, 32054, 32053, 32052, 32051, 32050, 32049, 32048, 32047, 32046, 32045, 32044, 32043, 32042, 32041, 32040, 32039, 32038, 32037, 32036, 32035, 32034, 32033, 32032, 32031, 32030, 32029, 32028, 32027, 32026, 32025, 32024, 32023, 32022, 32021, 32020, 32019, 32018, 32017, 32016, 32015, 32014, 32013, 32012, 32011, 32010, 32009, 32008, 32007, 32006, 32005, 32004, 32003, 32002, 32001, 32000]\n",
            "After\n",
            "['</s>', '<unk>', '<pad>', '[EXP]']\n",
            "[1, 2, 0, 32100]\n",
            "exp_id : 32100\n",
            "eos_id : 1\n",
            "pad_id : 0\n",
            "\n",
            "len(tokenizer) : 32101\n",
            "before : torch.Size([32128, 512])\n",
            "before resizing : Parameter containing:\n",
            "tensor([[ -2.0156,   0.2236,  -7.0938,  ...,  -0.3535,   2.6406,  -2.8906],\n",
            "        [ 12.6250,   8.1875, -11.6250,  ...,   7.9375,  -7.3125,   0.9453],\n",
            "        [ -8.7500,   7.1875,  27.8750,  ..., -26.7500,   0.8555,  -1.5156],\n",
            "        ...,\n",
            "        [-25.2500, -28.5000, -17.2500,  ..., -17.7500,  -5.2500,  27.3750],\n",
            "        [-25.5000, -29.3750, -18.2500,  ..., -17.7500,  -4.8125,  27.7500],\n",
            "        [-26.7500, -28.3750, -17.8750,  ..., -18.5000,  -7.0000,  27.6250]],\n",
            "       requires_grad=True)\n",
            "after :  torch.Size([32101, 512])\n",
            "after resizing : torch.Size([32101, 512])\n",
            "before changing weights : torch.Size([32101, 512])\n",
            "focus : tensor([[ -2.0156,   0.2236,  -7.0938,  ...,  -0.3535,   2.6406,  -2.8906],\n",
            "        [ 12.6250,   8.1875, -11.6250,  ...,   7.9375,  -7.3125,   0.9453],\n",
            "        [ -8.7500,   7.1875,  27.8750,  ..., -26.7500,   0.8555,  -1.5156],\n",
            "        ...,\n",
            "        [-13.1875,   5.2188, -18.0000,  ...,  14.0625,  19.3750,  -0.2422],\n",
            "        [-15.4375,   8.7500,   6.9688,  ...,   6.7500,   4.2812,  -0.4199],\n",
            "        [  7.8438,   8.5625,  -4.3750,  ...,   0.6719,  -2.4688,  -4.1562]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "focus.shape : torch.Size([32100, 512])\n",
            "after changing weights : Parameter containing:\n",
            "tensor([[-2.0156e+00,  2.2363e-01, -7.0938e+00,  ..., -3.5352e-01,\n",
            "          2.6406e+00, -2.8906e+00],\n",
            "        [ 1.2625e+01,  8.1875e+00, -1.1625e+01,  ...,  7.9375e+00,\n",
            "         -7.3125e+00,  9.4531e-01],\n",
            "        [-8.7500e+00,  7.1875e+00,  2.7875e+01,  ..., -2.6750e+01,\n",
            "          8.5547e-01, -1.5156e+00],\n",
            "        ...,\n",
            "        [-1.5438e+01,  8.7500e+00,  6.9688e+00,  ...,  6.7500e+00,\n",
            "          4.2812e+00, -4.1992e-01],\n",
            "        [ 7.8438e+00,  8.5625e+00, -4.3750e+00,  ...,  6.7188e-01,\n",
            "         -2.4688e+00, -4.1562e+00],\n",
            "        [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
            "         -1.0000e+04, -1.0000e+04]], requires_grad=True)\n",
            "new_weights.shape : torch.Size([32101, 512])\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "\n",
        "print(\"Before\")\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "special_tokens_dict = {\n",
        "    'additional_special_tokens': ['[EXP]']\n",
        "    }\n",
        "\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "exp_id = tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "eos_id = tokenizer.convert_tokens_to_ids('</s>')\n",
        "pad_id = tokenizer.convert_tokens_to_ids('<pad>')\n",
        "print(\"After\")\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "print(f\"exp_id : {exp_id}\")\n",
        "print(f\"eos_id : {eos_id}\")\n",
        "print(f\"pad_id : {pad_id}\\n\")\n",
        "print(f\"len(tokenizer) : {len(tokenizer)}\")\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "print(f\"before : {model.lm_head.weight.shape}\")\n",
        "print(f\"before resizing : {model.lm_head.weight}\")\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(\"after : \", model.lm_head.weight.shape)\n",
        "print(f\"after resizing : {model.lm_head.weight.shape}\")\n",
        "print(f\"before changing weights : {model.lm_head.weight.shape}\")\n",
        "print(f\"focus : {model.lm_head.weight[:-1, :]}\")\n",
        "print(f\"focus.shape : {model.lm_head.weight[:-1, :].shape}\")\n",
        "new_weights = torch.cat([model.lm_head.weight[:-1, :], torch.zeros(1, model.lm_head.weight.shape[1]) -10000])\n",
        "model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "print(f\"after changing weights : {model.lm_head.weight}\")\n",
        "print(f\"new_weights.shape : {model.lm_head.weight.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eFQyzQEhVTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21344255-27f1-4bb4-d03b-aa38d92dcb96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized input_text : ['▁Statement', ':', '▁When', '▁did', '▁the', '▁decline', '▁of', '▁coal', '▁start', '?', '▁It', '▁started', '▁when', '▁natural', '▁gas', '▁took', '▁off', '▁that', '▁started', '▁to', '▁begin', '▁in', '▁(', 'P', 'resident', '▁George', '▁W', '.', ')', '▁Bush', 's', '▁administration', '.', '▁Meta', 'data', ':', '▁energy', ',', 'his', 'tory', ',', 'job', '-', 'acco', 'mp', 'l', 'ish', 'ments', '▁', 's', 'cott', '-', 'sur', 'o', 've', 'll', '▁State', '▁de', 'legate', '▁Virginia', '▁', 'democrat', '▁', 'a', '▁floor', '▁speech', '.', '[EXP]']\n",
            "\n",
            "tokenized label_text : ['▁Statement', ':', '▁When', '▁did', '▁the', '▁decline', '▁of', '▁coal', '▁start', '?', '▁It', '▁started', '▁when', '▁natural', '▁gas', '▁took', '▁off', '▁that', '▁started', '▁to', '▁begin', '▁in', '▁(', 'P', 'resident', '▁George', '▁W', '.', ')', '▁Bush', 's', '▁administration', '.', '▁Meta', 'data', ':', '▁energy', ',', 'his', 'tory', ',', 'job', '-', 'acco', 'mp', 'l', 'ish', 'ments', '▁', 's', 'cott', '-', 'sur', 'o', 've', 'll', '▁State', '▁de', 'legate', '▁Virginia', '▁', 'democrat', '▁', 'a', '▁floor', '▁speech', '.', '[EXP]', '▁Sur', 'o', 've', 'll', '▁said', '▁the', '▁decline', '▁of', '▁coal', '▁\"', 'star', 'ted', '▁when', '▁natural', '▁gas', '▁took', '▁off', '▁That', '▁started', '▁to', '▁begin', '▁in', '▁President', '▁(', 'George', '▁W', '.', '▁', ')', '▁Bush', 's', '▁administration', '.', '▁\"', 'No', '▁doubt', ',', '▁natural', '▁gas', '▁has', '▁been', '▁', 'gaining', '▁ground', '▁on', '▁coal', '▁in', '▁', 'generating', '▁electricity', '.', '▁The', '▁trend', '▁started', '▁in', '▁the', '▁1990', 's', '▁but', '▁clearly', '▁gained', '▁speed', '▁during', '▁the', '▁Bush', '▁administration', '▁when', '▁the', '▁production', '▁of', '▁natural', '▁gas', '▁--', '▁', 'a', '▁competitor', '▁of', '▁coal', '▁--', '▁picked', '▁up', '.', '▁But', '▁analysts', '▁give', '▁little', '▁credit', '▁or', '▁blame', '▁to', '▁Bush', '▁for', '▁that', '▁trend', '.', '▁They', '▁note', '▁that', '▁other', '▁factors', ',', '▁such', '▁as', '▁technological', '▁innovation', ',', '▁', 'entrepreneurship', '▁and', '▁policies', '▁of', '▁previous', '▁administration', 's', ',', '▁had', '▁more', '▁to', '▁do', '▁with', '▁', 'laying', '▁the', '▁ground', 'work', '▁for', '▁the', '▁natural', '▁gas', '▁boom', '.', '</s>']\n",
            "input_text_length : 68\n",
            "input_encoding : {'input_ids': tensor([[16836,    10,   366,   410,     8,  7198,    13,  8416,   456,    58,\n",
            "            94,   708,   116,   793,  1807,   808,   326,    24,   708,    12,\n",
            "          1731,    16,    41,   345, 15704,  3080,   549,     5,    61,  8905,\n",
            "             7,  3602,     5, 14204,  6757,    10,   827,     6, 10193, 10972,\n",
            "             6, 16899,    18, 21007,  1167,    40,  1273,  4128,     3,     7,\n",
            "         10405,    18,  3042,    32,   162,   195,  1015,    20,  8791,  5382,\n",
            "             3, 23319,     3,     9,  1501,  5023,     5, 32100,     1,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "\n",
            "label_encoding : {'input_ids': tensor([[16836,    10,   366,   410,     8,  7198,    13,  8416,   456,    58,\n",
            "            94,   708,   116,   793,  1807,   808,   326,    24,   708,    12,\n",
            "          1731,    16,    41,   345, 15704,  3080,   549,     5,    61,  8905,\n",
            "             7,  3602,     5, 14204,  6757,    10,   827,     6, 10193, 10972,\n",
            "             6, 16899,    18, 21007,  1167,    40,  1273,  4128,     3,     7,\n",
            "         10405,    18,  3042,    32,   162,   195,  1015,    20,  8791,  5382,\n",
            "             3, 23319,     3,     9,  1501,  5023,     5, 32100,  3705,    32,\n",
            "           162,   195,   243,     8,  7198,    13,  8416,    96,  3624,  1054,\n",
            "           116,   793,  1807,   808,   326,   466,   708,    12,  1731,    16,\n",
            "          1661,    41, 31317,   549,     5,     3,    61,  8905,     7,  3602,\n",
            "             5,    96,  4168,  3228,     6,   793,  1807,    65,   118,     3,\n",
            "         11866,  1591,    30,  8416,    16,     3, 11600,  6373,     5,    37,\n",
            "          4166,   708,    16,     8,  5541,     7,    68,  3133,  6886,  1634,\n",
            "           383,     8,  8905,  3602,   116,     8,   999,    13,   793,  1807,\n",
            "          1636,     3,     9, 18766,    13,  8416,  1636,  4758,    95,     5,\n",
            "           299, 15639,   428,   385,   998,    42,  9100,    12,  8905,    21,\n",
            "            24,  4166,     5,   328,  2232,    24,   119,  2580,     6,   224,\n",
            "            38,  9974,  4337,     6,     3, 24849,    11,  3101,    13,  1767,\n",
            "          3602,     7,     6,   141,    72,    12,   103,    28,     3, 14720,\n",
            "             8,  1591,  1981,    21,     8,   793,  1807, 13997,     5,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "input prompt : Statement: When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration. Metadata: energy,history,job-accomplishments scott-surovell State delegate Virginia democrat a floor speech.[EXP]</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "label : Statement: When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration. Metadata: energy,history,job-accomplishments scott-surovell State delegate Virginia democrat a floor speech.[EXP] Surovell said the decline of coal \"started when natural gas took off That started to begin in President (George W. ) Bushs administration. \"No doubt, natural gas has been gaining ground on coal in generating electricity. The trend started in the 1990s but clearly gained speed during the Bush administration when the production of natural gas -- a competitor of coal -- picked up. But analysts give little credit or blame to Bush for that trend. They note that other factors, such as technological innovation, entrepreneurship and policies of previous administrations, had more to do with laying the groundwork for the natural gas boom.</s>\n",
            "labels before -100 masking : tensor([[16836,    10,   366,   410,     8,  7198,    13,  8416,   456,    58,\n",
            "            94,   708,   116,   793,  1807,   808,   326,    24,   708,    12,\n",
            "          1731,    16,    41,   345, 15704,  3080,   549,     5,    61,  8905,\n",
            "             7,  3602,     5, 14204,  6757,    10,   827,     6, 10193, 10972,\n",
            "             6, 16899,    18, 21007,  1167,    40,  1273,  4128,     3,     7,\n",
            "         10405,    18,  3042,    32,   162,   195,  1015,    20,  8791,  5382,\n",
            "             3, 23319,     3,     9,  1501,  5023,     5, 32100,  3705,    32,\n",
            "           162,   195,   243,     8,  7198,    13,  8416,    96,  3624,  1054,\n",
            "           116,   793,  1807,   808,   326,   466,   708,    12,  1731,    16,\n",
            "          1661,    41, 31317,   549,     5,     3,    61,  8905,     7,  3602,\n",
            "             5,    96,  4168,  3228,     6,   793,  1807,    65,   118,     3,\n",
            "         11866,  1591,    30,  8416,    16,     3, 11600,  6373,     5,    37,\n",
            "          4166,   708,    16,     8,  5541,     7,    68,  3133,  6886,  1634,\n",
            "           383,     8,  8905,  3602,   116,     8,   999,    13,   793,  1807,\n",
            "          1636,     3,     9, 18766,    13,  8416,  1636,  4758,    95,     5,\n",
            "           299, 15639,   428,   385,   998,    42,  9100,    12,  8905,    21,\n",
            "            24,  4166,     5,   328,  2232,    24,   119,  2580,     6,   224,\n",
            "            38,  9974,  4337,     6,     3, 24849,    11,  3101,    13,  1767,\n",
            "          3602,     7,     6,   141,    72,    12,   103,    28,     3, 14720,\n",
            "             8,  1591,  1981,    21,     8,   793,  1807, 13997,     5,     1]])\n",
            "labels after -100 masking : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  3705,    32,\n",
            "           162,   195,   243,     8,  7198,    13,  8416,    96,  3624,  1054,\n",
            "           116,   793,  1807,   808,   326,   466,   708,    12,  1731,    16,\n",
            "          1661,    41, 31317,   549,     5,     3,    61,  8905,     7,  3602,\n",
            "             5,    96,  4168,  3228,     6,   793,  1807,    65,   118,     3,\n",
            "         11866,  1591,    30,  8416,    16,     3, 11600,  6373,     5,    37,\n",
            "          4166,   708,    16,     8,  5541,     7,    68,  3133,  6886,  1634,\n",
            "           383,     8,  8905,  3602,   116,     8,   999,    13,   793,  1807,\n",
            "          1636,     3,     9, 18766,    13,  8416,  1636,  4758,    95,     5,\n",
            "           299, 15639,   428,   385,   998,    42,  9100,    12,  8905,    21,\n",
            "            24,  4166,     5,   328,  2232,    24,   119,  2580,     6,   224,\n",
            "            38,  9974,  4337,     6,     3, 24849,    11,  3101,    13,  1767,\n",
            "          3602,     7,     6,   141,    72,    12,   103,    28,     3, 14720,\n",
            "             8,  1591,  1981,    21,     8,   793,  1807, 13997,     5,     1]])\n",
            "labels.shape : torch.Size([1, 200])\n"
          ]
        }
      ],
      "source": [
        "input_text = 'Statement: When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration. Metadata: energy,history,job-accomplishments scott-surovell State delegate Virginia democrat a floor speech.'+ tokenizer.convert_ids_to_tokens(exp_id)\n",
        "justification_text = 'Surovell said the decline of coal \"started when natural gas took off  That started to begin in President (George W. ) Bushs administration. \"No doubt, natural gas has been gaining ground on coal in generating electricity. The trend started in the 1990s but clearly gained speed during the Bush administration when the production of natural gas -- a competitor of coal -- picked up. But analysts give little credit or blame to Bush for that trend. They note that other factors, such as technological innovation, entrepreneurship and policies of previous administrations, had more to do with laying the groundwork for the natural gas boom.'\n",
        "label_text = input_text  + justification_text + tokenizer.convert_ids_to_tokens(eos_id)\n",
        "print(f\"tokenized input_text : {tokenizer.tokenize(input_text)}\\n\")\n",
        "print(f\"tokenized label_text : {tokenizer.tokenize(label_text)}\")\n",
        "input_text_length = len(tokenizer.tokenize(input_text))\n",
        "justification_text_length = len(tokenizer.tokenize(justification_text))\n",
        "label_text_length = len(tokenizer.tokenize(label_text))\n",
        "print(f\"input_text_length : {input_text_length}\")\n",
        "\n",
        "input_encoding = tokenizer.encode_plus(\n",
        "    input_text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=label_text_length,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "\n",
        "print(f\"input_encoding : {input_encoding}\\n\")\n",
        "decoded_input_encoding = tokenizer.decode(input_encoding[\"input_ids\"][0])\n",
        "# print(f\"decoded input_encoding : {decoded_input_encoding}\\n\")\n",
        "\n",
        "label_encoding = tokenizer.encode_plus(\n",
        "    label_text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=label_text_length,\n",
        "    return_token_type_ids=False,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "\n",
        "print(f\"label_encoding : {label_encoding}\")\n",
        "# print(f\"label_encoding['input_ids'][0] : {label_encoding['input_ids'][0]}\")\n",
        "decoded_label_encoding = tokenizer.decode(label_encoding[\"input_ids\"][0])\n",
        "# print(f\"decoded label_encoding : {decoded_label_encoding}\")\n",
        "print(f\"input prompt : {decoded_input_encoding}\")\n",
        "print(f\"label : {decoded_label_encoding}\")\n",
        "\n",
        "labels = label_encoding[\"input_ids\"]\n",
        "print(f\"labels before -100 masking : {labels}\")\n",
        "# decoded_labels = tokenizer.decode(labels)\n",
        "# print(f\"decoded labels : {decoded_labels}\")\n",
        "labels[:, :input_text_length] = -100 # cross_entropy_ignore_index\n",
        "labels[:, label_text_length:] = -100\n",
        "print(f\"labels after -100 masking : {labels}\")\n",
        "print(f\"labels.shape : {labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYIrs850a-UJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf6349cd-5a36-4dd3-fabb-a162479d409e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model : T5ForConditionalGeneration(\n",
            "  (shared): Embedding(32101, 512)\n",
            "  (encoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32101, 512)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 8)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
            "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-5): 5 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
            "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (decoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32101, 512)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 8)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
            "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-5): 5 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
            "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=512, out_features=32101, bias=False)\n",
            ")\n",
            "type(model) : <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n",
            "type(outputs) : <class 'transformers.modeling_outputs.Seq2SeqLMOutput'>\n",
            "logits : tensor([[[-2.5476e+01, -1.2861e+01, -1.7625e+01,  ..., -1.3640e+01,\n",
            "          -3.2110e+00,  1.0486e+04],\n",
            "         [-2.5476e+01, -1.2861e+01, -1.7625e+01,  ..., -1.3640e+01,\n",
            "          -3.2110e+00,  1.0486e+04],\n",
            "         [-2.5476e+01, -1.2861e+01, -1.7625e+01,  ..., -1.3640e+01,\n",
            "          -3.2110e+00,  1.0486e+04],\n",
            "         ...,\n",
            "         [-5.4455e+01, -4.7042e+00, -2.2898e+01,  ..., -2.8213e+01,\n",
            "          -3.0636e+01,  1.3202e+04],\n",
            "         [-5.0904e+01, -5.3404e+00, -2.2956e+01,  ..., -2.7929e+01,\n",
            "          -2.8834e+01,  1.3415e+04],\n",
            "         [-4.8555e+01, -5.0367e+00, -2.2519e+01,  ..., -2.7969e+01,\n",
            "          -2.8701e+01,  1.3469e+04]]], grad_fn=<UnsafeViewBackward0>)\n",
            "logtis.size() : torch.Size([1, 200, 32101])\n",
            "loss : 6978.9580078125\n",
            "output : tensor([[[1.2420e-05, 1.1084e-06, 7.3320e-07,  ..., 2.0707e-04,\n",
            "          1.4493e-02, 0.0000e+00],\n",
            "         [1.2420e-05, 1.1084e-06, 7.3320e-07,  ..., 2.0707e-04,\n",
            "          1.4493e-02, 0.0000e+00],\n",
            "         [1.2420e-05, 1.1084e-06, 7.3320e-07,  ..., 2.0707e-04,\n",
            "          1.4493e-02, 0.0000e+00],\n",
            "         ...,\n",
            "         [3.2251e-18, 3.8631e-03, 3.7582e-09,  ..., 9.7075e-11,\n",
            "          1.7817e-14, 0.0000e+00],\n",
            "         [1.1244e-16, 2.0448e-03, 3.5469e-09,  ..., 1.2893e-10,\n",
            "          1.0790e-13, 0.0000e+00],\n",
            "         [1.1777e-15, 2.7704e-03, 5.4941e-09,  ..., 1.2380e-10,\n",
            "          1.2329e-13, 0.0000e+00]]], grad_fn=<SoftmaxBackward0>)\n",
            "output.size() : torch.Size([1, 200, 32101])\n",
            "argmax(ouput) : tensor([[32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 23591,\n",
            "         19828,   195,  1015,    10, 24409,    13,  8416,  3511, 12416,  1054,\n",
            "         23562,   410,  1807,   808,   326,   793,   708,    12,  1731,    16,\n",
            "            41,  3080,   345,   549,     5,    61, 12703,  8905,  7289, 10030,\n",
            "             5,  7243, 10555,  8052, 16919,  2199, 24436, 14428,   582,  8154,\n",
            "         31036, 15290, 15362,  6089,  6490, 15777, 27470,     1,  4471, 26958,\n",
            "          7512,  3256,   116, 22965, 15559, 17774,     1,  1703,     1, 27721,\n",
            "         13830, 20417,     1,  6863,     1, 10538, 27512,    13, 30390,  5556,\n",
            "         16646, 31942, 17560, 22587, 18955,     1, 10762, 28448,    95,     1,\n",
            "         30752, 17945, 28270, 26147,  2736,     1, 15620,     1,  1792,    31,\n",
            "             1,    58,     1, 26539,  9098,     1, 25562,  2580,     1,     1,\n",
            "           587,     1, 14500,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1, 32100,     1,     1,     1]])\n",
            "argmax(ouput).size() : torch.Size([1, 200])\n",
            "argmax(output).dtype() : torch.int64\n",
            "tensor([[32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 23591,\n",
            "         19828,   195,  1015,    10, 24409,    13,  8416,  3511, 12416,  1054,\n",
            "         23562,   410,  1807,   808,   326,   793,   708,    12,  1731,    16,\n",
            "            41,  3080,   345,   549,     5,    61, 12703,  8905,  7289, 10030,\n",
            "             5,  7243, 10555,  8052, 16919,  2199, 24436, 14428,   582,  8154,\n",
            "         31036, 15290, 15362,  6089,  6490, 15777, 27470,     1,  4471, 26958,\n",
            "          7512,  3256,   116, 22965, 15559, 17774,     1,  1703,     1, 27721,\n",
            "         13830, 20417,     1,  6863,     1, 10538, 27512,    13, 30390,  5556,\n",
            "         16646, 31942, 17560, 22587, 18955,     1, 10762, 28448,    95,     1,\n",
            "         30752, 17945, 28270, 26147,  2736,     1, 15620,     1,  1792,    31,\n",
            "             1,    58,     1, 26539,  9098,     1, 25562,  2580,     1,     1,\n",
            "           587,     1, 14500,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1, 32100,     1,     1,     1]])\n",
            "generated : <extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0>posonsvettell State: hallway of coal startsdidted 1/2\" did gas took off natural started to begin in ( GeorgeP W.)7) Bushsburg regime.azăWhenthinglessly although gases flows become experiencingbooming momentum clearance behalf suggestssistedianu</s> suppliesinformatiileodor continues when Brig 1930′</s>cher</s> eyebrowively parliament</s> Administration</s> lemn déclin of zahărgas pumpscostingoyeznnouncing thereof</s> puts senzati up</s> investitii yeah speculate speeches detail</s>deal</s> avoid'</s>?</s> Haftung relate</s> pahar factors</s></s> wie</s> advancement</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>[EXP]</s></s></s>\n",
            "len(generated) : 203\n"
          ]
        }
      ],
      "source": [
        "outputs = model(input_ids=input_encoding[\"input_ids\"], attention_mask=input_encoding[\"attention_mask\"], labels=labels)\n",
        "\n",
        "print(f\"model : {model}\")\n",
        "print(f\"type(model) : {type(model)}\")\n",
        "print(f\"type(outputs) : {type(outputs)}\")\n",
        "\n",
        "logits = outputs.logits\n",
        "loss = outputs.loss\n",
        "print(f\"logits : {logits}\")\n",
        "print(f\"logtis.size() : {logits.size()}\")\n",
        "print(f\"loss : {loss}\")\n",
        "\n",
        "m = nn.Softmax(dim=1)\n",
        "output = m(logits)\n",
        "print(f\"output : {output}\")\n",
        "print(f\"output.size() : {output.size()}\")\n",
        "arg = torch.argmax(output, dim=2)\n",
        "print(f\"argmax(ouput) : {arg}\") # --> tensor\n",
        "print(f\"argmax(ouput).size() : {arg.size()}\")\n",
        "print(f\"argmax(output).dtype() : {arg.dtype}\")\n",
        "print(arg)\n",
        "generated = tokenizer.decode(arg[0])\n",
        "print(f\"generated : {generated}\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNWyFtg7uWzL"
      },
      "source": [
        "## 推論"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k20XUiAPuPB9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "inputs = tokenizer([\"summarize: Today is\"], return_tensors=\"pt\")\n",
        "input_length = len(inputs[\"input_ids\"][0])\n",
        "print(f\"inputs_len : {input_length}\")\n",
        "\n",
        "# greedy_search\n",
        "outputs = model.generate(**inputs, max_new_tokens=15, return_dict_in_generate=True, output_scores=True)\n",
        "# beam_search\n",
        "# outputs = model.generate(\n",
        "#     **inputs,\n",
        "#     max_new_tokens=5,\n",
        "#     num_beams=4,\n",
        "#     num_return_sequences=1,\n",
        "#     return_dict_in_generate=True,\n",
        "#     output_scores=True,\n",
        "# )\n",
        "print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "tokens_list = outputs.sequences[0]\n",
        "print(tokens_list)\n",
        "full_text = tokenizer.decode(tokens_list)\n",
        "generated = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"full text : {full_text}\\n\")\n",
        "print(f\"generated text: {generated}\\n\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoDbJrtWKA8k"
      },
      "source": [
        "# BART\n",
        "- add_special_token=Trueにしたとき、先頭に`<s>`が追加されるため-100のマスキング時にinput_length+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H9d_ni0KDdO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "print(\"Before\")\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "special_tokens_dict = {\n",
        "    'additional_special_tokens': ['[EXP]']\n",
        "    }\n",
        "\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "exp_id = tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "eos_id = tokenizer.convert_tokens_to_ids('</s>')\n",
        "pad_id = tokenizer.convert_tokens_to_ids('<pad>')\n",
        "print(\"After\")\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "print(f\"exp_id : {exp_id}\")\n",
        "print(f\"eos_id : {eos_id}\")\n",
        "print(f\"pad_id : {pad_id}\\n\")\n",
        "print(f\"len(tokenizer) : {len(tokenizer)}\")\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
        "# print(f\"before : {model.lm_head.weight.shape}\")\n",
        "# print(f\"before resizing : {model.lm_head.weight}\")\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# print(\"after : \", model.lm_head.weight.shape)\n",
        "# print(f\"after resizing : {model.lm_head.weight.shape}\")\n",
        "# print(f\"before changing weights : {model.lm_head.weight.shape}\")\n",
        "# print(f\"focus : {model.lm_head.weight[:-1, :]}\")\n",
        "# print(f\"focus.shape : {model.lm_head.weight[:-1, :].shape}\")\n",
        "# new_weights = torch.cat([model.lm_head.weight[:-1, :], torch.zeros(1, model.lm_head.weight.shape[1]) -10000])\n",
        "# model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "# print(f\"after changing weights : {model.lm_head.weight}\")\n",
        "# print(f\"new_weights.shape : {model.lm_head.weight.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS-7CkYAKTI0"
      },
      "outputs": [],
      "source": [
        "input_text = 'Statement: When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration. Metadata: energy,history,job-accomplishments scott-surovell State delegate Virginia democrat a floor speech.'+ tokenizer.convert_ids_to_tokens(exp_id)\n",
        "justification_text = 'Surovell said the decline of coal \"started when natural gas took off  That started to begin in President (George W. ) Bushs administration. \"No doubt, natural gas has been gaining ground on coal in generating electricity. The trend started in the 1990s but clearly gained speed during the Bush administration when the production of natural gas -- a competitor of coal -- picked up. But analysts give little credit or blame to Bush for that trend. They note that other factors, such as technological innovation, entrepreneurship and policies of previous administrations, had more to do with laying the groundwork for the natural gas boom.'\n",
        "label_text = input_text  + justification_text + tokenizer.convert_ids_to_tokens(eos_id)\n",
        "print(f\"tokenized input_text : {tokenizer.tokenize(input_text)}\\n\")\n",
        "print(f\"tokenized label_text : {tokenizer.tokenize(label_text)}\")\n",
        "input_text_length = len(tokenizer.tokenize(input_text))\n",
        "justification_text_length = len(tokenizer.tokenize(justification_text))\n",
        "label_text_length = len(tokenizer.tokenize(label_text))\n",
        "print(f\"input_text_length : {input_text_length}\")\n",
        "\n",
        "input_encoding = tokenizer.encode_plus(\n",
        "    input_text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=label_text_length,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "\n",
        "print(f\"input_encoding : {input_encoding}\\n\")\n",
        "decoded_input_encoding = tokenizer.decode(input_encoding[\"input_ids\"][0])\n",
        "# print(f\"decoded input_encoding : {decoded_input_encoding}\\n\")\n",
        "\n",
        "label_encoding = tokenizer.encode_plus(\n",
        "    label_text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=label_text_length,\n",
        "    return_token_type_ids=False,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "\n",
        "print(f\"label_encoding : {label_encoding}\")\n",
        "# print(f\"label_encoding['input_ids'][0] : {label_encoding['input_ids'][0]}\")\n",
        "decoded_label_encoding = tokenizer.decode(label_encoding[\"input_ids\"][0])\n",
        "# print(f\"decoded label_encoding : {decoded_label_encoding}\")\n",
        "print(f\"input prompt : {decoded_input_encoding}\")\n",
        "print(f\"label : {decoded_label_encoding}\")\n",
        "\n",
        "labels = label_encoding[\"input_ids\"]\n",
        "print(f\"labels before -100 masking : {labels}\")\n",
        "# decoded_labels = tokenizer.decode(labels)\n",
        "# print(f\"decoded labels : {decoded_labels}\")\n",
        "labels[:, :input_text_length+1] = -100 # cross_entropy_ignore_index\n",
        "labels[:, label_text_length:] = -100\n",
        "print(f\"labels after -100 masking : {labels}\")\n",
        "print(f\"labels.shape : {labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fy4pRgFbKYRU"
      },
      "outputs": [],
      "source": [
        "outputs = model(input_ids=input_encoding[\"input_ids\"], attention_mask=input_encoding[\"attention_mask\"], labels=labels)\n",
        "\n",
        "print(f\"model : {model}\")\n",
        "print(f\"type(model) : {type(model)}\")\n",
        "print(f\"type(outputs) : {type(outputs)}\")\n",
        "\n",
        "logits = outputs.logits\n",
        "loss = outputs.loss\n",
        "print(f\"logits : {logits}\")\n",
        "print(f\"logtis.size() : {logits.size()}\")\n",
        "print(f\"loss : {loss}\")\n",
        "\n",
        "m = nn.Softmax(dim=1)\n",
        "output = m(logits)\n",
        "print(f\"output : {output}\")\n",
        "print(f\"output.size() : {output.size()}\")\n",
        "arg = torch.argmax(output, dim=2)\n",
        "print(f\"argmax(ouput) : {arg}\") # --> tensor\n",
        "print(f\"argmax(ouput).size() : {arg.size()}\")\n",
        "print(f\"argmax(output).dtype() : {arg.dtype}\")\n",
        "print(arg)\n",
        "generated = tokenizer.decode(arg[0])\n",
        "print(f\"generated : {generated}\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ_6sUIJtXs5"
      },
      "source": [
        "## 推論"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOaCTJIdtZ_R"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP6H2OdCuZ1tWbNH1ugPTLi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dc8e6502ca524d56b850fb907f2db571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53b96e0a9ddc404ba0951cd97b9a955c",
              "IPY_MODEL_e2d739710f5449bc9ae2a1b9f4c72390",
              "IPY_MODEL_6b298d9a51d5495ba37d60522f63631d"
            ],
            "layout": "IPY_MODEL_5db6968e683e45b5be0ac029dbbb6d2b"
          }
        },
        "53b96e0a9ddc404ba0951cd97b9a955c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33860f83168a4a50afae2d5d68324cd6",
            "placeholder": "​",
            "style": "IPY_MODEL_90273627371a4e3ba12886a285ea1256",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "e2d739710f5449bc9ae2a1b9f4c72390": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ab71d209e0042718b4a2779d6525974",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5ffb6b6912e423d81a3939df1483f78",
            "value": 2
          }
        },
        "6b298d9a51d5495ba37d60522f63631d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17edcb814d6748cbb45c114955a30667",
            "placeholder": "​",
            "style": "IPY_MODEL_e6720586bf5546a49c062cb0c6467136",
            "value": " 2/2 [00:25&lt;00:00,  0.08it/s]"
          }
        },
        "5db6968e683e45b5be0ac029dbbb6d2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "33860f83168a4a50afae2d5d68324cd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90273627371a4e3ba12886a285ea1256": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ab71d209e0042718b4a2779d6525974": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5ffb6b6912e423d81a3939df1483f78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17edcb814d6748cbb45c114955a30667": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6720586bf5546a49c062cb0c6467136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf2ea8ca8ed445e7b04aa41cd79386b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_780af9616aff427eb6efdaf7237909f2",
              "IPY_MODEL_5c86a7747f7841aca00493973c6708bb",
              "IPY_MODEL_c230e4433cc44daba470d60cc7b28070"
            ],
            "layout": "IPY_MODEL_a89780dfc78448cdbcd1bde0a75fb28c"
          }
        },
        "780af9616aff427eb6efdaf7237909f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59919f1f1ec54976978b656c109b2282",
            "placeholder": "​",
            "style": "IPY_MODEL_578cfcd81d7f4cc19cc034dd6a869327",
            "value": "Epoch 0:   1%"
          }
        },
        "5c86a7747f7841aca00493973c6708bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00178289c1054b7cb9f4d4affd1621a6",
            "max": 5752,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_413e580643074e629c2927541652837a",
            "value": 57
          }
        },
        "c230e4433cc44daba470d60cc7b28070": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_028bc788895b4939b36e9a77fc2ba21b",
            "placeholder": "​",
            "style": "IPY_MODEL_75f4e4ec02c142339374b1eb0febaebe",
            "value": " 57/5752 [01:07&lt;1:52:22,  0.84it/s, v_num=4, train_loss=13.70]"
          }
        },
        "a89780dfc78448cdbcd1bde0a75fb28c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "59919f1f1ec54976978b656c109b2282": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "578cfcd81d7f4cc19cc034dd6a869327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00178289c1054b7cb9f4d4affd1621a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "413e580643074e629c2927541652837a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "028bc788895b4939b36e9a77fc2ba21b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75f4e4ec02c142339374b1eb0febaebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b5b875c76fd400c9d78e1aff0732e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e288f2c151ca419d9cf4198d1d4849e2",
              "IPY_MODEL_713a07cdc7644e4a8ea6677e2f218632",
              "IPY_MODEL_4a3cf79ff8aa4bdeaf205c5275dd3304"
            ],
            "layout": "IPY_MODEL_c7a1094731594f0f871ad1ffe5833189"
          }
        },
        "e288f2c151ca419d9cf4198d1d4849e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76851d531d1640f39ac092513421f84a",
            "placeholder": "​",
            "style": "IPY_MODEL_9473b58e0e2548c296576bfe6d5e450c",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "713a07cdc7644e4a8ea6677e2f218632": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07922ef591cd4edab36b910915ada873",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56c6af83517047abae2175f9fb8f2aed",
            "value": 2
          }
        },
        "4a3cf79ff8aa4bdeaf205c5275dd3304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42a79fbc356d466990485fd812e55920",
            "placeholder": "​",
            "style": "IPY_MODEL_0b608772164d46bfbbca458c06dab360",
            "value": " 2/2 [00:21&lt;00:00,  0.09it/s]"
          }
        },
        "c7a1094731594f0f871ad1ffe5833189": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "76851d531d1640f39ac092513421f84a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9473b58e0e2548c296576bfe6d5e450c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07922ef591cd4edab36b910915ada873": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56c6af83517047abae2175f9fb8f2aed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42a79fbc356d466990485fd812e55920": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b608772164d46bfbbca458c06dab360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3477e8a0433a4d289206f0ad98611c4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56657abfb5cc44eb85d9d74f6ece77de",
              "IPY_MODEL_d4709f5f2c2346a493f8c37ebbbe191b",
              "IPY_MODEL_b88efb638a664133922996911e38dd3d"
            ],
            "layout": "IPY_MODEL_27f7399f836046e694e5f4c5517b2ed2"
          }
        },
        "56657abfb5cc44eb85d9d74f6ece77de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c897186058e4367b5c149c537752283",
            "placeholder": "​",
            "style": "IPY_MODEL_c8e0ef82fb634235a8b276a6950b5c43",
            "value": "Epoch 0:   0%"
          }
        },
        "d4709f5f2c2346a493f8c37ebbbe191b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d80f8dd02f274b8d86534c2a994bdf20",
            "max": 5752,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30c49dd30213446eba7029e2fca4068e",
            "value": 12
          }
        },
        "b88efb638a664133922996911e38dd3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaf9ff85e31448d88922a33deb019502",
            "placeholder": "​",
            "style": "IPY_MODEL_a8de57c923134ae3a98897fc5c2fa084",
            "value": " 12/5752 [00:15&lt;2:04:13,  0.77it/s, v_num=0, train_loss=13.30]"
          }
        },
        "27f7399f836046e694e5f4c5517b2ed2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "8c897186058e4367b5c149c537752283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8e0ef82fb634235a8b276a6950b5c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d80f8dd02f274b8d86534c2a994bdf20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30c49dd30213446eba7029e2fca4068e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aaf9ff85e31448d88922a33deb019502": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8de57c923134ae3a98897fc5c2fa084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85b528cc3d674592a1d75b31df5f4cac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1fd46f03cc941acac149df9b6e1f466",
              "IPY_MODEL_0b3c2f1cd33442adadbe1a9c79a88c72",
              "IPY_MODEL_0f2c615dcad14c7a875ac1a2f76e948a"
            ],
            "layout": "IPY_MODEL_9d5fb1289f46455ea81bdf57654db58c"
          }
        },
        "d1fd46f03cc941acac149df9b6e1f466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88efbe05fe8c4212b749d4ebb5d1d0ca",
            "placeholder": "​",
            "style": "IPY_MODEL_0ab491ed66244e3887ec7ed421857a02",
            "value": "config.json: 100%"
          }
        },
        "0b3c2f1cd33442adadbe1a9c79a88c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e1783b4b3bc4402be734a20a1622ae2",
            "max": 718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_579eec66b0784d54be5b53c162a6fd65",
            "value": 718
          }
        },
        "0f2c615dcad14c7a875ac1a2f76e948a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87780854080f48fbb3a5b3e4889446e1",
            "placeholder": "​",
            "style": "IPY_MODEL_2e9824bcc2e84880ab97614fa7ea0e23",
            "value": " 718/718 [00:00&lt;00:00, 34.0kB/s]"
          }
        },
        "9d5fb1289f46455ea81bdf57654db58c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88efbe05fe8c4212b749d4ebb5d1d0ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ab491ed66244e3887ec7ed421857a02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e1783b4b3bc4402be734a20a1622ae2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "579eec66b0784d54be5b53c162a6fd65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "87780854080f48fbb3a5b3e4889446e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e9824bcc2e84880ab97614fa7ea0e23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed14a584911343688055c02d91acc62a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b0077a65e2a4f45bb43d4285e9181d6",
              "IPY_MODEL_f87889a3083a46478bdde8f31917ff3c",
              "IPY_MODEL_6059c2b73962419f8f17445330154a13"
            ],
            "layout": "IPY_MODEL_4bfe0c276b234be3a487d9c5b15f2b58"
          }
        },
        "8b0077a65e2a4f45bb43d4285e9181d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd630fa908b54bd795102509c12ae023",
            "placeholder": "​",
            "style": "IPY_MODEL_d89c003bcc5f4d6c95a81001ee7774dd",
            "value": "vocab.json: 100%"
          }
        },
        "f87889a3083a46478bdde8f31917ff3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbb5310502594faaaf73988e537b8ab8",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ae6737e33004c7bad880aac673e1fca",
            "value": 1042301
          }
        },
        "6059c2b73962419f8f17445330154a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_963b266b4119478b9c1e53dda98e29f0",
            "placeholder": "​",
            "style": "IPY_MODEL_0cc44e80862742629edd7b8d26b308f6",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 1.34MB/s]"
          }
        },
        "4bfe0c276b234be3a487d9c5b15f2b58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd630fa908b54bd795102509c12ae023": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d89c003bcc5f4d6c95a81001ee7774dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbb5310502594faaaf73988e537b8ab8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ae6737e33004c7bad880aac673e1fca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "963b266b4119478b9c1e53dda98e29f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cc44e80862742629edd7b8d26b308f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9c9022492664083975b929c9977e4a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b50c150288df40e3b2190f0d4cf1109f",
              "IPY_MODEL_d40e091cba7846758d4ee302feb6a4cc",
              "IPY_MODEL_59ec0f063f72469a952f7fc394f8487c"
            ],
            "layout": "IPY_MODEL_9b19254422f34ed49cc1d0f4292169fd"
          }
        },
        "b50c150288df40e3b2190f0d4cf1109f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0256a74ed52f4519924140971863dcbe",
            "placeholder": "​",
            "style": "IPY_MODEL_0dfc2605cf8846c0bc0aafd14de62458",
            "value": "merges.txt: 100%"
          }
        },
        "d40e091cba7846758d4ee302feb6a4cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a0b8d597ebe42d3ae555e03b8de2d87",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_119e12dfd40c4c57998927f0be7df301",
            "value": 456318
          }
        },
        "59ec0f063f72469a952f7fc394f8487c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ba523e8fc7f4bc9a75e11932f0ea704",
            "placeholder": "​",
            "style": "IPY_MODEL_ea13fcf1c3664ef5bab28150b770fdf2",
            "value": " 456k/456k [00:00&lt;00:00, 29.5MB/s]"
          }
        },
        "9b19254422f34ed49cc1d0f4292169fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0256a74ed52f4519924140971863dcbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dfc2605cf8846c0bc0aafd14de62458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a0b8d597ebe42d3ae555e03b8de2d87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "119e12dfd40c4c57998927f0be7df301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ba523e8fc7f4bc9a75e11932f0ea704": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea13fcf1c3664ef5bab28150b770fdf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc4822d09dc84cdebf95a55442af36c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f5a6f6a67984e679050a752323cc70d",
              "IPY_MODEL_7100ac2213a74096b4ae9d3c9fe4f04f",
              "IPY_MODEL_c6e04b90778d43f9a690ee82b1f02971"
            ],
            "layout": "IPY_MODEL_e2ac3c96f3e342b9aba6875325d6fee8"
          }
        },
        "9f5a6f6a67984e679050a752323cc70d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69d3de89868d49049cd148f8d871ce29",
            "placeholder": "​",
            "style": "IPY_MODEL_1db8da5fe1f34ddeaa7de77d60803ef8",
            "value": "tokenizer.json: 100%"
          }
        },
        "7100ac2213a74096b4ae9d3c9fe4f04f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56b1e0019001442e9b754fe2ab6fd700",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f10d452bf3cc4662838ae9d9eea7e30f",
            "value": 1355256
          }
        },
        "c6e04b90778d43f9a690ee82b1f02971": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cff07948ca54d8ca1d7691b67204c10",
            "placeholder": "​",
            "style": "IPY_MODEL_b9089834ca1e40dd92be3668a54b8933",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 6.92MB/s]"
          }
        },
        "e2ac3c96f3e342b9aba6875325d6fee8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69d3de89868d49049cd148f8d871ce29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1db8da5fe1f34ddeaa7de77d60803ef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56b1e0019001442e9b754fe2ab6fd700": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f10d452bf3cc4662838ae9d9eea7e30f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4cff07948ca54d8ca1d7691b67204c10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9089834ca1e40dd92be3668a54b8933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}