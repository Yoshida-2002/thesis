{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yoshida-2002/thesis/blob/main/CEG_omo_on_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Lefh-Mu8KRcM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcbb8ba0-f489-44a8-e014-75d16accc7d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "aSs33HuiK1M6",
        "outputId": "6294a8d2-2d81-4de0-e376-beebfc98fc67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning\n",
            "' CEG omo on colab'           'data for CEG'     models                \u001b[0m\u001b[01;34mplot_figures\u001b[0m/\n",
            "' CEG omo on colab のコピー'   \u001b[01;34mlightning_logs\u001b[0m/  'models のコピー'      \u001b[01;34mresult_log\u001b[0m/\n",
            " \u001b[01;34mdata\u001b[0m/                         \u001b[01;34mmodel\u001b[0m/            \u001b[01;34mmodel_transformers\u001b[0m/   \u001b[01;34msrc\u001b[0m/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning\n",
        "%ls\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jRrJ4uFkLMCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8f76766-2495-4424-e1dc-ea178e3b62f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.1.3-py3-none-any.whl (777 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.7/777.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch-lightning) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.10.1 pytorch-lightning-2.1.3 torchmetrics-1.3.0.post0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pathを辞書にまとめて管理する"
      ],
      "metadata": {
        "id": "_fm0w0jc01Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_dict = {\n",
        "    # 生データのパス\n",
        "    'raw_train_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/train2.tsv',\n",
        "    'raw_test_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/test2.tsv',\n",
        "    'raw_val_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/val2.tsv',\n",
        "\n",
        "    # 欠損値処理を行った後のデータのパス\n",
        "    'missing_value_processed_train_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_train2.tsv',\n",
        "    'missing_value_processed_test_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_test2.tsv',\n",
        "    'missing_value_processed_val_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_val2.tsv',\n",
        "\n",
        "    # 欠損値処理を行った後、さらなる整形を行ったデータ\n",
        "    'input_for_lightning_model_train_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_train2.tsv',\n",
        "    'input_for_lightning_model_test_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_test2.tsv',\n",
        "    'input_for_lightning_model_val_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_val2.tsv',\n",
        "\n",
        "    # 集合に分割した後のデータ (main)\n",
        "    'main_input_for_lightning_model_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/train/true_set2.tsv',\n",
        "    'main_input_for_lightning_model_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/train/false_set2.tsv',\n",
        "    'main_input_for_lightning_model_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/test/true_set2.tsv',\n",
        "    'main_input_for_lightning_model_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/test/false_set2.tsv',\n",
        "    'main_input_for_lightning_model_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/val/true_set2.tsv',\n",
        "    'main_input_for_lightning_model_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/val/false_set2.tsv',\n",
        "\n",
        "    # 集合に分割した後のデータ (toy)\n",
        "    'toy_input_for_lightning_model_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/train/true_set2.tsv',\n",
        "    'toy_input_for_lightning_model_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/train/false_set2.tsv',\n",
        "    'toy_input_for_lightning_model_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/test/true_set2.tsv',\n",
        "    'toy_input_for_lightning_model_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/test/false_set2.tsv',\n",
        "    'toy_input_for_lightning_model_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/val/true_set2.tsv',\n",
        "    'toy_input_for_lightning_model_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/val/false_set2.tsv',\n",
        "\n",
        "    # モデルを保存するPath\n",
        "    'saved_model_path' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/src/models',\n",
        "    'TensorBoardLogger': 'lightning_logs',\n",
        "\n",
        "    # 拡張したデータの保存場所(main)\n",
        "    'main_extended_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/train/true_set2.tsv',\n",
        "    'main_extended_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/train/false_set2.tsv',\n",
        "    'main_extended_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/test/true_set2.tsv',\n",
        "    'main_extended_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/test/false_set2.tsv',\n",
        "    'main_extended_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/val/true_set2.tsv',\n",
        "    'main_extended_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/val/false_set2.tsv',\n",
        "\n",
        "\n",
        "    # 拡張したデータの保存場所(main)\n",
        "    'toy_extended_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/train/true_set2.tsv',\n",
        "    'toy_extended_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/train/false_set2.tsv',\n",
        "    'toy_extended_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/test/true_set2.tsv',\n",
        "    'toy_extended_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/test/false_set2.tsv',\n",
        "    'toy_extended_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/val/true_set2.tsv',\n",
        "    'toy_extended_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/val/false_set2.tsv',\n",
        "\n",
        "  }\n"
      ],
      "metadata": {
        "id": "VrElytlN06OI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-H8IgiRLeom"
      },
      "source": [
        "# data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2W3EWwDNM-t"
      },
      "source": [
        "## data_reader.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xDRiQFePNLVS"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"data_reader module is written for read files\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def read_csv(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "\n",
        "    :param path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "def read_tsv(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "\n",
        "    :param path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return pd.read_csv(path, sep=\"\\t\", header=None)\n",
        "\n",
        "\n",
        "def read_npy(path: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    load a list of numpy elements into memory\n",
        "    :param path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return np.load(path, allow_pickle=True)\n",
        "\n",
        "\n",
        "def read_excel(path:str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    :param path: where to load data from\n",
        "    :return: pd.DataFrame\n",
        "    \"\"\"\n",
        "    return pd.read_excel(path, engine=\"openpyxl\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sdEJztwNSgt"
      },
      "source": [
        "## data_writer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "od80SyiCNVaE"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"data_writer module is written for write data in files\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def write_npy(path: str, data: list) -> None:\n",
        "    \"\"\"\n",
        "    save a list of numpy elements into disk\n",
        "    :param path:\n",
        "    :param data:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return np.save(path, data, allow_pickle=True)\n",
        "\n",
        "def write_dataframe_in_tsv(data: pd.DataFrame, path: str) -> None:\n",
        "\n",
        "  \"\"\"\n",
        "  save pd.Dataframe in tsv file\n",
        "  :param path:\n",
        "  :param data:\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  data.to_csv(path, sep='\\t', index=False, header=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lROVJGpnfD8U"
      },
      "source": [
        "## data_missing_value_processor\n",
        "- 欠損値の処理を行う\n",
        "- やること\n",
        "  1. 全ての値がNaNの列を削除する\n",
        "  2. NaNを0で補完する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mFLX1BVOfi1v"
      },
      "outputs": [],
      "source": [
        "def data_missing_value_processor(data: pd.DataFrame, path: str) -> pd.DataFrame:\n",
        "\n",
        "  print('input data size : ' + str(len(data)))\n",
        "\n",
        "  data_without_invalid_row = data.dropna(how=\"all\")\n",
        "  print(str(len(data) - len(data_without_invalid_row)) + \" data was dropped because all of the values were NaN. \")\n",
        "\n",
        "  processed_data = data_without_invalid_row.fillna(0)\n",
        "  print(\"NaN in dataframe was filled with 0. \")\n",
        "  print(\"processed input data size : \" + str(len(processed_data)))\n",
        "  print(\"removed data size : \" + str(len(data) - len(processed_data)) )\n",
        "  print('\\n')\n",
        "\n",
        "  write_dataframe_in_tsv(processed_data, path)\n",
        "\n",
        "  return processed_data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phuOrNG7eh6i"
      },
      "source": [
        "##data_pre_processor_for_LIAR_PLUS_Dataset\n",
        "- LIAR_PLUS_Datasetクラスの入力に用いるdataframeを作成する\n",
        "- 作成したdataframeを保存する\n",
        "- statement, metadata, justification, credit_scoreの四つの列からなるdataframeを出力する\n",
        "- やること\n",
        "  1. 値として0(元々は欠損値)を持つ場合、\"None\"で補完する\n",
        "  2. 列を結合してstaetment, metadata, justification, credit_scoreを作成する\n",
        "  3. credit_scoreのnanを0.5で補完する\n",
        "  4. 作成したdataframeを保存する\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y5D6XvWcefqX"
      },
      "outputs": [],
      "source": [
        "def data_pre_processor_for_LIAR_PLUS_Dataset(data: pd.DataFrame, path: str) -> pd.DataFrame:\n",
        "\n",
        "  # 1. 値として0(元々は欠損値)を持つ場合、\"None\"で補完する\n",
        "\n",
        "  data[4].replace(0, 'None', inplace=True)\n",
        "  data[5].replace(0, 'None', inplace=True)\n",
        "  data[6].replace(0, 'None', inplace=True)\n",
        "  data[7].replace(0, 'None', inplace=True)\n",
        "  data[8].replace(0, 'None', inplace=True)\n",
        "  data[14].replace(0, 'None', inplace=True)\n",
        "  data[15].replace(0, 'None', inplace=True)\n",
        "\n",
        "  # 2. 列を結合してstaetment, metadata, justificationを作成する\n",
        "\n",
        "  data_for_LIAR_PLUS_Dataset = pd.DataFrame(\n",
        "      data = {\n",
        "      'id' : data[0],\n",
        "      'label' : data[2],\n",
        "      'statement' : data[3],\n",
        "      'metadata' : data[4]+ ' ' + data[5] + ' ' + data[6] + ' ' + data[7] + ' ' + data[8] + ' ' + data[14],\n",
        "      'justification' : data[15],\n",
        "      'credit_score' : (data[12]*0.2 + data[11]*0.5 + data[9]*0.75 + data[10]*0.9 + data[13]*1)/(data[12] + data[11] + data[9] + data[10] + data[13]),\n",
        "      'unique_id' : data[1]\n",
        "      })\n",
        "\n",
        "  data_for_LIAR_PLUS_Dataset['credit_score'] = data_for_LIAR_PLUS_Dataset['credit_score'].fillna(0.5)\n",
        "\n",
        "  write_dataframe_in_tsv(data_for_LIAR_PLUS_Dataset, path)\n",
        "\n",
        "  print(\"dataframe was saved in\" + path + \"\\n\")\n",
        "\n",
        "  data_for_LIAR_PLUS_Dataset.info()\n",
        "\n",
        "\n",
        "  return data_for_LIAR_PLUS_Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeBeWogIddoI"
      },
      "source": [
        "# dataset_separator_on_labels\n",
        "- プロンプトを作成する前にラベルで条件付けられた集合を作成\n",
        "- 名前以外はgenerated_prompt_separatorと同じ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mHRtX4hxdtCE"
      },
      "outputs": [],
      "source": [
        "def dataset_separator_on_labels(path: dict, data: pd.DataFrame) -> dict:\n",
        "\n",
        "  true_set = pd.DataFrame()\n",
        "  false_set = pd.DataFrame()\n",
        "  total_len = len(data)\n",
        "\n",
        "  for i in range(len(data)):\n",
        "\n",
        "    data_row = data.iloc[i]\n",
        "   # print(data_row)\n",
        "\n",
        "    if data_row[1] == \"true\":\n",
        "        true_set = true_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"half-true\":\n",
        "        true_set = true_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"mostly-true\":\n",
        "        true_set = true_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"false\":\n",
        "        false_set = false_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"barely-true\":\n",
        "        false_set = false_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"pants-fire\":\n",
        "        false_set = false_set.append(data_row, ignore_index=True)\n",
        "    else:\n",
        "        print(f\"Invalid label was found! {data_row[1]}\")\n",
        "\n",
        "  print(\"===================================================\\n\")\n",
        "  print(f\"total len : {total_len}\")\n",
        "  print(f\"true_set : {len(true_set)}\")\n",
        "  print(f\"false_set : {len(false_set)}\")\n",
        "  print(\"===================================================\\n\")\n",
        "\n",
        "  write_dataframe_in_tsv(true_set, path[\"true_set\"])\n",
        "  write_dataframe_in_tsv(false_set, path[\"false_set\"])\n",
        "\n",
        "\n",
        "  return dict(\n",
        "      true_set=true_set,\n",
        "      false_set=true_set\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbE1iUnbvV1w"
      },
      "source": [
        "# prompt_generator\n",
        "・プロンプトの作成を一任\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yghLEX3Uvc53"
      },
      "outputs": [],
      "source": [
        "# def prompt_generator(data: pd.DataFrame, path :str) -> pd.DataFrame :\n",
        "#   \"\"\"\n",
        "#   this function is for generating and saving prompts\n",
        "\n",
        "#   param  : stage(\"fine-tuning\" of \"completion\")\n",
        "#   param  : data(row data for prompt generation)\n",
        "#   param  : path(path to save generated promps dataframe)\n",
        "#   return : prompts(generated prompts)\n",
        "#   \"\"\"\n",
        "#   generated_prompts = \"Statement: \" + data[2] + \" Metadata: \" + data[3]\n",
        "\n",
        "#   prompts_df = pd.DataFrame(\n",
        "#       data = {\n",
        "#       'id' : data[0],\n",
        "#       'label' : data[1],\n",
        "#       'concat' : generated_prompts,\n",
        "#       'justification': data[4],\n",
        "#       'credit_score' : data[5],\n",
        "#       'unique_id' : data[6]\n",
        "#       })\n",
        "\n",
        "#   write_dataframe_in_tsv(prompts_df, path)\n",
        "\n",
        "#   return prompts_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XMKn8TNcYi1g"
      },
      "outputs": [],
      "source": [
        "# def prompt_generator(data: pd.DataFrame, path:str, config_dict:dict) -> pd.DataFrame :\n",
        "#   \"\"\"\n",
        "#   this function is for generating and saving prompts\n",
        "\n",
        "#   param  : stage(\"fine-tuning\" of \"completion\")\n",
        "#   param  : data(row data for prompt generation)\n",
        "#   param  : path(path to save generated promps dataframe)\n",
        "#   return : prompts(generated prompts)\n",
        "#   \"\"\"\n",
        "\n",
        "\n",
        "#   generated_prompts = ''\n",
        "\n",
        "#   if 'statement' in config_dict['PROMPT_COMPONENTS']:\n",
        "#     generated_prompts = 'Statement: ' + data[2]\n",
        "\n",
        "#   if 'metadata' in config_dict['PROMPT_COMPONENTS']:\n",
        "#     generated_prompts += \" Metadata: \" + data[3]\n",
        "\n",
        "#   prompts_df = pd.DataFrame(\n",
        "#       data = {\n",
        "#       'id' : data[0],\n",
        "#       'label' : data[1],\n",
        "#       'concat' : generated_prompts,\n",
        "#       'justification': data[4],\n",
        "#       'credit_score' : data[5],\n",
        "#       'unique_id' : data[6]\n",
        "#       })\n",
        "\n",
        "#   return prompts_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UubLg77lkNfo"
      },
      "source": [
        "# concatinated_inputs_generator\n",
        "- concatとfull-promptを作成し諸々の情報を返す\n",
        "- 例外処理のコードはまだ書いていない"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WVwpH7jzkvku"
      },
      "outputs": [],
      "source": [
        "def concatinated_inputs_generator(data: pd.Series, config_dict:dict, tokenizer, prepare_tokenizer) -> dict :\n",
        "\n",
        "  concatinated_input = ''\n",
        "  full_input = ''\n",
        "\n",
        "  if 'statement' in config_dict['PROMPT_COMPONENTS']:\n",
        "    concatinated_input = 'Statement: ' + data[2]\n",
        "\n",
        "  if 'metadata' in config_dict['PROMPT_COMPONENTS']:\n",
        "    concatinated_input += ' Metadata: ' + data[3]\n",
        "\n",
        "  concatinated_input += prepare_tokenizer.exp_token\n",
        "  concatinated_input_length = len(tokenizer.tokenize(concatinated_input))\n",
        "\n",
        "  if 'justification' in config_dict['PROMPT_COMPONENTS']:\n",
        "    full_input = concatinated_input + data[4] + prepare_tokenizer.eos_token\n",
        "\n",
        "  full_input_length = len(tokenizer.tokenize(full_input))\n",
        "\n",
        "  # bartではspecial_token<s>が文頭に付くため、その分だけ入力長が1大きくなる\n",
        "  if config_dict[\"MODEL_NAME\"] == 'facebook/bart-base':\n",
        "      concatinated_input_length = concatinated_input_length + 1\n",
        "\n",
        "  if concatinated_input_length > config_dict['MAX_LENGTH']:\n",
        "      concatinated_input_length = config_dict['MAX_LENGTH']\n",
        "\n",
        "  if full_input_length > config_dict['MAX_LENGTH']:\n",
        "      full_input_length = config_dict['MAX_LENGTH']\n",
        "\n",
        "\n",
        "  return concatinated_input, full_input, concatinated_input_length, full_input_length\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nlnS8xxD3yZv"
      },
      "outputs": [],
      "source": [
        "# train_prompt_path = \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts/main/train_prompts.tsv\"\n",
        "# test_prompt_path = \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts/main/test_prompts.tsv\"\n",
        "# val_prompt_path = \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts/main/val_prompts.tsv\"\n",
        "\n",
        "# train_prompts = read_tsv(\"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/main/input_train2.tsv\")\n",
        "# test_prompts = read_tsv(\"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/main/input_test2.tsv\")\n",
        "# val_prompts = read_tsv(\"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/main/input_val2.tsv\")\n",
        "\n",
        "# train_prompts = prompt_generator(train_prompts, train_prompt_path)\n",
        "# test_prompts = prompt_generator(test_prompts, test_prompt_path)\n",
        "# val_prompts = prompt_generator(val_prompts, val_prompt_path)\n",
        "\n",
        "# print(len(train_prompts))\n",
        "# train_prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pQTF6thEEeYI"
      },
      "outputs": [],
      "source": [
        "# x = read_tsv(train_prompt_path)\n",
        "# x.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMUYg-qvy_Yw"
      },
      "source": [
        "# generated_prompts_separator\n",
        "- ラベルごとに分類する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Rz-tzHX9B1yv"
      },
      "outputs": [],
      "source": [
        "# def generated_prompts_separator(path: dict, data: pd.DataFrame) -> dict:\n",
        "\n",
        "#   true_set = pd.DataFrame()\n",
        "#   false_set = pd.DataFrame()\n",
        "#   total_len = len(data)\n",
        "\n",
        "#   for i in range(len(data)):\n",
        "\n",
        "#     data_row = data.iloc[i]\n",
        "#    # print(data_row)\n",
        "\n",
        "#     if data_row[1] == \"true\":\n",
        "#         true_set = true_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"half-true\":\n",
        "#         true_set = true_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"mostly-true\":\n",
        "#         true_set = true_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"false\":\n",
        "#         false_set = false_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"barely-true\":\n",
        "#         false_set = false_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"pants-fire\":\n",
        "#         false_set = false_set.append(data_row, ignore_index=True)\n",
        "#     else:\n",
        "#         print(f\"Invalid label was found! {data_row[1]}\")\n",
        "\n",
        "#   print(\"===================================================\\n\")\n",
        "#   print(f\"total len : {total_len}\")\n",
        "#   print(f\"true_set : {len(true_set)}\")\n",
        "#   print(f\"false_set : {len(false_set)}\")\n",
        "#   print(\"===================================================\\n\")\n",
        "\n",
        "#   write_dataframe_in_tsv(true_set, path[\"true_set\"])\n",
        "#   write_dataframe_in_tsv(false_set, path[\"false_set\"])\n",
        "\n",
        "\n",
        "#   return dict(\n",
        "#       true_set=true_set,\n",
        "#       false_set=true_set\n",
        "#   )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RGPWmYuKW7AT"
      },
      "outputs": [],
      "source": [
        "# fine_main_train_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/train/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/train/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_main_test_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/test/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/test/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_main_val_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/val/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/val/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_toy_train_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/train/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/train/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_toy_test_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/test/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/test/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_toy_val_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/val/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/val/false_sets.tsv\"\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OGnq4oOKMIN-"
      },
      "outputs": [],
      "source": [
        "# main_train_df = read_tsv(train_prompt_path)\n",
        "# main_test_df = read_tsv(test_prompt_path)\n",
        "# main_val_df = read_tsv(val_prompt_path)\n",
        "\n",
        "# toy_train_df = main_train_df[:600]\n",
        "# toy_test_df = main_test_df[:200]\n",
        "# toy_val_df = main_val_df[:200]\n",
        "\n",
        "\n",
        "# main_train_set = generated_prompts_separator(fine_main_train_path_dict, main_train_df)\n",
        "# main_test_set = generated_prompts_separator(fine_main_test_path_dict, main_test_df)\n",
        "# main_val_set = generated_prompts_separator(fine_main_val_path_dict, main_val_df)\n",
        "\n",
        "# toy_train_set = generated_prompts_separator(fine_toy_train_path_dict, toy_train_df)\n",
        "# toy_test_set = generated_prompts_separator(fine_toy_test_path_dict, toy_test_df)\n",
        "# toy_val_set = generated_prompts_separator(fine_toy_val_path_dict, toy_val_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-pyPfFDRkUFP"
      },
      "outputs": [],
      "source": [
        "# y = read_tsv(\"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/train/false_sets.tsv\")\n",
        "# y.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# データ前処理の全行程"
      ],
      "metadata": {
        "id": "SxZNpMBC4_xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path_dict = {\n",
        "#     # 生データのパス\n",
        "#     'raw_train_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/train2.tsv',\n",
        "#     'raw_test_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/test2.tsv',\n",
        "#     'raw_val_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/val2.tsv',\n",
        "\n",
        "#     # 欠損値処理を行った後のデータのパス\n",
        "#     'missing_value_processed_train_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_train2.tsv',\n",
        "#     'missing_value_processed_test_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_test2.tsv',\n",
        "#     'missing_value_processed_val_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_val2.tsv',\n",
        "\n",
        "#     # 欠損値処理を行った後、さらなる整形を行ったデータ\n",
        "#     'input_for_lightning_model_train_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_train2.tsv',\n",
        "#     'input_for_lightning_model_test_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_test2.tsv',\n",
        "#     'input_for_lightning_model_val_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_val2.tsv',\n",
        "\n",
        "#     # 集合に分割した後のデータ (main)\n",
        "#     'main_input_for_lightning_model_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/train/true_set2.tsv',\n",
        "#     'main_input_for_lightning_model_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/train/false_set2.tsv',\n",
        "#     'main_input_for_lightning_model_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/test/true_set2.tsv',\n",
        "#     'main_input_for_lightning_model_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/test/false_set2.tsv',\n",
        "#     'main_input_for_lightning_model_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/val/true_set2.tsv',\n",
        "#     'main_input_for_lightning_model_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/val/false_set2.tsv',\n",
        "\n",
        "#     # 集合に分割した後のデータ (toy)\n",
        "#     'toy_input_for_lightning_model_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/train/true_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/train/false_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/test/true_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/test/false_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/val/true_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/val/false_set2.tsv',\n",
        "\n",
        "#   }\n"
      ],
      "metadata": {
        "id": "1xrPuNRyL4Sv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 生データの読み込み\n",
        "# raw_train_data = read_tsv(path_dict['raw_train_data'])\n",
        "# raw_test_data = read_tsv(path_dict['raw_test_data'])\n",
        "# raw_val_data = read_tsv(path_dict['raw_val_data'])\n",
        "\n",
        "# # 基本的な前処理\n",
        "# processed_train_data = data_missing_value_processor(\n",
        "#     raw_train_data,\n",
        "#     path_dict['missing_value_processed_train_data']\n",
        "# )\n",
        "# processed_test_data = data_missing_value_processor(\n",
        "#     raw_test_data,\n",
        "#     path_dict['missing_value_processed_test_data']\n",
        "# )\n",
        "# processed_val_data = data_missing_value_processor(\n",
        "#     raw_val_data,\n",
        "#     path_dict[\"missing_value_processed_val_data\"]\n",
        "# )\n",
        "\n",
        "# # タスクに向けた前処理と保存\n",
        "# train_data_for_LIAR_PLUS_Dataset = data_pre_processor_for_LIAR_PLUS_Dataset(\n",
        "#     processed_train_data,\n",
        "#     path_dict['input_for_lightning_model_train_base']\n",
        "#     )\n",
        "# test_data_for_LIAR_PLUS_Dataset = data_pre_processor_for_LIAR_PLUS_Dataset(\n",
        "#     processed_test_data,\n",
        "#     path_dict['input_for_lightning_model_test_base']\n",
        "#     )\n",
        "# val_data_for_LIAR_PLUS_Dataset = data_pre_processor_for_LIAR_PLUS_Dataset(\n",
        "#     processed_val_data,\n",
        "#     path_dict['input_for_lightning_model_val_base']\n",
        "#     )\n",
        "\n",
        "# # 集合に分割し保存 (main)\n",
        "# main_train_true_set, main_train_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['main_input_for_lightning_model_train_data_true'],\n",
        "#      'false_set': path_dict['main_input_for_lightning_model_train_data_false']},\n",
        "#     train_data_for_LIAR_PLUS_Dataset\n",
        "# )\n",
        "\n",
        "# main_test_true_set, main_test_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['main_input_for_lightning_model_test_data_true'],\n",
        "#      'false_set': path_dict['main_input_for_lightning_model_test_data_false']},\n",
        "#     test_data_for_LIAR_PLUS_Dataset\n",
        "# )\n",
        "\n",
        "# main_val_true_set, main_val_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['main_input_for_lightning_model_val_data_true'],\n",
        "#      'false_set': path_dict['main_input_for_lightning_model_val_data_false']},\n",
        "#     val_data_for_LIAR_PLUS_Dataset\n",
        "# )\n",
        "\n",
        "\n",
        "# # 集合に分割し保存 (toy)\n",
        "# toy_train_true_set, toy_train_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "#      'false_set': path_dict['toy_input_for_lightning_model_train_data_false']},\n",
        "#     train_data_for_LIAR_PLUS_Dataset[:120]\n",
        "# )\n",
        "\n",
        "# toy_test_true_set, toy_test_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "#      'false_set': path_dict['toy_input_for_lightning_model_test_data_false']},\n",
        "#     test_data_for_LIAR_PLUS_Dataset[:40]\n",
        "# )\n",
        "\n",
        "# toy_val_true_set, toy_val_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "#      'false_set': path_dict['toy_input_for_lightning_model_val_data_false']},\n",
        "#     val_data_for_LIAR_PLUS_Dataset[:40]\n",
        "# )"
      ],
      "metadata": {
        "id": "SBQLyeNu4_Lw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ_ZwRAby_hP"
      },
      "source": [
        "# Google Driveへ接続しディレクトリを移動"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "L31wefW7zMo9"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vpFyWy0BzU0v"
      },
      "outputs": [],
      "source": [
        "# %cd /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning\n",
        "# %ls\n",
        "# %pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB5rz2m-y_uq"
      },
      "source": [
        "# TransformersとPytorch Lightningをインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "izeLGyCTznqQ"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X4Le2kXY8K3",
        "outputId": "0706eed5-70af-4742-ac3a-3d4a1d9d8c28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.35.2\n",
            "2.1.3\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "import pytorch_lightning as pl\n",
        "import pandas as pd\n",
        "print(transformers.__version__)\n",
        "print(pl.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciFDDUpS572n"
      },
      "source": [
        "# class LIAR_PLUS_Dataset_For_CEG(Dataset):を定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Zf6kfB036jme"
      },
      "outputs": [],
      "source": [
        "# # -*- coding: utf-8 -*-\n",
        "# # pylint: disable-msg=import-error\n",
        "# # pylint: disable-msg=no-member\n",
        "# # ========================================================\n",
        "# \"\"\"dataset module is written for create data module\"\"\"\n",
        "# # ========================================================\n",
        "\n",
        "\n",
        "# # ========================================================\n",
        "# # Imports\n",
        "# # ========================================================\n",
        "# import pandas as pd\n",
        "# import pytorch_lightning as pl\n",
        "# import torch\n",
        "# import time\n",
        "\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# from typing import Optional\n",
        "\n",
        "# class LIAR_PLUS_Dataset_For_CEG(Dataset):\n",
        "#     \"\"\"\n",
        "#     this class is for encoding input dataframe for GPT2\n",
        "\n",
        "#     Input dataframe needs to be consist of 5 columns, \"id\", \"label\", \"prompt\", \"justification\" and \"credit_score\"\n",
        "\n",
        "#     later we wrap a lightning data module around it.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, data: pd.DataFrame, config_dict: dict):\n",
        "#       # max_token_length should be flexible depending on input sequence so it needs to be changed later.\n",
        "#         self.data = data\n",
        "#         self.config_dict = config_dict\n",
        "#         self.prepare_tokenizer = Prepare_Tokenizer(self.config_dict)\n",
        "#         self.tokenizer = self.prepare_tokenizer.get_tokenizer()\n",
        "#         self.prepare_tokenizer.get_tokenizer_info()\n",
        "#         self.max_token_len = config_dict['MAX_LENGTH']\n",
        "#         # print(\"Before\")\n",
        "#         # print(self.tokenizer.all_special_tokens)\n",
        "#         # print(self.tokenizer.all_special_ids)\n",
        "#         # special_tokens_dict = {\n",
        "#         # 'additional_special_tokens': ['[EXP]'],\n",
        "#         # \"eos_token\" : \"<|endoftext|>\",\n",
        "#         # \"pad_token\" : \"<|endoftext|>\"\n",
        "#         # }\n",
        "#         # self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "#         # self.exp_id = tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "#         # self.eos_id = tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "#         # self.pad_id = tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "#         # print(\"After\")\n",
        "#         # print(tokenizer.all_special_tokens)\n",
        "#         # print(tokenizer.all_special_ids)\n",
        "#         # print(f\"exp_id : {self.exp_id}\")\n",
        "#         # print(f\"eos_id : {self.eos_id}\")\n",
        "#         # print(f\"pad_id : {self.pad_id}\\n\")\n",
        "#         # print(f\"len(tokenizer) : {len(self.tokenizer)}\")\n",
        "\n",
        "#          # it could be 512\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, index: int):\n",
        "\n",
        "#         data_row = self.data.iloc[index]\n",
        "\n",
        "#         concat_text = data_row[2] + \"[EXP]\"\n",
        "#         justification_text = data_row[3]\n",
        "#         full_prompt_text = concat_text + justification_text + \"[EOS]\"\n",
        "\n",
        "#         concat_text_length = len(self.tokenizer.tokenize(concat_text))\n",
        "#         # bartではspecial_token<s>が文頭に付くため、その分だけ入力長が1大きくなる\n",
        "#         if self.config_dict[\"MODEL_NAME\"] == 'facebook/bart-base':\n",
        "#             concat_text_length += concat_text_length\n",
        "\n",
        "#         if concat_text_length > self.max_token_len:\n",
        "#             concat_text_length = self.max_token_len\n",
        "#         justification_text_length = len(self.tokenizer.tokenize(justification_text))\n",
        "#         if justification_text_length > self.max_token_len:\n",
        "#             justification_text_length = self.max_token_len\n",
        "#         full_prompt_text_length = len(self.tokenizer.tokenize(full_prompt_text))\n",
        "#         if full_prompt_text_length > self.max_token_len:\n",
        "#             full_prompt_text_length = self.max_token_len\n",
        "\n",
        "#         if data_row[1] == \"true\":\n",
        "#             label = torch.tensor([1,0], dtype=torch.float32)\n",
        "#         elif data_row[1] == \"half-true\":\n",
        "#             label = torch.tensor([1,0], dtype=torch.float32)\n",
        "#         elif data_row[1] == \"mostly-true\":\n",
        "#             label = torch.tensor([1,0], dtype=torch.float32)\n",
        "#         elif data_row[1] == \"false\":\n",
        "#             label = torch.tensor([0,1], dtype=torch.float32)\n",
        "#         elif data_row[1] == \"barely-true\":\n",
        "#             label = torch.tensor([0,1], dtype=torch.float32)\n",
        "#         elif data_row[1] == \"pants-fire\":\n",
        "#             label = torch.tensor([0,1], dtype=torch.float32)\n",
        "#         else:\n",
        "#             print(f\"Invalid label was found! {data_row[1]}\")\n",
        "#             time.sleep(30)\n",
        "\n",
        "#         concat_encoding = self.tokenizer.encode_plus(\n",
        "#             concat_text,\n",
        "#             add_special_tokens=True,\n",
        "#             max_length = self.max_token_len,\n",
        "#             return_token_type_ids=False,\n",
        "#             padding=\"max_length\",\n",
        "#             truncation=True,\n",
        "#             return_attention_mask=True,\n",
        "#             return_tensors='pt',\n",
        "#         )\n",
        "\n",
        "#         full_prompt_encoding = self.tokenizer.encode_plus(\n",
        "#             full_prompt_text,\n",
        "#             add_special_tokens=True,\n",
        "#             max_length = self.max_token_len,\n",
        "#             return_token_type_ids=False,\n",
        "#             padding=\"max_length\",\n",
        "#             truncation=True,\n",
        "#             return_attention_mask=True,\n",
        "#             return_tensors='pt',\n",
        "#         )\n",
        "\n",
        "#         # labels = full_prompt_encoding[\"input_ids\"].flatten()\n",
        "#         # labels[:concat_text_length] = -100 # cross_entropy_ignore_index\n",
        "\n",
        "#         # print(\"=================================================================\\n\")\n",
        "#         # print(f\"concat_text : {concat_text}\\n\")\n",
        "#         # print(f\"concat_text_length : {concat_text_length}\\n\")\n",
        "#         # print(f\"justification_text : {justification_text}\\n\")\n",
        "#         # print(f\"justification_text_length : {justification_text_length}\\n\")\n",
        "#         # print(f\"full_prompt_text : {full_prompt_text}\\n\")\n",
        "#         # print(f\"full_prompt_text_length : {full_prompt_text_length}\\n\")\n",
        "#         # print(f\"labels before padding : {labels}\\n\")\n",
        "#         # print(f\"labels after padding: {labels}\\n\")\n",
        "#         # print(\"=================================================================\\n\")\n",
        "\n",
        "#         return dict(\n",
        "#             input_ids1=concat_encoding[\"input_ids\"].flatten(),\n",
        "#             attention_mask1=concat_encoding[\"attention_mask\"].flatten(),\n",
        "#             input_ids2=full_prompt_encoding[\"input_ids\"].flatten(),\n",
        "#             attention_mask2=full_prompt_encoding[\"attention_mask\"].flatten(),\n",
        "#             concat_text_length=concat_text_length,\n",
        "#             full_prompt_text_length=full_prompt_text_length,\n",
        "#             # labels = labels\n",
        "#         )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NnU6wf8TntZG"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# pylint: disable-msg=import-error\n",
        "# pylint: disable-msg=no-member\n",
        "# ========================================================\n",
        "\"\"\"dataset module is written for create data module\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import time\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "class LIAR_PLUS_Dataset_For_CEG(Dataset):\n",
        "    \"\"\"\n",
        "    this class is for encoding input dataframe for GPT2\n",
        "\n",
        "    Input dataframe needs to be consist of 5 columns, \"id\", \"label\", \"prompt\", \"justification\" and \"credit_score\"\n",
        "\n",
        "    later we wrap a lightning data module around it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data: pd.DataFrame, config_dict: dict):\n",
        "      # max_token_length should be flexible depending on input sequence so it needs to be changed later.\n",
        "        self.data = data\n",
        "        self.config_dict = config_dict\n",
        "        self.prepare_tokenizer = Prepare_Tokenizer(self.config_dict)\n",
        "        self.tokenizer = self.prepare_tokenizer.get_tokenizer()\n",
        "        self.prepare_tokenizer.get_tokenizer_info()\n",
        "        self.max_token_len = config_dict['MAX_LENGTH']\n",
        "        # print(\"Before\")\n",
        "        # print(self.tokenizer.all_special_tokens)\n",
        "        # print(self.tokenizer.all_special_ids)\n",
        "        # special_tokens_dict = {\n",
        "        # 'additional_special_tokens': ['[EXP]'],\n",
        "        # \"eos_token\" : \"<|endoftext|>\",\n",
        "        # \"pad_token\" : \"<|endoftext|>\"\n",
        "        # }\n",
        "        # self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "        # self.exp_id = tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "        # self.eos_id = tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        # self.pad_id = tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        # print(\"After\")\n",
        "        # print(tokenizer.all_special_tokens)\n",
        "        # print(tokenizer.all_special_ids)\n",
        "        # print(f\"exp_id : {self.exp_id}\")\n",
        "        # print(f\"eos_id : {self.eos_id}\")\n",
        "        # print(f\"pad_id : {self.pad_id}\\n\")\n",
        "        # print(f\"len(tokenizer) : {len(self.tokenizer)}\")\n",
        "\n",
        "         # it could be 512\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "\n",
        "        data_row = self.data.iloc[index]\n",
        "        # print('------------------------------------------------------------------\\n')\n",
        "        # print(f'data_row : {data_row}\\n')\n",
        "        # print('------------------------------------------------------------------\\n')\n",
        "        concat_text, full_prompt_text, concat_text_length, full_prompt_text_length = concatinated_inputs_generator(data_row, self.config_dict, self.tokenizer, self.prepare_tokenizer )\n",
        "\n",
        "        # print('------------------------------------------------------------------\\n')\n",
        "        # print(f'concat_text : {concat_text}\\n')\n",
        "        # print(f'full_prompt_text : {full_prompt_text}\\n')\n",
        "        # print(f'concat_text_length : {concat_text_length}\\n')\n",
        "        # print(f'full_prompt_text_length : {full_prompt_text_length}\\n')\n",
        "        # print('------------------------------------------------------------------\\n')\n",
        "        concat_encoding = self.tokenizer.encode_plus(\n",
        "            concat_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length = self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        full_prompt_encoding = self.tokenizer.encode_plus(\n",
        "            full_prompt_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length = self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        # labels = full_prompt_encoding[\"input_ids\"].flatten()\n",
        "        # labels[:concat_text_length] = -100 # cross_entropy_ignore_index\n",
        "\n",
        "        # print(\"=================================================================\\n\")\n",
        "        # print(f\"concat_text : {concat_text}\\n\")\n",
        "        # print(f\"concat_text_length : {concat_text_length}\\n\")\n",
        "        # print(f\"justification_text : {justification_text}\\n\")\n",
        "        # print(f\"justification_text_length : {justification_text_length}\\n\")\n",
        "        # print(f\"full_prompt_text : {full_prompt_text}\\n\")\n",
        "        # print(f\"full_prompt_text_length : {full_prompt_text_length}\\n\")\n",
        "        # print(f\"labels before padding : {labels}\\n\")\n",
        "        # print(f\"labels after padding: {labels}\\n\")\n",
        "        # print(\"=================================================================\\n\")\n",
        "\n",
        "        return dict(\n",
        "            input_ids1=concat_encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask1=concat_encoding[\"attention_mask\"].flatten(),\n",
        "            input_ids2=full_prompt_encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask2=full_prompt_encoding[\"attention_mask\"].flatten(),\n",
        "            concat_text_length=concat_text_length,\n",
        "            full_prompt_text_length=full_prompt_text_length,\n",
        "            # labels = labels\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf6XZBBC58F9"
      },
      "source": [
        "# class LIAR_PLUS_DataModule_For_CEG(pl.LightningDataModule)を定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "gFsOZ6LW6zLM"
      },
      "outputs": [],
      "source": [
        "class LIAR_PLUS_DataModule_For_CEG(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, config_dict, train_df: pd.DataFrame, test_df: pd.DataFrame, val_df: pd.DataFrame):\n",
        "        super().__init__()\n",
        "        self.config_dict = config_dict\n",
        "        self.train_df = train_df\n",
        "        self.test_df = test_df\n",
        "        self.val_df = val_df\n",
        "        self.training_batch_size = config_dict[\"TRAINING_BATCH_SIZE\"]\n",
        "        self.validation_batch_size = config_dict[\"VALIDATION_BATCH_SIZE\"]\n",
        "        self.test_batch_size = config_dict[\"TEST_BATCH_SIZE\"]\n",
        "        self.max_token_len = self.config_dict[\"MAX_LENGTH\"]\n",
        "        self.train_dataset, self.test_dataset, self.val_dataset = None, None, None\n",
        "\n",
        "        # 以下の処理はDataset内でやってくれるのかも\n",
        "        # print(\"Before\")\n",
        "        # print(self.tokenizer.all_special_tokens)\n",
        "        # print(self.tokenizer.all_special_ids)\n",
        "        # special_tokens_dict = {'additional_special_tokens': ['[EXP]'], \"eos_token\" : \"[EOS]\"}\n",
        "        # self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "        # self.exp_id = tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "        # self.eos_id = tokenizer.convert_tokens_to_ids('[EOS]')\n",
        "        # print(\"After\")\n",
        "        # print(tokenizer.all_special_tokens)\n",
        "        # print(tokenizer.all_special_ids)\n",
        "        # print(f\"exp_id : {self.exp_id}\")\n",
        "        # print(f\"eos_id : {self.eos_id}\\n\")\n",
        "\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None) -> None:\n",
        "        self.train_dataset = LIAR_PLUS_Dataset_For_CEG(self.train_df, self.config_dict)\n",
        "        print(f\"train_dataset size : {len(self.train_dataset)}\\n\")\n",
        "        self.test_dataset = LIAR_PLUS_Dataset_For_CEG(self.test_df, self.config_dict)\n",
        "        print(f\"test_dataset size : {len(self.test_dataset)}\\n\")\n",
        "        self.val_dataset = LIAR_PLUS_Dataset_For_CEG(self.val_df, self.config_dict)\n",
        "        print(f\"val_dataset size : {len(self.val_dataset)}\\n\")\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.training_batch_size,\n",
        "            shuffle=True,\n",
        "            drop_last=True,\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.validation_batch_size,\n",
        "            drop_last=True,\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            drop_last=True,\n",
        "            num_workers=2\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FKESVDD58Nr"
      },
      "source": [
        "# build_checkpoint_callbackを定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zVqrSIBr7hYX"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"helper module is written for write useful function in indexer package\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "def build_checkpoint_callback(config_dict, base_time, batch_size, set_type, filename=\"QTag-{epoch:02d}-{val_loss:.2f}\",\n",
        "                              monitor=\"val_loss\"):\n",
        "    \"\"\"\n",
        "\n",
        "    :param save_top_k:\n",
        "    :param filename:\n",
        "    :param monitor:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # dirpath = config_dict['SAVED_MODEL_PATH'] + f'/{base_time}/' + config_dict['TensorBoardLogger_NAME'] + f'/batch_size={batch_size}'\n",
        "    dirpath = config_dict['SAVED_MODEL_PATH'] + '/' + config_dict['TensorBoardLogger_NAME'] + f'/{base_time}/{set_type}_batch_size={batch_size}'\n",
        "    print(f'dirpath : {dirpath}')\n",
        "    checkpoint_callback = ModelCheckpoint(monitor=monitor,  # monitored quantity\n",
        "                                          filename=filename,\n",
        "                                          save_top_k=config_dict['SAVE_TOP_K'],  # save the top k models\n",
        "                                          dirpath=dirpath,\n",
        "                                          mode=\"min\",  # mode of the monitored quantity for optimization\n",
        "                                          )\n",
        "    print(f\"checkpoint_callback : {checkpoint_callback}\")\n",
        "    return checkpoint_callback, dirpath"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mqQ2UHy58SM"
      },
      "source": [
        "# class CEG(pl.LightningModule)を定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "E9axfgge7tV3"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# pylint: disable=too-many-arguments\n",
        "# pylint: disable=import-error\n",
        "# ========================================================\n",
        "\"\"\"This module is written for write BERT classifier.\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "from typing import List\n",
        "import pytorch_lightning as pl\n",
        "from torch import nn\n",
        "import torch\n",
        "import torchmetrics\n",
        "from transformers import GPT2Tokenizer, AutoModel, AutoConfig, AdamW, get_linear_schedule_with_warmup, GPT2LMHeadModel, T5ForConditionalGeneration, BartForConditionalGeneration\n",
        "\n",
        "class CEG(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    creates a pytorch lightning model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config_dict,\n",
        "                 n_warmup_steps: int = None,\n",
        "                 n_training_steps: int = None,\n",
        "                 n_classes: int = None,\n",
        "                 result_manager = None):\n",
        "        super().__init__()\n",
        "        self.config_dict = config_dict\n",
        "        self.result_manager = result_manager\n",
        "        self.prepare_tokenizer = Prepare_Tokenizer(self.config_dict)\n",
        "        self.tokenizer = self.prepare_tokenizer.get_tokenizer()\n",
        "        self.prepare_tokenizer.get_tokenizer_info()\n",
        "        self.prepare_model = Prepare_Model(config=self.config_dict, prepare_tokenizer=self.prepare_tokenizer)\n",
        "        self.model = self.prepare_model.get_model()\n",
        "        self.prepare_model.get_model_info()\n",
        "        # self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "        # print(\"Before\")\n",
        "        # print(self.tokenizer.all_special_tokens)\n",
        "        # print(self.tokenizer.all_special_ids)\n",
        "        # special_tokens_dict = {\n",
        "        #     'additional_special_tokens': ['[EXP]'],\n",
        "        #     \"eos_token\" : \"<|endoftext|>\",\n",
        "        #     \"pad_token\" : \"<|endoftext|>\"\n",
        "        #     }\n",
        "        # self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "        # exp_id = self.tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "        # eos_id = self.tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        # pad_id = self.tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        # print(\"After\")\n",
        "        # print(self.tokenizer.all_special_tokens)\n",
        "        # print(self.tokenizer.all_special_ids)\n",
        "        # print(f\"exp_id : {exp_id}\")\n",
        "        # print(f\"eos_id : {eos_id}\")\n",
        "        # print(f\"pad_id : {pad_id}\\n\")\n",
        "        # print(f\"len(self.tokenizer) : {len(self.tokenizer)}\")\n",
        "\n",
        "        # gpt2config = AutoConfig.from_pretrained('gpt2', bos_token_id=self.tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id, additional_special_tokens_id = exp_id, output_hidden_states=False)\n",
        "\n",
        "        # self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=gpt2config)\n",
        "        # self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "        # self.lm_head = self.model.lm_head\n",
        "        # new_weights = torch.cat([self.lm_head.weight[:-1, :], torch.zeros(1, self.lm_head.weight.shape[1]) -10000])\n",
        "        # self.model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.batch_size = config_dict['TRAINING_BATCH_SIZE']\n",
        "        self.n_training_steps = n_training_steps\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.config_dict['CROSS_ENTROPY_IGNORE_INDEX'])\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, concat_text_length, full_prompt_text_length):\n",
        "\n",
        "        # print(\"===============================forward=================================\\n\")\n",
        "        labels = input_ids2\n",
        "        # print(f\"labels : {labels}\")\n",
        "        # print(f\"len(labels) : {len(labels)}\")\n",
        "        # print(full_prompt_text_length)\n",
        "        batch_max_length = max(full_prompt_text_length)\n",
        "        # print(f\"batch_max_length : {batch_max_length}\")\n",
        "\n",
        "\n",
        "        for i in range(len(labels)):\n",
        "            labels[i][:concat_text_length[i]] = -100 # cross_entropy_ignore_index\n",
        "            labels[i][full_prompt_text_length[i]:] = -100\n",
        "        # print(f\"input_ids1 before reshaping : {input_ids1.shape}\")\n",
        "        # print(f\"labels before reshaping : {labels.shape}\")\n",
        "        # print(f\"attention_mask1 before reshaping : {attention_mask1.shape}\")\n",
        "        labels = labels[:,:batch_max_length].contiguous()\n",
        "        input_ids1 = input_ids1[:,:batch_max_length].contiguous()\n",
        "        attention_mask1 = attention_mask1[:,:batch_max_length].contiguous()\n",
        "        # print(f\"input_ids1 after reshaping : {input_ids1.shape}\")\n",
        "        # print(f\"labels after reshaping : {labels.shape}\")\n",
        "        # print(f\"attention_mask1 after reshaping : {attention_mask1.shape}\")\n",
        "            # print(f\"labels after -100 padding : {labels[0]}\")\n",
        "        # print(f\"len(labels[0]) before shifting : {len(labels[0])}\\n\")\n",
        "        # print(f\"input_ids1 : {input_ids1}\\n\")\n",
        "        # print(f\"attention_mask1 : {attention_mask1}\\n\")\n",
        "        # print(f\"input_ids2 : {input_ids2}\\n\")\n",
        "        # print(f\"concat_text_length : {concat_text_length}\\n\")\n",
        "        # print(f\"full_prompt_length : {full_prompt_text_length}\\n\")\n",
        "        # print(f\"labels : {labels}\\n\")\n",
        "        # print(f\"labels before shifting : {labels}, labels.size() : {labels.size()}\\n\")\n",
        "        # labels = labels[0][1:]\n",
        "        # labels = torch.Tensor(labels)\n",
        "        # print(f\"labels after shifting : {labels}, labels.size() : {labels.size()}\\n\")\n",
        "        # additional_pad = tokenizer.tokenize(\"[PAD]\")\n",
        "        # additional_pad = torch.Tensor(tokenizer.convert_tokens_to_ids(additional_pad))\n",
        "        # additional_pad = torch.Tensor(additional_pad)\n",
        "        # labels = torch.cat((labels, additional_pad))\n",
        "        # print(f\"labels after shifting : {labels}, labels.size() : {labels.size()}\\n\")\n",
        "\n",
        "        #outputs = self.model(input_ids=input_ids1, attention_mask=attention_mask1, labels=labels)\n",
        "\n",
        "        outputs = self.model(input_ids=input_ids1, attention_mask=attention_mask1, labels=labels)\n",
        "        logits = outputs.logits\n",
        "        loss = 0\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # print(f\"outputs : {outputs}\\n\")\n",
        "        # print(f\"outputs type : {type(outputs)}\")\n",
        "        # print(f\"input.shape : {outputs.shape}\")\n",
        "        # print(f\"logits : {logits}\")\n",
        "        # print(f\"logits type : {type(logits)}\")\n",
        "        # print(f\"logits shape : {logits.shape}\")\n",
        "        # print(f\"loss type : {type(loss)}\")\n",
        "        # print(f\"loss : {loss}\")\n",
        "        softmax_output = self.softmax(logits)\n",
        "        arg = torch.argmax(softmax_output, dim=2)\n",
        "        # print(f\"argmax(ouput) : {arg}\") # --> tensor\n",
        "        # print(f\"argmax(ouput).size() : {arg.size()}\")\n",
        "        # print(f\"argmax(output).dtype() : {arg.dtype}\")\n",
        "        # print(arg)\n",
        "        generated = self.tokenizer.decode(arg[0])\n",
        "        # print(f\"generated text: {generated}\\n\")\n",
        "        # print(f\"len(generated) : {len(self.tokenizer.tokenize(generated))}\\n\")\n",
        "        # print(f\"loss type : {type(loss)}\\n\")\n",
        "        # print(f\"loss shape : {loss.shape}\\n\")\n",
        "\n",
        "        return loss, outputs\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "\n",
        "        :param batch:\n",
        "        :param batch_idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # print(f\"training batch : {batch}\")\n",
        "        input_ids1 = batch[\"input_ids1\"]\n",
        "        attention_mask1 = batch[\"attention_mask1\"]\n",
        "        input_ids2 = batch[\"input_ids2\"]\n",
        "        attention_mask2 = batch[\"attention_mask2\"]\n",
        "        concat_text_length = batch[\"concat_text_length\"]\n",
        "        full_prompt_text_length = batch[\"full_prompt_text_length\"]\n",
        "        # labels = batch[\"labels\"]\n",
        "\n",
        "        loss, output = self(input_ids1, attention_mask1, input_ids2, attention_mask2, concat_text_length, full_prompt_text_length)\n",
        "\n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "        # print(f'type(np_loss) : {type(np_loss)}')\n",
        "        # print(f'np_loss : {np_loss}')\n",
        "\n",
        "        self.result_manager.trainLoss_batch.append(np_loss)\n",
        "        # print(\"===============================training=================================\\n\")\n",
        "        # print(f\"output : {outputs}\\n\")\n",
        "        # print(f\"output type : {type(outputs)}\\n\")\n",
        "        # print(f\"loss : {loss}\\n\")\n",
        "        # print(f\"loss type : {type(loss)}\\n\")\n",
        "        # print(f\"loss shape : {loss.shape}\\n\")\n",
        "\n",
        "        return {\"loss\": loss, \"predictions\": output}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "\n",
        "        :param batch:\n",
        "        :param batch_idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # print(f\"validation batch : {batch}\")\n",
        "        input_ids1 = batch[\"input_ids1\"]\n",
        "        attention_mask1 = batch[\"attention_mask1\"]\n",
        "        input_ids2 = batch[\"input_ids2\"]\n",
        "        attention_mask2 = batch[\"attention_mask2\"]\n",
        "        concat_text_length = batch[\"concat_text_length\"]\n",
        "        full_prompt_text_length = batch[\"full_prompt_text_length\"]\n",
        "        # labels = batch[\"labels\"]\n",
        "\n",
        "        loss, output = self(input_ids1, attention_mask1, input_ids2, attention_mask2, concat_text_length, full_prompt_text_length)\n",
        "\n",
        "        self.log(\"val_loss\", loss, prog_bar=True, logger=True, on_step=False)\n",
        "\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "        # print(f'type(np_loss) : {type(np_loss)}')\n",
        "        # print(f'np_loss : {np_loss}')\n",
        "        self.result_manager.valLoss_batch.append(np_loss)\n",
        "        # print(\"===============================validating=================================\\n\")\n",
        "        # print(f\"output : {outputs}\\n\")\n",
        "        # print(f\"output type : {type(outputs)}\\n\")\n",
        "        # print(f\"loss : {loss}\\n\")\n",
        "        # print(f\"loss type : {type(loss)}\\n\")\n",
        "        # print(f\"loss shape : {loss.shape}\\n\")\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "\n",
        "        :param batch:\n",
        "        :param batch_idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        # print(\"test batch : {batch}\")\n",
        "        input_ids1 = batch[\"input_ids1\"]\n",
        "        attention_mask1 = batch[\"attention_mask1\"]\n",
        "        input_ids2 = batch[\"input_ids2\"]\n",
        "        attention_mask2 = batch[\"attention_mask2\"]\n",
        "        concat_text_length = batch[\"concat_text_length\"]\n",
        "        full_prompt_text_length = batch[\"full_prompt_text_length\"]\n",
        "        # labels = batch[\"labels\"]\n",
        "\n",
        "        loss, output = self(input_ids1, attention_mask1, input_ids2, attention_mask2, concat_text_length, full_prompt_text_length)\n",
        "\n",
        "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
        "\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "        # print(f'type(np_loss) : {type(np_loss)}')\n",
        "        # print(f'np_loss : {np_loss}')\n",
        "        self.result_manager.testLoss_batch.append(np_loss)\n",
        "\n",
        "        # print(\"===============================testing=================================\\n\")\n",
        "        # print(f\"output : {outputs}\\n\")\n",
        "        # print(f\"output type : {type(outputs)}\\n\")\n",
        "        # print(f\"loss : {loss}\\n\")\n",
        "        # print(f\"loss type : {type(loss)}\\n\")\n",
        "        # print(f\"loss shape : {loss.shape}\\n\")\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        optimizer = AdamW(self.parameters(), lr=self.config_dict['LR'])\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.n_warmup_steps,\n",
        "                                                    num_training_steps=self.n_training_steps)\n",
        "        return dict(optimizer=optimizer, lr_scheduler=dict(scheduler=scheduler, interval=\"step\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA2UW3Xf58Vh"
      },
      "source": [
        "# calculate_warmup_stepsを定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4ljK1VdE8BP5"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"This module is written for write useful function.\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_warmup_steps(train_df: pd.DataFrame, num_epochs: int, batch_size: int):\n",
        "    steps_per_epoch = len(train_df) // batch_size\n",
        "    total_training_steps = steps_per_epoch * num_epochs\n",
        "    warmup_steps = total_training_steps // 5\n",
        "    return total_training_steps, warmup_steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8eAFYfq3-CD"
      },
      "source": [
        "# ModelConfigを定義する\n",
        "- colab上では辞書\n",
        "- モジュール分けするときはjsonに"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dPiXl1ej7b3"
      },
      "source": [
        "## GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "kpgeqGQOkCpL"
      },
      "outputs": [],
      "source": [
        "# GPT2-small\n",
        "gpt2_small_config = {\n",
        "    'MODEL_NAME' : 'gpt2',\n",
        "    'TOKENIZER_NAME' : 'gpt2',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/small',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'small']\n",
        "}\n",
        "\n",
        "# GPT2-medium\n",
        "gpt2_medium_config = {\n",
        "    'MODEL_NAME' : 'gpt2-medium',\n",
        "    'TOKENIZER_NAME' : 'gpt2-medium',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/medium',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'medium']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXnvK6fjj9uf"
      },
      "source": [
        "## T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "27MGdEADkWyL"
      },
      "outputs": [],
      "source": [
        "# T5-small\n",
        "t5_small_config = {\n",
        "    'MODEL_NAME' : 't5-small',\n",
        "    'TOKENIZER_NAME' : 't5-small',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 't5/small',\n",
        "    'MODEL_FOLDER' : ['t5', 'small']\n",
        "}\n",
        "\n",
        "# T5-base\n",
        "t5_base_config = {\n",
        "    'MODEL_NAME' : 't5-base',\n",
        "    'TOKENIZER_NAME' : 't5-base',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 't5/base',\n",
        "    'MODEL_FOLDER' : ['t5', 'base']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy9zBiRqkAC3"
      },
      "source": [
        "## BART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5d4CXx8T4BWp"
      },
      "outputs": [],
      "source": [
        "# BART_base\n",
        "bart_base_config = {\n",
        "    'MODEL_NAME' : 'facebook/bart-base',\n",
        "    'TOKENIZER_NAME' : 'facebook/bart-base',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 'bart/base',\n",
        "    'MODEL_FOLDER' : ['bart', 'base']\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model_Dict"
      ],
      "metadata": {
        "id": "QoWojRc90I8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model_Config_Dict = {\n",
        "    'gpt2_small_config' : gpt2_small_config,\n",
        "    'gpt2_medium_config' : gpt2_medium_config,\n",
        "    't5_small_config' : t5_small_config,\n",
        "    't5_base_config' : t5_base_config,\n",
        "    'bart_base_config' : bart_base_config\n",
        "}"
      ],
      "metadata": {
        "id": "IILYrc640NFr"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shph_NDW3HpB"
      },
      "source": [
        "# class Prepare_tokenizerを定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "D0mJ9pIG3MHe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import warnings\n",
        "\n",
        "class Prepare_Tokenizer():\n",
        "    def __init__(self, config:dict):\n",
        "        self.config = config\n",
        "        self.TOKENIZER_NAME = self.config['TOKENIZER_NAME']\n",
        "        self.tokenizer = None\n",
        "        self.tokenizer_length = None\n",
        "        self.additional_special_tokens = None\n",
        "        self.eos_token = None\n",
        "        self.eos_token_id = None\n",
        "        self.pad_token = None\n",
        "        self.pad_token_id = None\n",
        "        self.exp_token = '[EXP]'\n",
        "        self.exp_token_id = None\n",
        "        self.initialization_flag = 0\n",
        "\n",
        "    def get_tokenizer(self):\n",
        "        if self.initialization_flag != 0:\n",
        "            raise ValueError(f\"tokenizer is already initialized. This instance is for {self.tokenizer}\")\n",
        "\n",
        "        if self.TOKENIZER_NAME in ['gpt2', 'gpt2-medium']:\n",
        "            self.initialization_flag = 1\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.TOKENIZER_NAME)\n",
        "            print(f\"tokenizer : AutoTokenizer.from_pretrained({self.TOKENIZER_NAME})\")\n",
        "            print(\"Before adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            special_tokens_dict = {\n",
        "            'additional_special_tokens': ['[EXP]'],\n",
        "            \"eos_token\" : \"<|endoftext|>\",\n",
        "            \"pad_token\" : \"<|endoftext|>\"\n",
        "            }\n",
        "            self.eos_token = \"<|endoftext|>\"\n",
        "            self.pad_token = \"<|endoftext|>\"\n",
        "            self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "            self.exp_token_id = self.tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "            self.eos_token_id = self.tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "            self.pad_token_id = self.tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "            print(\"After adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            print(f\"exp_id : {self.exp_token_id}\")\n",
        "            print(f\"eos_id : {self.eos_token_id}\")\n",
        "            print(f\"pad_id : {self.pad_token_id}\")\n",
        "            self.tokenizer_length = len(self.tokenizer)\n",
        "            print(f\"len(tokenizer) : {self.tokenizer_length}\")\n",
        "\n",
        "            return self.tokenizer\n",
        "\n",
        "        elif self.TOKENIZER_NAME in ['t5-small','t5-base']:\n",
        "            self.initialization_flag = 1\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.TOKENIZER_NAME)\n",
        "            print(f\"tokenizer : AutoTokenizer.from_pretrained({self.TOKENIZER_NAME})\")\n",
        "            print(\"Before adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            special_tokens_dict = {\n",
        "            'additional_special_tokens': ['[EXP]'],\n",
        "            }\n",
        "            self.eos_token = '</s>'\n",
        "            self.pad_token = '<pad>'\n",
        "            self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "            self.exp_token_id = self.tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "            self.eos_token_id = self.tokenizer.convert_tokens_to_ids('</s>')\n",
        "            self.pad_token_id = self.tokenizer.convert_tokens_to_ids('<pad>')\n",
        "            print(\"After adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            print(f\"exp_id : {self.exp_token_id}\")\n",
        "            print(f\"eos_id : {self.eos_token_id}\")\n",
        "            print(f\"pad_id : {self.pad_token_id}\")\n",
        "            self.tokenizer_length = len(self.tokenizer)\n",
        "            print(f\"len(tokenizer) : {self.tokenizer_length}\")\n",
        "\n",
        "            return self.tokenizer\n",
        "\n",
        "        elif self.TOKENIZER_NAME in ['facebook/bart-base']:\n",
        "            self.initialization_flag = 1\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.TOKENIZER_NAME)\n",
        "            print(f\"tokenizer : AutoTokenizer.from_pretrained({self.TOKENIZER_NAME})\")\n",
        "            print(\"Before adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            special_tokens_dict = {\n",
        "            'additional_special_tokens': ['[EXP]'],\n",
        "            }\n",
        "            self.eos_token = '</s>'\n",
        "            self.pad_token = '<pad>'\n",
        "            self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "            self.exp_token_id = self.tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "            self.eos_token_id = self.tokenizer.convert_tokens_to_ids('</s>')\n",
        "            self.pad_token_id = self.tokenizer.convert_tokens_to_ids('<pad>')\n",
        "            print(\"After adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            print(f\"exp_id : {self.exp_token_id}\")\n",
        "            print(f\"eos_id : {self.eos_token_id}\")\n",
        "            print(f\"pad_id : {self.pad_token_id}\")\n",
        "            self.tokenizer_length = len(self.tokenizer)\n",
        "            print(f\"len(tokenizer) : {self.tokenizer_length}\")\n",
        "\n",
        "            return self.tokenizer\n",
        "        else:\n",
        "            raise ValueError('Wrong Tokenizer Type : you have to indicate tokenizer type out of the folloing list.\\n [\"gpt2\", \"gpt2-medium\", \"t5-small\", \"t5-base\", \"bart-base\"]\\n')\n",
        "\n",
        "    def get_tokenizer_info(self):\n",
        "        if self.initialization_flag == 0:\n",
        "            print(\"there is no information. you need to call get_tokenizer() first.\")\n",
        "        else:\n",
        "            print(\"---------- tokenizer information ----------\")\n",
        "            print(f\"self.TOKENIZER_NAME : {self.TOKENIZER_NAME}\")\n",
        "            print(f\"self.tokenizer : {self.tokenizer}\")\n",
        "            print(f\"self.tokenizer_length : {self.tokenizer_length}\")\n",
        "            print(f\"self.additional_special_tokens : {self.additional_special_tokens}\")\n",
        "            print(f\"self.eos_token : {self.eos_token}\")\n",
        "            print(f\"self.eos_token_id : {self.eos_token_id}\")\n",
        "            print(f\"self.pad_token : {self.pad_token}\")\n",
        "            print(f\"self.pad_token_id : {self.pad_token_id}\")\n",
        "            print(f\"self.exp_token : {self.exp_token}\")\n",
        "            print(f\"self.exp_token_id : {self.exp_token_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkBEZqVYGCj7"
      },
      "source": [
        "# class Prepare_Modelを定義する\n",
        "- 今のところ、モデルごとに操作の大きな違いはなさそう\n",
        "- BARTに関して,文の頭に`<s>`が付く"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "HE6f_5MZGr0k"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, T5ForConditionalGeneration, BartForConditionalGeneration, AutoConfig\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Prepare_Model():\n",
        "\n",
        "    def __init__(self, config:dict, prepare_tokenizer):\n",
        "        self.MODEL_NAME = config['MODEL_NAME']\n",
        "        self.TOKENIZER_LENGTH = prepare_tokenizer.tokenizer_length\n",
        "        self.additional_model_config = None\n",
        "        self.prepare_tokenizer = prepare_tokenizer\n",
        "        self.model = None\n",
        "        self.initialization_flag = 0\n",
        "\n",
        "\n",
        "    def get_model(self):\n",
        "        if self.initialization_flag == 1:\n",
        "            raise ValueError('You cannot call get_model() because this is already initialized.\\n You are supossed to instantiate Prepare_Model_Class and then call get_model() again.')\n",
        "\n",
        "        if self.MODEL_NAME in ['gpt2', 'gpt2-medium']:\n",
        "\n",
        "            if self.prepare_tokenizer is None:\n",
        "                raise ValueError('prepare_tokenizer is None and this is not good for gpt2!')\n",
        "            self.initialization_flag = 1\n",
        "            self.additional_model_config = AutoConfig.from_pretrained(self.MODEL_NAME, eos_token_id=self.prepare_tokenizer.eos_token_id, pad_token_id=self.prepare_tokenizer.pad_token_id, additional_special_tokens_id = self.prepare_tokenizer.exp_token_id, output_hidden_states=False)\n",
        "            self.model = GPT2LMHeadModel.from_pretrained(self.MODEL_NAME, config=self.additional_model_config)\n",
        "            self.model.resize_token_embeddings(self.TOKENIZER_LENGTH)\n",
        "            self.lm_head = self.model.lm_head\n",
        "            new_weights = torch.cat([self.lm_head.weight[:-1, :], torch.zeros(1, self.lm_head.weight.shape[1]) -10000])\n",
        "            self.model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "\n",
        "            return self.model\n",
        "\n",
        "        elif self.MODEL_NAME in ['t5-small','t5-base']:\n",
        "\n",
        "            self.initialization_flag = 1\n",
        "            self.model = T5ForConditionalGeneration.from_pretrained(self.MODEL_NAME)\n",
        "            self.model.resize_token_embeddings(self.TOKENIZER_LENGTH)\n",
        "            # not sure the following 2 lines are actually necessary\n",
        "            print(f\"weights.shape : {self.model.lm_head.weight.shape}\")\n",
        "            # new_weights = torch.cat([self.model.lm_head.weight[:-1, :], torch.zeros(1, self.model.lm_head.weight.shape[1]) -10000])\n",
        "            # self.model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "\n",
        "\n",
        "            # print(f\"new_weights.shape : {self.model.lm_head.weight.shape}\")\n",
        "\n",
        "            return self.model\n",
        "\n",
        "        elif self.MODEL_NAME in ['facebook/bart-base']:\n",
        "\n",
        "            self.initialization_flag = 1\n",
        "            self.model = BartForConditionalGeneration.from_pretrained(self.MODEL_NAME)\n",
        "            self.model.resize_token_embeddings(self.TOKENIZER_LENGTH)\n",
        "            # not sure the following 2 lines are actually necessary\n",
        "            new_weights = torch.cat([self.model.lm_head.weight[:-1, :], torch.zeros(1, self.model.lm_head.weight.shape[1]) -10000])\n",
        "            self.model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "            return self.model\n",
        "        else:\n",
        "            raise ValueError('Wrong Model Type : you have to indicate model type out of the folloing list.\\n [\"gpt2\", \"gpt2-medium\", \"t5-small\", \"t5-base\", \"bart-base\"]\\n')\n",
        "\n",
        "\n",
        "    def get_model_info(self):\n",
        "        if self.initialization_flag == 0:\n",
        "            print(\"there is no information. you need to call get_model() first.\")\n",
        "        else:\n",
        "            print(\"---------- model information ----------\")\n",
        "            print(f\"self.MODEL_NAME : {self.MODEL_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "2WCJwC68PT4G"
      },
      "outputs": [],
      "source": [
        "# my_prepare_tokenizer = Prepare_Tokenizer(bart_base_config)\n",
        "# my_prepare_tokenizer.get_tokenizer_info()\n",
        "# my_tokenizer = my_prepare_tokenizer.get_tokenizer()\n",
        "# my_prepare_tokenizer.get_tokenizer_info()\n",
        "# my_prepare_model = Prepare_Model(config=bart_base_config, prepare_tokenizer=my_prepare_tokenizer)\n",
        "# my_prepare_model.get_model_info()\n",
        "# my_model = my_prepare_model.get_model()\n",
        "# my_prepare_model.get_model_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1HyKsIv73SD"
      },
      "source": [
        "# 結果を確認する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xRHqmgR0F59h"
      },
      "outputs": [],
      "source": [
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir=lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "157MXEFW9qNZ"
      },
      "source": [
        "# class Fine_Tuned_Model\n",
        "- fine-tuning済みのモデルをロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "EmN_mlcS9xxq"
      },
      "outputs": [],
      "source": [
        "class Fine_Tuned_Model():\n",
        "    def __init__(self, config_dict, ckpt_file_name, file_name:str):\n",
        "\n",
        "        self.config_dict = config_dict\n",
        "        self.ckpt_file_name = ckpt_file_name\n",
        "        self.file_name = file_name\n",
        "        self.component_name = config_dict['COMPONENT_NAME']\n",
        "        self.MODEL_NAME = config_dict[\"MODEL_NAME\"]\n",
        "        self.transformers_model_path = f'./model_transformers/{self.MODEL_NAME}/{self.file_name}'\n",
        "        self.initialization_flag = 0\n",
        "        self.model = None\n",
        "        self.pretrained_model = None\n",
        "\n",
        "    def create_directory_for_transformers_model(self):\n",
        "        if self.initialization_flag == 1:\n",
        "            raise ValueError('self.initialization_flag == 1 : you are supposed to instantiate Fine_Tuned_Model once and call it again.')\n",
        "        else:\n",
        "            self.initialization_flag = 1\n",
        "\n",
        "        if self.component_name == 'CEG':\n",
        "            self.model = CEG.load_from_checkpoint(gpt2_small_config['SAVED_MODEL_PATH']+ f'/{self.ckpt_file_name}')\n",
        "            self.model.model.save_pretrained(self.transformers_model_path)\n",
        "            print(f'{self.MODEL_NAME} was saved as {self.component_name} in {self.transformers_model_path}.')\n",
        "        elif self.component_name == 'EP':\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(f'self.component_name : {self.component_name} is not supported.')\n",
        "\n",
        "\n",
        "    def load_model(self):\n",
        "        if self.initialization_flag == 0:\n",
        "            raise ValueError('self.initialization_flag == 0 : you are supposed to call create_directory_for_transformer_model() first.')\n",
        "\n",
        "        if self.MODEL_NAME in ['gpt2', 'gpt2-medium']:\n",
        "            self.pretrained_model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/' + f'{self.MODEL_NAME}/{self.file_name}')\n",
        "            print(self.pretrained_model)\n",
        "\n",
        "            return self.pretrained_model\n",
        "\n",
        "        elif self.MODEL_NAME in ['t5-small','t5-base']:\n",
        "            self.pretrained_model = T5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/' + f'{self.MODEL_NAME}/{self.file_name}')\n",
        "            print(self.pretrained_model)\n",
        "\n",
        "            return self.pretrained_model\n",
        "\n",
        "        elif self.MODEL_NAME in ['facebook/bart-base']:\n",
        "            self.pretrained_model = BartForConditionalGeneration.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/' + f'{self.MODEL_NAME}/{self.file_name}')\n",
        "            print(self.pretrained_model)\n",
        "\n",
        "            return self.pretrained_model\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Wrong Model Type : you have to indicate model type out of the folloing list.\\n [\"gpt2\", \"gpt2-medium\", \"t5-small\", \"t5-base\", \"bart-base\"]\\n')\n",
        "\n",
        "\n",
        "    def get_model_info(self):\n",
        "        if self.initialization_flag == 0:\n",
        "            print(\"there is no information. you need to call load_model() first.\")\n",
        "        else:\n",
        "            print(\"---------- model information ----------\")\n",
        "            print(f'self.config_dict : {self.config_dict}')\n",
        "            print(f'self.ckpt_file_name : {self.ckpt_file_name}')\n",
        "            print(f'self.file_name : {self.file_name}')\n",
        "            print(f'self.component_name : {self.component_name}')\n",
        "            print(f'self.MODEL_NAME : {self.MODEL_NAME}')\n",
        "            print(f'self.transformers_model_path : {self.transformers_model_path}')\n",
        "            print(f'self.initialization_flag : {self.initialization_flag}')\n",
        "            print(f'self.model : {self.model}')\n",
        "            print(f'self.pretrained_model : {self.pretrained_model}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class Result_Manager\n",
        "- 結果の保存や表示を行う\n",
        "- 次の指標を保存する\n",
        "## バッチごと\n",
        "- trainAcc\n",
        "- trainF1\n",
        "- trainloss\n",
        "- valAcc\n",
        "- valF1\n",
        "- valloss\n",
        "- testAcc\n",
        "- testF1\n",
        "- testloss\n",
        "\n",
        "\n",
        "## エポックごと\n",
        "- trainAcc\n",
        "- trainF1\n",
        "- trainloss\n",
        "- valAcc\n",
        "- valF1\n",
        "- valoss\n"
      ],
      "metadata": {
        "id": "ylG8KaRfYSd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "import datetime\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "yHeo6y028rRe"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class  Result_Manager():\n",
        "    def __init__(self, base_time, batch_size, model_config, set_type):\n",
        "        self.model_config = model_config\n",
        "        self.batch_size = batch_size\n",
        "        self.set_type = set_type\n",
        "        self.epoch_size = None\n",
        "        self.dt_now = base_time\n",
        "        self.metrics_name_list = [\n",
        "            'trainAcc_batch',\n",
        "            'trainF1_batch',\n",
        "            'trainLoss_batch',\n",
        "            'valAcc_batch',\n",
        "            'valF1_batch',\n",
        "            'valLoss_batch',\n",
        "            'testAcc_batch',\n",
        "            'testF1_batch',\n",
        "            'testLoss_batch',\n",
        "            'trainAcc_epoch',\n",
        "            'trainF1_epoch',\n",
        "            'trainLoss_epoch',\n",
        "            'valAcc_epoch',\n",
        "            'valF1_epoch',\n",
        "            'valLoss_epoch',\n",
        "        ]\n",
        "\n",
        "        self.trainAcc_batch = []\n",
        "        self.trainF1_batch = []\n",
        "        self.trainLoss_batch = []\n",
        "        self.valAcc_batch = []\n",
        "        self.valF1_batch = []\n",
        "        self.valLoss_batch = []\n",
        "        self.testAcc_batch = []\n",
        "        self.testF1_batch = []\n",
        "        self.testLoss_batch = []\n",
        "\n",
        "        self.trainAcc_epoch = []\n",
        "        self.trainF1_epoch = []\n",
        "        self.trainLoss_epoch = []\n",
        "        self.valAcc_epoch = []\n",
        "        self.valF1_epoch = []\n",
        "        self.valLoss_epoch = []\n",
        "\n",
        "        self.metrics_list = [\n",
        "            self.trainAcc_batch,\n",
        "            self.trainF1_batch,\n",
        "            self.trainLoss_batch,\n",
        "            self.valAcc_batch,\n",
        "            self.valF1_batch,\n",
        "            self.valLoss_batch,\n",
        "            self.testAcc_batch,\n",
        "            self.testF1_batch,\n",
        "            self.testLoss_batch,\n",
        "            self.trainAcc_epoch,\n",
        "            self.trainF1_epoch,\n",
        "            self.trainLoss_epoch,\n",
        "            self.valAcc_epoch,\n",
        "            self.valF1_epoch,\n",
        "            self.valLoss_epoch,\n",
        "        ]\n",
        "\n",
        "    def _build_folder(self, base_path: str):\n",
        "\n",
        "        for folder in self.model_config['MODEL_FOLDER']:\n",
        "            base_path = f'{base_path}/{folder}'\n",
        "            if not os.path.exists(base_path):\n",
        "                os.mkdir(base_path)\n",
        "\n",
        "        return base_path\n",
        "\n",
        "\n",
        "    def save_result_log(self, base_path: str):\n",
        "\n",
        "        base_path = f'{base_path}/result_log/{self.dt_now}'\n",
        "        if not os.path.exists(base_path):\n",
        "            os.mkdir(base_path)\n",
        "\n",
        "        model_path = self._build_folder(base_path)\n",
        "\n",
        "        folder = f'{model_path}/{self.set_type}_batch_size:{self.batch_size}'\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "\n",
        "        for idx in range(len(self.metrics_list)):\n",
        "\n",
        "            if len(self.metrics_list[idx]) != 0:\n",
        "                file_name = self.metrics_name_list[idx]+'.csv'\n",
        "                # print(f'file_name : {file_name}')\n",
        "                # np_metric = [item.detach().cpu().numpy().astype(float) for item in self.metrics_list[idx]]\n",
        "                np_metric = self.metrics_list[idx]\n",
        "                print(self.metrics_name_list[idx])\n",
        "                print(f'len(np_metric) : {len(np_metric)}')\n",
        "                data = pd.DataFrame({self.metrics_name_list[idx]:np_metric})\n",
        "                data.to_csv(f'{folder}/{file_name}', header=True, index=False)\n",
        "\n",
        "\n",
        "        # pd.Series(self.trainAcc_batch).to_csv(folder+'/trainAcc_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainF1_batch).to_csv(folder+'/trainF1_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainLoss_batch).to_csv(folder+'/trainLoss_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valAcc_batch).to_csv(folder+'/valAcc_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valF1_batch).to_csv(folder+'/valF1_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valLoss_batch).to_csv(folder+'/valLoss_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.testAcc_batch).to_csv(folder+'/testAcc_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.testF1_batch).to_csv(folder+'/testF1_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.testLoss_batch).to_csv(folder+'/testLoss_batch.csv', header=False, index=False)\n",
        "\n",
        "        # pd.Series(self.trainAcc_epoch).to_csv(folder+'/trainAcc_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainF1_epoch).to_csv(folder+'/trainF1_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainLoss_epoch).to_csv(folder+'/trainLoss_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valAcc_epoch).to_csv(folder+'/valAcc_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valF1_epoch).to_csv(folder+'/valF1_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valLoss_epoch).to_csv(folder+'/valLoss_epoch.csv', header=False, index=False)\n",
        "\n",
        "\n",
        "    def make_single_result_img(self, base_path:str):\n",
        "\n",
        "        base_path = f'{base_path}/plot_figures/{self.dt_now}'\n",
        "        if not os.path.exists(base_path):\n",
        "            os.mkdir(base_path)\n",
        "\n",
        "        model_path = self._build_folder(base_path)\n",
        "\n",
        "        folder = f'{model_path}/{self.set_type}_batch_size:{self.batch_size}'\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "\n",
        "        for idx in range(len(self.metrics_name_list)):\n",
        "\n",
        "            save_fig_path = f'/{folder}/' + self.metrics_name_list[idx] + '.png'\n",
        "\n",
        "            if len(self.metrics_list[idx]) == 0:\n",
        "                continue\n",
        "            else:\n",
        "                plt.figure()\n",
        "                np_metric = self.metrics_list[idx]\n",
        "                # print(f'type(np_metric) : {type(np_metric)}')\n",
        "                # print(self.metrics_name_list[idx])\n",
        "                # print(f'np_metric : {np_metric}')\n",
        "                data = pd.DataFrame({self.metrics_name_list[idx]:np_metric})\n",
        "                # print(f'type(data) : {type(data)}')\n",
        "                data = data.astype(float)\n",
        "                # print(f'type(data) : {type(data)}')\n",
        "                data.plot(\n",
        "                title=self.metrics_name_list[idx],\n",
        "                legend=True\n",
        "                )\n",
        "                plt.savefig(save_fig_path)\n",
        "                # plt.close('all')\n",
        "\n",
        "\n",
        "            # plt.figure()\n",
        "            # if len(self.metrics_list[idx]) != 0:\n",
        "            #     np_metric = self.metrics_list[idx]\n",
        "            #     print(f'type(np_metric) : {type(np_metric)}')\n",
        "            #     print(self.metrics_name_list[idx])\n",
        "            #     print(f'np_metric : {np_metric}')\n",
        "            #     data = pd.Series(np_metric)\n",
        "            #     print(f'type(data) : {type(data)}')\n",
        "            #     data = data.astype(float)\n",
        "            #     print(f'type(data) : {type(data)}')\n",
        "            #     data.plot(\n",
        "            #     title=self.metrics_name_list[idx],\n",
        "            #     legend=True\n",
        "            #     )\n",
        "            # plt.savefig(save_fig_path)\n",
        "            # plt.close('all')\n",
        "\n",
        "    # def make_imgs_into_one(self, base_path:str, metrics: list, figure_name:str):\n",
        "\n",
        "    #     base_path = base_path + '/plot_figures/' + self.dt_now + '/'\n",
        "    #     imgs = []\n",
        "\n",
        "    #     for i in range(len(metrics)):\n",
        "    #         j = base_path + metrics[i] + '.png'\n",
        "    #         imgs.append(j)\n",
        "    #         j = ''\n",
        "\n",
        "    #     save_fig_path = base_path + figure_name +'.png'\n",
        "    #     fig, axs = plt.subplots(1, len(metrics), figsize=(10, 5))\n",
        "\n",
        "    #     for i, im in zip(range(len(axs)), imgs):\n",
        "    #         img = Image.open(im)\n",
        "    #         axs[i].imshow(img)\n",
        "    #         axs[i].set_title(metrics[i])\n",
        "    #         axs[i].axis('off')\n",
        "\n",
        "    #     plt.show()\n",
        "    #     plt.savefig(save_fig_path)\n",
        "\n",
        "    # def make_data_into_one(self, base_path:str, metrics: list, figure_name:str):\n",
        "\n",
        "    #     base_path = base_path + '/result_log/' + self.dt_now + '/'\n",
        "    #     data = []\n",
        "\n",
        "    #     for i in range(len(metrics)):\n",
        "    #         data_path = base_path + i + '.csv'\n",
        "    #         data.append(pd.read_csv(data_path))\n",
        "    #         data_path = ''\n",
        "\n",
        "    #     save_fig_path = base_path + figure_name +'.png'\n",
        "    #     fig, axs = plt.subplots(1, len(metrics), figsize=(10, 5))\n",
        "\n",
        "    #     for i, im in zip(range(len(axs)), imgs):\n",
        "    #         img = Image.open(im)\n",
        "    #         axs[i].imshow(img) # cmap=カラーマップの選択\n",
        "    #         axs[i].set_title(metrics[i])\n",
        "    #         axs[i].axis('off')\n",
        "\n",
        "    #     plt.show()\n",
        "    #     plt.savefig(save_fig_path)\n",
        "\n",
        "    # def get_result_img(self, base_path:str, image_name:str):\n",
        "    #     imgs = base_path + '/plot_figures/' + self.dt_now + '/' + image_name\n",
        "    #     img = imread(fname=imgs, format='png')\n",
        "    #     plt.imshow(\n",
        "    #         img,\n",
        "    #     )\n",
        "    #     plt.show()"
      ],
      "metadata": {
        "id": "s0FmH8BGY5Zw"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# base_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning'\n",
        "\n",
        "# my_Result_Manager = Result_Manager()\n",
        "# my_Result_Manager.save_result_log(base_path)\n",
        "# my_Result_Manager.make_single_result_img(base_path)\n",
        "# my_Result_Manager.make_imgs_into_one(base_path, ['trainLoss_batch', 'valLoss_batch','trainLoss_batch', 'valLoss_batch', 'trainLoss_batch', 'valLoss_batch'], 'example_image')"
      ],
      "metadata": {
        "id": "zx_rIKKtDDqm"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class myclass():\n",
        "#     def __init__(self, num:int):\n",
        "#         self.num = num\n",
        "\n",
        "#     def _add_one(self):\n",
        "#         return self.num +1\n",
        "\n",
        "#     def add_one(self):\n",
        "#         return self._add_one()\n"
      ],
      "metadata": {
        "id": "G3lf9p7sLyYO"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mc = myclass(99)\n",
        "# print(mc.add_one())"
      ],
      "metadata": {
        "id": "wdXGjnkbMm9s"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train_and_test.py"
      ],
      "metadata": {
        "id": "yJThITDcX55U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer"
      ],
      "metadata": {
        "id": "sBM_FaGPz8Bm"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train_and_test_config"
      ],
      "metadata": {
        "id": "SDufpKSVE3ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_test_config =  {\n",
        "    'MODEL_NAME' : ['gpt2_small_config', 'gpt2_medium_config', 't5_small_config', 't5_base_config', 'bart_base_config'],\n",
        "    'BATCH_SIZE' : [1, 2, 4, 8], # train\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "lwiMog4M1qv9"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save_pretrained_model_as_transformers_CEG.py"
      ],
      "metadata": {
        "id": "asT-m5I2Zb8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_pretrained_model_as_transformers_CEG(saved_model_path:str):\n",
        "\n",
        "    file_type=''\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    for i in range(len(saved_model_path)):\n",
        "        # print(saved_model_path[-i])\n",
        "        if saved_model_path[-i-1] == '/':\n",
        "            counter += 1\n",
        "        if counter == 4:\n",
        "            path_for_model_name = saved_model_path[-i:]\n",
        "            continue\n",
        "        if counter == 6:\n",
        "            print(saved_model_path[-i:])\n",
        "            file_type = saved_model_path[-i:]\n",
        "            print(f'file_type: {file_type}')\n",
        "            break\n",
        "\n",
        "    for j in range(len(path_for_model_name)):\n",
        "        if path_for_model_name[j] == '/':\n",
        "            path_for_model_name = path_for_model_name[j+1:]\n",
        "            print(path_for_model_name)\n",
        "            break\n",
        "\n",
        "    file_list = os.listdir(saved_model_path)\n",
        "    ckpt_name = file_list[0]\n",
        "    ckpt_path = f'{file_type}/{ckpt_name}'\n",
        "    print(f'ckpt_name : {ckpt_name}')\n",
        "    print(f'ckpt_path : {ckpt_path}')\n",
        "\n",
        "    tr_model_path = f'./model_transformers/{path_for_model_name}'\n",
        "    model = CEG.load_from_checkpoint(ckpt_path)\n",
        "    model.model.save_pretrained(tr_model_path)\n",
        "\n",
        "    # if model_name in ['gpt2', 'gpt2-medium']:\n",
        "    #     model_from_pretrained = GPT2LMHeadModel.from_pretrained(tr_model_path)\n",
        "    # elif model_name in ['t5-small','t5-base']:\n",
        "    #     model_from_pretrained = T5ForConditionalGeneration.from_pretrained(tr_model_path)\n",
        "    # elif model_name in ['facebook/bart-base']:\n",
        "    #     model_from_pretrained = BartForConditionalGeneration.from_pretrained(tr_model_path)\n",
        "    # else:\n",
        "    #     raise ValueError('Wrong Model Type')\n",
        "\n",
        "    # print(f'pretrained model was saved in {tr_model_path}')\n",
        "\n",
        "    # return model_from_pretrained"
      ],
      "metadata": {
        "id": "1rhdfYSVZhjv"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def train_and_test_CEG\n",
        "- バッチをいろいろ変えて実験を行う"
      ],
      "metadata": {
        "id": "F8DU2vwmi62H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_test_CEG(model_config, train_and_test_config, batch_size:int, base_time, set_type):\n",
        "\n",
        "    print(f'----------- training and testing has just started (batch_size : {batch_size}) -----------')\n",
        "    pl.seed_everything(train_and_test_config['RANDOM_SEED'])\n",
        "\n",
        "    train_data = read_tsv(train_and_test_config[\"TRAINING_DATA_PATH\"])\n",
        "    test_data = read_tsv(train_and_test_config[\"TEST_DATA_PATH\"])\n",
        "    val_data = read_tsv(train_and_test_config[\"VALIDATION_DATA_PATH\"])\n",
        "    model_config['TRAINING_BATCH_SIZE'] = batch_size\n",
        "    result_manager = Result_Manager(base_time, batch_size, model_config, set_type)\n",
        "    base_path = path_dict['TensorBoardLogger'] + f'/{base_time}'\n",
        "    folder = model_config['TensorBoardLogger_NAME'] + f'/batch_size:{batch_size}'\n",
        "\n",
        "    data_module =  LIAR_PLUS_DataModule_For_CEG(\n",
        "                    model_config,\n",
        "                    train_data,\n",
        "                    test_data,\n",
        "                    val_data\n",
        "                    )\n",
        "\n",
        "    total_training_steps, warmup_steps = calculate_warmup_steps(train_data, model_config[\"TRAINING_EPOCHS\"],\n",
        "                                                        model_config[\"TRAINING_BATCH_SIZE\"])\n",
        "    print(f\"total_training_steps : {total_training_steps} , warmup_steps : {warmup_steps} \")\n",
        "    model = CEG(\n",
        "            config_dict=model_config,\n",
        "            n_warmup_steps=warmup_steps,\n",
        "            n_training_steps=total_training_steps,\n",
        "            n_classes=2,\n",
        "            result_manager=result_manager)\n",
        "\n",
        "    checkpoint_callback, saved_model_path = build_checkpoint_callback(model_config, base_time, batch_size, set_type)\n",
        "    logger = TensorBoardLogger(base_path, name=folder)\n",
        "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        logger=logger,\n",
        "        callbacks=[checkpoint_callback, early_stopping_callback],\n",
        "        max_epochs=model_config['TRAINING_EPOCHS'])\n",
        "\n",
        "    trainer.fit(model, datamodule=data_module)\n",
        "    trainer.test(datamodule=data_module)\n",
        "\n",
        "    model.result_manager.save_result_log('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning')\n",
        "    model.result_manager.make_single_result_img('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning')\n",
        "\n",
        "    save_pretrained_model_as_transformers_CEG(saved_model_path)\n",
        "    # print(f'model_from_pretrained : {model_from_pretrained}')\n"
      ],
      "metadata": {
        "id": "gYTebhHDjLvC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def get_base_time\n",
        "- ベースタイムを取得する"
      ],
      "metadata": {
        "id": "4gN4z8_DGBp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_time():\n",
        "\n",
        "    now_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "    base_time = str(now_time)[:-16]\n",
        "\n",
        "    print(f'you got base_time : {base_time}')\n",
        "\n",
        "    return base_time\n"
      ],
      "metadata": {
        "id": "g97VCxNfGZFk"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base_time = get_base_time()\n",
        "# train_and_test_CEG(\n",
        "#     model_config=gpt2_small_config,\n",
        "#     train_and_test_config=train_and_test_config,\n",
        "#     batch_size=1,\n",
        "#     base_time=base_time,\n",
        "#     set_type='true')"
      ],
      "metadata": {
        "id": "5uW-MK4zqqvJ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train_and_test_CEGの実行による各モデルの訓練"
      ],
      "metadata": {
        "id": "60u4GTAT0Cvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # gpt2-small\n",
        "# base_time = get_base_time()\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_small_config,\n",
        "#         train_and_test_config=train_and_test_config,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time,\n",
        "#         set_type=true)"
      ],
      "metadata": {
        "id": "2LoaFzU1zb37"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # gpt2-medium\n",
        "base_time = get_base_time()\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_medium_config,\n",
        "#         train_and_test_config=train_and_test_config,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time)"
      ],
      "metadata": {
        "id": "IbWO94Io0RLO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a7688f-9f2c-45a4-9327-d3af785b25a5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you got base_time : 2024-01-19 16:57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # t5-small\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=t5_small_config,\n",
        "#         train_and_test_config=train_and_test_config,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time)"
      ],
      "metadata": {
        "id": "sHL8J0lI0Rlx"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # t5-base\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=t5_base_config,\n",
        "#         train_and_test_config=train_and_test_config,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time)"
      ],
      "metadata": {
        "id": "a9ZSBLd60Rs8"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # bart-base\n",
        "# for batch_size in [1, 2]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=bart_base_config,\n",
        "#         train_and_test_config=train_and_test_config,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time)"
      ],
      "metadata": {
        "id": "SVbTeLvm0Rwf"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print('----------- training and testing has just started -----------')\n",
        "# pl.seed_everything(train_and_test_config['RANDOM_SEED'])\n",
        "\n",
        "# model_num = len(train_and_test_config['MODEL_NAME'])\n",
        "# batch_num = len(train_and_test_config['BATCH_SIZE'])\n",
        "\n",
        "# print(\"train_data_path : \" + train_and_test_config[\"TRAINING_DATA_PATH\"])\n",
        "# print(\"test_data_path : \" + train_and_test_config[\"TEST_DATA_PATH\"])\n",
        "# print(\"val_data_path : \" + train_and_test_config[\"VALIDATION_DATA_PATH\"])\n",
        "# train_data = read_tsv(train_and_test_config[\"TRAINING_DATA_PATH\"])\n",
        "# test_data = read_tsv(train_and_test_config[\"TEST_DATA_PATH\"])\n",
        "# val_data = read_tsv(train_and_test_config[\"VALIDATION_DATA_PATH\"])\n",
        "\n",
        "# for model_name in train_and_test_config['MODEL_NAME']:\n",
        "#     for batch_size in train_and_test_config['BATCH_SIZE']:\n",
        "\n",
        "#         # print(f'model name : {model_name},  batch_size : {batch_size}' )\n",
        "\n",
        "#         model_config = Model_Config_Dict[model_name]\n",
        "#         result_manager = Result_Manager()\n",
        "#         model_config['TRAINING_BATCH_SIZE'] = batch_size\n",
        "#         data_module =  LIAR_PLUS_DataModule_For_CEG(\n",
        "#                         model_config,\n",
        "#                         train_data,\n",
        "#                         test_data,\n",
        "#                         val_data\n",
        "#                         )\n",
        "#         total_training_steps, warmup_steps = calculate_warmup_steps(train_data, model_config[\"TRAINING_EPOCHS\"],\n",
        "#                                                             model_config[\"TRAINING_BATCH_SIZE\"])\n",
        "#         print(f\"total_training_steps : {total_training_steps} , warmup_steps : {warmup_steps} \")\n",
        "#         model = CEG(\n",
        "#                 config_dict=model_config,\n",
        "#                 n_warmup_steps=warmup_steps,\n",
        "#                 n_training_steps=total_training_steps,\n",
        "#                 n_classes=2,\n",
        "#                 result_manager=result_manager)\n",
        "\n",
        "#         checkpoint_callback = build_checkpoint_callback(model_config)\n",
        "#         logger = TensorBoardLogger(path_dict['TensorBoardLogger'], name=model_config['TensorBoardLogger_NAME'])\n",
        "#         early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "#         trainer = pl.Trainer(\n",
        "#             logger=logger,\n",
        "#             callbacks=[checkpoint_callback, early_stopping_callback],\n",
        "#             max_epochs=model_config['TRAINING_EPOCHS'])\n",
        "\n",
        "#         trainer.fit(model, datamodule=data_module)\n",
        "#         trainer.test(datamodule=data_module)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zCGO8KOo4myK"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# predict_CEG.py"
      ],
      "metadata": {
        "id": "M6eFhN6gYCIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prediction_config"
      ],
      "metadata": {
        "id": "FClHb_Z7QKDM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6yLpINWQdYu"
      },
      "source": [
        "### GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ZcS52jgHQdZJ"
      },
      "outputs": [],
      "source": [
        "# GPT2-small\n",
        "gpt2_small_config_for_prediction = {\n",
        "    'MODEL_NAME' : 'gpt2',\n",
        "    'TOKENIZER_NAME' : 'gpt2',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/small',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'small']\n",
        "}\n",
        "\n",
        "# GPT2-medium\n",
        "gpt2_medium_config_for_prediction = {\n",
        "    'MODEL_NAME' : 'gpt2-medium',\n",
        "    'TOKENIZER_NAME' : 'gpt2-medium',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/medium',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'medium']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDWJumDfQdZK"
      },
      "source": [
        "### T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ktq3nYX8QdZK"
      },
      "outputs": [],
      "source": [
        "# T5-small\n",
        "t5_small_config_for_prediction = {\n",
        "    'MODEL_NAME' : 't5-small',\n",
        "    'TOKENIZER_NAME' : 't5-small',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata'],\n",
        "    'TensorBoardLogger_NAME' : 't5/small',\n",
        "    'MODEL_FOLDER' : ['t5', 'small']\n",
        "}\n",
        "\n",
        "# T5-base\n",
        "t5_base_config_for_prediction = {\n",
        "    'MODEL_NAME' : 't5-base',\n",
        "    'TOKENIZER_NAME' : 't5-base',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata'],\n",
        "    'TensorBoardLogger_NAME' : 't5/base',\n",
        "    'MODEL_FOLDER' : ['t5', 'base']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9471NurQdZL"
      },
      "source": [
        "### BART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "YJ7l-9zUQdZM"
      },
      "outputs": [],
      "source": [
        "# BART_base\n",
        "bart_base_config_for_prediction = {\n",
        "    'MODEL_NAME' : 'facebook/bart-base',\n",
        "    'TOKENIZER_NAME' : 'facebook/bart-base',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata'],\n",
        "    'TensorBoardLogger_NAME' : 'bart/base',\n",
        "    'MODEL_FOLDER' : ['bart', 'base']\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def load_transformers_model()"
      ],
      "metadata": {
        "id": "7obqzggSvFZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_transformers_model(model_name:str, tr_model_path:str):\n",
        "\n",
        "    if model_name in ['gpt2', 'gpt2-medium']:\n",
        "        model_from_pretrained = GPT2LMHeadModel.from_pretrained(tr_model_path)\n",
        "    elif model_name in ['t5-small','t5-base']:\n",
        "        model_from_pretrained = T5ForConditionalGeneration.from_pretrained(tr_model_path)\n",
        "    elif model_name in ['facebook/bart-base']:\n",
        "        model_from_pretrained = BartForConditionalGeneration.from_pretrained(tr_model_path)\n",
        "    else:\n",
        "        raise ValueError('Wrong Model Type')\n",
        "\n",
        "    return model_from_pretrained"
      ],
      "metadata": {
        "id": "4LKKGgntvKDM"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tr_model_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/gpt2/small/2024-01-16 18:36/batch_size=1'\n",
        "# model_from_pretrained = load_transformers_model('gpt2', tr_model_path)\n",
        "# train_data = read_tsv(train_and_test_config[\"TRAINING_DATA_PATH\"])"
      ],
      "metadata": {
        "id": "AUJzynDQvyXs"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def get_textualized_outputs()"
      ],
      "metadata": {
        "id": "vNhDiYAaPCTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_textualized_outputs(outputs, input_length, tokenizer):\n",
        "\n",
        "    # print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "    generated_tokens = outputs.sequences[:, input_length:]\n",
        "    # print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "    tokens_list = outputs.sequences[0]\n",
        "    # print(tokens_list)\n",
        "    full_text = tokenizer.decode(tokens_list)\n",
        "    generated_text = tokenizer.decode(generated_tokens[0])\n",
        "\n",
        "    # print(f\"full text : {full_text}\\n\")\n",
        "    # print(f\"generated text: {generated_text}\")\n",
        "    # print(f\"len(generated) : {len(tokenizer.tokenize(generated_text))}\\n\")\n",
        "\n",
        "    return full_text, generated_text"
      ],
      "metadata": {
        "id": "u9HjMNUUPIOx"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def extend_data_with_generated_justification()"
      ],
      "metadata": {
        "id": "t8m33Kj6LRTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extend_data_with_generated_justification(data, data_row, generated_text):\n",
        "\n",
        "    # df_data_row = pd.DataFrame(data_row)\n",
        "    df = pd.DataFrame([generated_text])\n",
        "\n",
        "    concatinated_data_row = pd.concat([data_row, df], ignore_index=True, axis=0).T\n",
        "    data = pd.concat([data, concatinated_data_row], axis=0).reset_index(drop=True)\n",
        "\n",
        "# data = pd.DataFrame()\n",
        "# data_row = train_data.iloc[0]\n",
        "# j = pd.DataFrame(['1111'])\n",
        "# concatinated_data_row = pd.concat([data_row, j], axis=0).T\n",
        "# # print(len(concatinated_data_row))\n",
        "# # concatinated_data_row\n",
        "# data_1 = pd.concat([data, concatinated_data_row], axis=0).reset_index(drop=True)\n",
        "# data_2 = pd.concat([data_1, concatinated_data_row], axis=0).reset_index(drop=True)\n",
        "# data_3 = pd.concat([data_2, concatinated_data_row], axis=0).reset_index(drop=True)\n",
        "# data_3\n",
        "    return data"
      ],
      "metadata": {
        "id": "fRABoNMGLnt0"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = pd.DataFrame()\n",
        "# data_row = train_data.iloc[0]\n",
        "# j = pd.DataFrame(['1111'])\n",
        "# concatinated_data_row = pd.concat([data_row, j], axis=0).T\n",
        "# # print(len(concatinated_data_row))\n",
        "# # concatinated_data_row\n",
        "# data_1 = pd.concat([data, concatinated_data_row], axis=0).reset_index(drop=True)\n",
        "# data_2 = pd.concat([data_1, concatinated_data_row], axis=0).reset_index(drop=True)\n",
        "# data_3 = pd.concat([data_2, concatinated_data_row], axis=0).reset_index(drop=True)\n",
        "# data_3"
      ],
      "metadata": {
        "id": "hZ1WETHd1NpO"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_4 = pd.concat([data_1, concatinated_data_row], axis=1).reset_index(drop=True).T\n",
        "# data_4"
      ],
      "metadata": {
        "id": "hty2xuXX35jT"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extend_data_with_generated_justification(data, data_row, generated_text)"
      ],
      "metadata": {
        "id": "HvkF3LiI1e1w"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def predict()"
      ],
      "metadata": {
        "id": "J_AtPhqnvb-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(tokenizer, prepare_tokenizer, model_from_pretrained, model_config, data:pd.DataFrame):\n",
        "\n",
        "    extended_data = pd.DataFrame()\n",
        "\n",
        "    for idx in range(len(data)):\n",
        "\n",
        "        # ilocでデータ列を取得\n",
        "        data_row = data.iloc[idx]\n",
        "\n",
        "        # get_concatinated_input\n",
        "        prompt, full_input, concatinated_input_length, full_input_length = \\\n",
        "        concatinated_inputs_generator(data_row, model_config, tokenizer, prepare_tokenizer)\n",
        "\n",
        "        # モデルに入力\n",
        "        inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
        "        prompt_length = len(inputs[\"input_ids\"][0])\n",
        "        print(f\"inputs_len : {prompt_length}\")\n",
        "\n",
        "        # greedy_search\n",
        "        # outputs = model.generate(**inputs, max_new_tokens=15, return_dict_in_generate=True, output_scores=True)\n",
        "        # beam_search\n",
        "        outputs = model_from_pretrained.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            num_beams=4,\n",
        "            num_return_sequences=1,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "        )\n",
        "\n",
        "        # get_textualized_outputs()\n",
        "        full_text, generated_text = get_textualized_outputs(outputs, prompt_length, tokenizer)\n",
        "\n",
        "        print(f'generated text : {generated_text}')\n",
        "\n",
        "        # extend_data_with_generated_justification()\n",
        "        extended_data = extend_data_with_generated_justification(extended_data, data_row, generated_text)\n",
        "\n",
        "        del data_row\n",
        "\n",
        "    return extended_data\n"
      ],
      "metadata": {
        "id": "qCQsJDlcYObg"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if model_name in ['gpt2', 'gpt2-medium']:\n",
        "#         tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "#         inputs = tokenizer([input], return_tensors=\"pt\")\n",
        "#         input_length = len(inputs[\"input_ids\"][0])\n",
        "#         outputs = model_from_pretrained.generate(\n",
        "#                     **inputs,\n",
        "#                     max_new_tokens=,\n",
        "#                     num_beams=4,\n",
        "#                     num_return_sequences=1,\n",
        "#                     return_dict_in_generate=True,\n",
        "#                     output_scores=True,\n",
        "#                     )\n",
        "\n",
        "#         print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "#         generated_tokens = outputs.sequences[:, input_length:]\n",
        "#         print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "#         tokens_list = outputs.sequences[0]\n",
        "#         print(tokens_list)\n",
        "#         full_text = tokenizer.decode(tokens_list)\n",
        "#         generated = tokenizer.decode(generated_tokens[0])\n",
        "\n",
        "#         print(f\"full text : {full_text}\\n\")\n",
        "#         print(f\"generated text: {generated}\\n\")\n",
        "#         print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\\n\")\n",
        "\n",
        "#     elif model_name in ['t5-small','t5-base']:\n",
        "#         pass\n",
        "#     elif model_name in ['facebook/bart-base']:\n",
        "#         pass\n",
        "#     else:\n",
        "#         raise ValueError('Wrong Model Type')\n",
        "\n",
        "\n",
        "    # print(f\"full text : {full_text}\\n\")\n",
        "    # print(f\"generated text: {generated}\\n\")\n",
        "    # print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\\n\")\n",
        "\n",
        "    # return full_text, generated_text, generated_text_length"
      ],
      "metadata": {
        "id": "GuoMqZf7Lvrf"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def predict_CEG.py"
      ],
      "metadata": {
        "id": "XMye9Hsw2RDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_CEG(model_config, model_name:str, data:pd.DataFrame, ckpt_path, stage:str, binary_type:str):\n",
        "    replaced_ckpt_path = ckpt_path.replace('/', '_')\n",
        "    tr_model_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/' + model_config['TensorBoardLogger_NAME'] +f'/{ckpt_path}'\n",
        "    saved_data_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/' + model_config['TensorBoardLogger_NAME'] + f'/{stage}/{binary_type}_{replaced_ckpt_path}.tsv'\n",
        "    model_from_pretrained = load_transformers_model(model_name, tr_model_path)\n",
        "\n",
        "    prepare_tokenizer = Prepare_Tokenizer(model_config)\n",
        "    tokenizer = prepare_tokenizer.get_tokenizer()\n",
        "\n",
        "    extended_data = predict(tokenizer, prepare_tokenizer, model_from_pretrained, model_config, data)\n",
        "    write_dataframe_in_tsv(extended_data, saved_data_path)\n",
        "\n",
        "    print(f'the extended data was saved in {saved_data_path}')"
      ],
      "metadata": {
        "id": "HnPtr-4jrobd"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データのロード\n",
        "train_data = read_tsv(train_and_test_config[\"TRAINING_DATA_PATH\"])\n",
        "test_data = read_tsv(train_and_test_config[\"TEST_DATA_PATH\"])\n",
        "val_data = read_tsv(train_and_test_config[\"VALIDATION_DATA_PATH\"])\n",
        "\n",
        "# ckptの定義\n",
        "\n",
        "ckpt = '2024-01-19 16:34/true_batch_size=1'\n",
        "\n",
        "# 推論の実行\n",
        "predict_CEG(gpt2_small_config_for_prediction, 'gpt2', train_data, ckpt, 'train', 'true')\n",
        "predict_CEG(gpt2_small_config_for_prediction, 'gpt2', test_data, ckpt, 'test', 'true')\n",
        "predict_CEG(gpt2_small_config_for_prediction, 'gpt2', val_data, ckpt, 'val', 'true')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d3ac4456643447569282f723a9aec791",
            "e8f0f937f79847bb88971c724e52c7ff",
            "784f9b0047d845f5a715b53bf8f1061f",
            "40c642fe9b8440739e545f7553d657e8",
            "f25a2ab8c12245938fc0ae61198d0a33",
            "3189440e500d48529fc8abeac3fecdcd",
            "bf2f0d82e6d9493699547a3edd89340a",
            "2f54117ff0fd4bb19bc72eacb805051c",
            "77c4a89eddef45a594e74e1406a81e50",
            "fcbb6532da96487f88d0ba01ee846a80",
            "d3b2367a78cd441c9b595e19857e8a1a",
            "61385a6a48434cbf8ec69952e7043e73",
            "cc7fc98a5a8f479b95f1da78f617ecac",
            "35ddbb83e7d54779affd1286ddeab5e8",
            "506de6ba392c4503957cad9c3da4b1b2",
            "db5ac87a8a2b42508cf1bdb07b8f255a",
            "09a6ccb9a88d49a39052249aec027d1d",
            "86cf4cac2537494981b524d5d73490a1",
            "8d4c5c4ef86048c59dd91f4e8880f04c",
            "b70b7239297a457fb3847641ec5bab00",
            "9dcc248d91d54da389ad2e97cbf123b9",
            "4d6655472c504244b488723fdc6699a9",
            "bc1d03d5bb6248bb95d3f379110e6b8c",
            "5586f729d5da4a6da931ab79d88f89fa",
            "6aaa4b7b680e4017ad891401c688acb4",
            "67d805ee09944942a0e845c5aa1114bf",
            "5a4c2ff6502a44748214808ddce79f87",
            "2ac920905a7e490fb04f0f7169591e76",
            "ea33d0e9ad8149d5be15d44c6fc1e9f9",
            "2ca92db7b21f4829821ae343350bd6d2",
            "fbafb94bde934eb3bcc74e45705b1961",
            "9fe88c6359cd49b28b908b57e76a36d6",
            "5a3b8aa87a95415aac6d2fb0af7984c1",
            "d029fe4a5e4641f48f7cb3000c64b224",
            "df15377de5974278ac4c2e3b9b13aeac",
            "dea38d38643140e0a9a7f76217ee1764",
            "9f0388d393574c4d8f2b4bb4ae658112",
            "2754487c94aa4a448a66566d8d2c266a",
            "682a27acf11e4471a4b36c2294596739",
            "f47e9931c13443e7a5d7f0bc06428f38",
            "32305e25f36046fd864265ca946a8920",
            "8ccdc78c142f4aeb9140309decbeb168",
            "2a12b6e5dcc848cebaea566bbd728952",
            "01b6ff01a61142d989e93b322e165199"
          ]
        },
        "id": "NqGVIziOwZ85",
        "outputId": "05cd4ff6-60b3-4179-a016-0b9a486a7fb5"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3ac4456643447569282f723a9aec791"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61385a6a48434cbf8ec69952e7043e73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc1d03d5bb6248bb95d3f379110e6b8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d029fe4a5e4641f48f7cb3000c64b224"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text :  the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal. the coal\n",
            "inputs_len : 39\n",
            "generated text :  \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \". \".\n",
            "inputs_len : 32\n",
            "generated text :  the economic turnaround started at the end of my term.<|endoftext|>\n",
            "inputs_len : 55\n",
            "generated text :  the Bears have had more starting quarterbacks in the last 10 years than the total number of tenured<|endoftext|>\n",
            "inputs_len : 56\n",
            "generated text : , that's the only time that he has been on the Senate floor. that's the only time that he has been on the Senate floor. that's the only time that he has been on the Senate floor. that's the only time that he has been on the Senate floor. that's the only time that he has been on the Senate floor. that's the only time that he has been on the Senate floor. that's the only time that he has been on the Senate floor. that's the only time that he has been on the Senate floor. that's the only time that he has been on the Senate floor. that\n",
            "inputs_len : 49\n",
            "generated text :  to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon Lottery to the Oregon\n",
            "inputs_len : 59\n",
            "generated text :  $788 million in higher electricity costs.<|endoftext|>\n",
            "inputs_len : 45\n",
            "generated text :  the Latino vote margin is smaller than the Latino vote margin<|endoftext|>\n",
            "inputs_len : 52\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 44\n",
            "generated text :  $24 billion.<|endoftext|>\n",
            "inputs_len : 67\n",
            "generated text :  vote to not to vote.<|endoftext|>\n",
            "inputs_len : 50\n",
            "generated text :  said that the government would not buy American-made motorcycles.<|endoftext|>\n",
            "inputs_len : 61\n",
            "generated text :  the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate\n",
            "inputs_len : 43\n",
            "generated text :  Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico, Puerto Rico\n",
            "inputs_len : 54\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 36\n",
            "generated text :  the best year for the auto industry in America in history.<|endoftext|>\n",
            "inputs_len : 45\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 49\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 48\n",
            "generated text :  Perry has never lost an election and remains the only person to have won the Texas governorship three times in landslide elections.<|endoftext|>\n",
            "inputs_len : 36\n",
            "generated text :  minority.<|endoftext|>\n",
            "inputs_len : 36\n",
            "generated text :  Ryan is still endorsing Trump<|endoftext|>\n",
            "inputs_len : 47\n",
            "generated text :  the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of the tax rate of\n",
            "inputs_len : 42\n",
            "generated text :  Pryor votes with Obama 93 percent of the time.<|endoftext|>\n",
            "inputs_len : 63\n",
            "generated text :  the federal government is not the authority to regulate.<|endoftext|>\n",
            "inputs_len : 50\n",
            "generated text :  Austin has doubled in size every 25 years or so since it was founded.<|endoftext|>\n",
            "inputs_len : 62\n",
            "generated text :  of the nuclear test.<|endoftext|>\n",
            "inputs_len : 64\n",
            "generated text :  trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion trillion\n",
            "inputs_len : 43\n",
            "generated text :  Rubio skipped 18 defense votes including one to arm the Kurds to fight ISIS<|endoftext|>\n",
            "inputs_len : 47\n",
            "generated text :  unemployment is 51 percent.<|endoftext|>\n",
            "inputs_len : 34\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 59\n",
            "generated text :  the unemployment rate is the lowest in the history of the New York state.<|endoftext|>\n",
            "inputs_len : 37\n",
            "generated text :  women earn 82 cents for every dollar earned by men<|endoftext|>\n",
            "inputs_len : 66\n",
            "generated text :  the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "inputs_len : 49\n",
            "generated text :  only 54 percent of Latinos were registered to vote and only 35 percent actually turned out.<|endoftext|>\n",
            "inputs_len : 49\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 67\n",
            "generated text : , the Supreme Court ruled that she was wrong.<|endoftext|>\n",
            "inputs_len : 51\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 57\n",
            "generated text :  the company was the first of its kind in the country to start a company that was the first of its kind in the country to start a company that was the first of its kind in the country to start a company that was the first of its kind in the country to start a company that was the first of its kind in the country to start a company that was the first of its kind in the country to start a company that was the first of its kind in the country to start a company that was the first of its kind in the country to start a company that was the first of its kind in the country to start a company that\n",
            "inputs_len : 44\n",
            "generated text :  Virginia is net zero on job creation since. Virginia is net zero on job creation since.<|endoftext|>\n",
            "inputs_len : 59\n",
            "generated text :  the law says that any police officer can stop anyone who appears to be reasonably suspicious of being an undocumented person<|endoftext|>\n",
            "inputs_len : 55\n",
            "generated text :  the debt was the end of World War II<|endoftext|>\n",
            "inputs_len : 49\n",
            "generated text :  deficit is the single-biggest factor driving down the federal budget deficit<|endoftext|>\n",
            "inputs_len : 59\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 60\n",
            "generated text :  of the wind power and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar and solar\n",
            "inputs_len : 46\n",
            "generated text :  the bill to allow women to have birth control.<|endoftext|>\n",
            "inputs_len : 36\n",
            "generated text : . of women use birth control.<|endoftext|>\n",
            "inputs_len : 38\n",
            "generated text :  to keep the shutdown going.<|endoftext|>\n",
            "inputs_len : 40\n",
            "generated text :  the Texas nurses from other countries.<|endoftext|>\n",
            "inputs_len : 44\n",
            "generated text : . to. to. to. to.<|endoftext|>\n",
            "inputs_len : 52\n",
            "generated text :  prison to keep up with our mass incarceration explosion of nonviolent offenders.<|endoftext|>\n",
            "inputs_len : 36\n",
            "generated text :  the AFL-CIO.<|endoftext|>\n",
            "inputs_len : 36\n",
            "generated text :  \" \"<|endoftext|>\n",
            "inputs_len : 44\n",
            "generated text :  the only Republican candidate who could beat Obama in Texas<|endoftext|>\n",
            "inputs_len : 38\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 52\n",
            "generated text :  the average student in Florida, what they actually pay out of pocket at our major universities for tuition is as much or less than what they spend on cellphones.<|endoftext|>\n",
            "inputs_len : 47\n",
            "generated text : <|endoftext|>\n",
            "inputs_len : 57\n",
            "generated text :  a foreclosure tax that would take effect in 2013 could mean a high tax bill for those facing foreclosure and millions of families who modified their mortgage or had a short sale through their lender.<|endoftext|>\n",
            "inputs_len : 56\n",
            "generated text :  the minimum wage would help lift over a million Americans out of poverty<|endoftext|>\n",
            "inputs_len : 77\n",
            "generated text :  said that the Reagan administration did not engage in terrorism. Kerry said that the Reagan administration did not engage in terrorism. Kerry said that the Reagan administration did not engage in terrorism. Kerry said that the Reagan administration did not engage in terrorism. Kerry said that the Reagan administration did not engage in terrorism. Kerry said that the Reagan administration did not engage in terrorism. Kerry said that the Reagan administration did not engage in terrorism. Kerry said that the Reagan administration did not engage in terrorism. Kerry said that the Reagan administration did not engage in terrorism. Kerry said that the Reagan administration did not engage in terrorism. Kerry said that the Reagan administration did not engage\n",
            "inputs_len : 40\n",
            "generated text :  Obama has always had a 100 percent prochoice rating.<|endoftext|>\n",
            "inputs_len : 79\n",
            "generated text :  $. $. $. $.<|endoftext|>\n",
            "inputs_len : 53\n",
            "generated text :  $4 billion.<|endoftext|>\n",
            "inputs_len : 51\n",
            "generated text :  would not have to lay off teachers, firefighters and police officers<|endoftext|>\n",
            "inputs_len : 51\n",
            "generated text :  not to have to search for work.<|endoftext|>\n",
            "inputs_len : 70\n",
            "generated text :  $2,500 per year.<|endoftext|>\n",
            "inputs_len : 59\n",
            "generated text :  groups that had progressive in their name.<|endoftext|>\n",
            "inputs_len : 35\n",
            "generated text :  the fastest pace since 1999.<|endoftext|>\n",
            "inputs_len : 48\n",
            "generated text :  not to meet with Netanyahu in New York.<|endoftext|>\n",
            "inputs_len : 57\n",
            "generated text :  the same.<|endoftext|>\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/gpt2/small/train/true_2024-01-19 16:34_true_batch_size=1.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text :  the wall will take years.<|endoftext|>\n",
            "inputs_len : 44\n",
            "generated text : , the plan that will cut choice for Medicare Advantage seniors.<|endoftext|>\n",
            "inputs_len : 53\n",
            "generated text :  $ $ $ $ $ $<|endoftext|>\n",
            "inputs_len : 71\n",
            "generated text : . $. $. $. $.<|endoftext|>\n",
            "inputs_len : 49\n",
            "generated text :  to go back to the gay-and-lesbians.<|endoftext|>\n",
            "inputs_len : 35\n",
            "generated text :  the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "inputs_len : 42\n",
            "generated text :  the unemployment rate for college graduates is 4.4 percent and over 10 percent for noncollege-educated.<|endoftext|>\n",
            "inputs_len : 64\n",
            "generated text : . \"<|endoftext|>\n",
            "inputs_len : 44\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 68\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 38\n",
            "generated text :  Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio Rubio\n",
            "inputs_len : 32\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 54\n",
            "generated text : , Ginsburg, Ginsburg, Ginsburg, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins, Gins,\n",
            "inputs_len : 42\n",
            "generated text :  tuition, but now claims to champion affordability.<|endoftext|>\n",
            "inputs_len : 56\n",
            "generated text : , that would be, the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of the rate of\n",
            "inputs_len : 45\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 30\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 49\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 30\n",
            "generated text :  turnout rate.<|endoftext|>\n",
            "inputs_len : 66\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 54\n",
            "generated text : . to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to\n",
            "inputs_len : 60\n",
            "generated text :  the Fed created $1.2 trillion out of nothing, gave it to banks, and some of them foreign banks, so that they could stabilize their operations.<|endoftext|>\n",
            "inputs_len : 53\n",
            "generated text :  $10 billion since we successfully fought to restore Texas sales tax deduction<|endoftext|>\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/gpt2/small/test/true_2024-01-19 16:34_true_batch_size=1.tsv\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "inputs_len : 37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text :  of the state of Oregon.<|endoftext|>\n",
            "inputs_len : 60\n",
            "generated text :  stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus stimulus\n",
            "inputs_len : 58\n",
            "generated text :  to the state of Tennessee. to the state of Tennessee.<|endoftext|>\n",
            "inputs_len : 62\n",
            "generated text :  said he would not rent apartments to African-Americans.<|endoftext|>\n",
            "inputs_len : 37\n",
            "generated text :  to the state of Texas. to the state of Texas. to the state of Texas. to the state of Texas. to the state of Texas. to the state of Texas. to the state of Texas. to the state of Texas. to the state of Texas. to the state of Texas. to the state of Texas. to the state of Texas. to the state of Texas. to the state of Texas. to the state of Texas. to the state of Texas. to the state of Texas. to the state of Texas. to the state of Texas. to the state of Texas. to the state of Texas. to the\n",
            "inputs_len : 53\n",
            "generated text :  jobs would be lost.<|endoftext|>\n",
            "inputs_len : 32\n",
            "generated text :  state- budget estimates have missed the mark.<|endoftext|>\n",
            "inputs_len : 56\n",
            "generated text :  the median income of a middle class family went down $2,100 from 2001 to 2007.<|endoftext|>\n",
            "inputs_len : 50\n",
            "generated text : . Perry has advocated abandoning Social Security, scuttling Medicaid and ending the federal income tax.<|endoftext|>\n",
            "inputs_len : 57\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 31\n",
            "generated text :  the first 100 days of this term.<|endoftext|>\n",
            "inputs_len : 65\n",
            "generated text :  not.<|endoftext|>\n",
            "inputs_len : 34\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 48\n",
            "generated text :  $500 million.<|endoftext|>\n",
            "inputs_len : 41\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 50\n",
            "generated text : .<|endoftext|>\n",
            "inputs_len : 64\n",
            "generated text :  the rate is about 8 percent.<|endoftext|>\n",
            "inputs_len : 41\n",
            "generated text :  jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs. jobs.\n",
            "inputs_len : 51\n",
            "generated text :  the Wisconsin state of Wisconsin has access to health-care.<|endoftext|>\n",
            "inputs_len : 53\n",
            "generated text : .<|endoftext|>\n",
            "the extended data was saved in /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/gpt2/small/val/true_2024-01-19 16:34_true_batch_size=1.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 情報の定義\n",
        "# tr_model_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/gpt2/small/2024-01-16 18:36/batch_size=1'\n",
        "# model_config = gpt2_small_config_for_prediction\n",
        "\n",
        "# # load_transformers_model()\n",
        "# model_from_pretrained = load_transformers_model('gpt2', tr_model_path)\n",
        "\n",
        "# # prepare_tokenizerによるtokenizerに関わるもろもろの情報の取得\n",
        "# my_prepare_tokenizer = Prepare_Tokenizer(gpt2_small_config)\n",
        "# my_prepare_tokenizer.get_tokenizer_info()\n",
        "# my_tokenizer = my_prepare_tokenizer.get_tokenizer()\n",
        "# my_prepare_tokenizer.get_tokenizer_info()\n",
        "\n",
        "# # データのロード\n",
        "# train_data = read_tsv(train_and_test_config[\"TRAINING_DATA_PATH\"])\n",
        "# test_data = read_tsv(train_and_test_config[\"TEST_DATA_PATH\"])\n",
        "# val_data = read_tsv(train_and_test_config[\"VALIDATION_DATA_PATH\"])\n",
        "\n",
        "# # predict()の実行\n",
        "# extended_train_data = predict(my_tokenizer, my_prepare_tokenizer, model_from_pretrained, model_config, train_data[:3])\n",
        "# extended_test_data = predict(my_tokenizer, my_prepare_tokenizer, model_from_pretrained, model_config, test_data[:3])\n",
        "# extended_val_data = predict(my_tokenizer, my_prepare_tokenizer, model_from_pretrained, model_config, val_data[:3])\n",
        "\n",
        "# # 拡張したデータの保存\n",
        "# write_dataframe_in_tsv(extended_train_data, path_dict['toy_extended_train_data_true'])\n",
        "# write_dataframe_in_tsv(extended_test_data, path_dict['toy_extended_train_data_true'])\n",
        "# write_dataframe_in_tsv(extended_val_data, path_dict['toy_extended_train_data_true'])"
      ],
      "metadata": {
        "id": "-RwfqUXT2YHq"
      },
      "execution_count": 310,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extended_train_data"
      ],
      "metadata": {
        "id": "5xSeSE0n0-Jl"
      },
      "execution_count": 311,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# row_1 = extended_train_data.iloc[1]\n",
        "# for i in range(len(row_1)):\n",
        "#     print(row_1[i])"
      ],
      "metadata": {
        "id": "R-hSgVLavnxB"
      },
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# series = pd.DataFrame(['its like'])\n",
        "\n",
        "# frame = pd.DataFrame(['1'])\n",
        "\n",
        "# o = pd.concat([series, frame], axis=1)\n",
        "\n",
        "# j = pd.concat([o,o], axis=0)\n",
        "# j"
      ],
      "metadata": {
        "id": "plbE-ZUliWWk"
      },
      "execution_count": 314,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input = \"Statement: Building a wall on the U.S.-Mexico border will take literally years. Metadata: immigration rick-perry Governor Texas republican Radio interview[EXP]\"\n",
        "# model_name = 'gpt2'\n",
        "# predict(model_from_pretrained, model_name, input)"
      ],
      "metadata": {
        "id": "bkRkaJ_Hs_27"
      },
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def predict_single_item()"
      ],
      "metadata": {
        "id": "lSPUHtxpKskA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p11tfx25tR73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 以降メモ"
      ],
      "metadata": {
        "id": "ljIKzJBCXzur"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIcmjm_ShvOG"
      },
      "source": [
        "# GPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebbsKHDktRwJ"
      },
      "source": [
        "## 推論"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9VQsfvyI7Gw"
      },
      "outputs": [],
      "source": [
        "## modelのロード\n",
        "my_model = CEG.load_from_checkpoint(gpt2_small_config['SAVED_MODEL_PATH']+'/QTag-epoch=20-val_loss=7.58.ckpt')\n",
        "my_model.model.save_pretrained('./model_transformers/gpt2_small')\n",
        "my_model_from_pretrained = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/gpt2_small')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUo_aStv4pZA"
      },
      "outputs": [],
      "source": [
        "my_model_from_pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAR1j9UE6y_W"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, AutoConfig, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/gpt2_small')\n",
        "\n",
        "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
        "input_length = len(inputs[\"input_ids\"][0])\n",
        "print(f\"inputs_len : {input_length}\")\n",
        "\n",
        "# greedy_search\n",
        "# outputs = model.generate(**inputs, max_new_tokens=15, return_dict_in_generate=True, output_scores=True)\n",
        "# beam_search\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=5,\n",
        "    num_beams=4,\n",
        "    num_return_sequences=1,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        ")\n",
        "print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "tokens_list = outputs.sequences[0]\n",
        "print(tokens_list)\n",
        "full_text = tokenizer.decode(tokens_list)\n",
        "generated = tokenizer.decode(generated_tokens[0])\n",
        "\n",
        "print(f\"full text : {full_text}\\n\")\n",
        "print(f\"generated text: {generated}\\n\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "z2_lpCRDuYqo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "outputId": "0835a782-3cdd-4b57-c697-8f0be46a34b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there is no information. you need to call get_tokenizer() first.\n",
            "tokenizer : AutoTokenizer.from_pretrained(gpt2)\n",
            "Before adding additional_special_tokens\n",
            "['<|endoftext|>']\n",
            "[50256]\n",
            "After adding additional_special_tokens\n",
            "['<|endoftext|>', '[EXP]']\n",
            "[50256, 50257]\n",
            "exp_id : 50257\n",
            "eos_id : 50256\n",
            "pad_id : 50256\n",
            "len(tokenizer) : 50258\n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : gpt2\n",
            "self.tokenizer : GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['[EXP]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t50257: AddedToken(\"[EXP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : 50258\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : <|endoftext|>\n",
            "self.eos_token_id : 50256\n",
            "self.pad_token : <|endoftext|>\n",
            "self.pad_token_id : 50256\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : 50257\n",
            "inputs_len : 33\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'my_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-b994aee1b337>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"inputs_len : {input_length}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m outputs = my_model.generate(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'my_model' is not defined"
          ]
        }
      ],
      "source": [
        "## tokenzierの準備\n",
        "my_prepare_tokenizer = Prepare_Tokenizer(gpt2_small_config)\n",
        "my_prepare_tokenizer.get_tokenizer_info()\n",
        "my_tokenizer = my_prepare_tokenizer.get_tokenizer()\n",
        "my_prepare_tokenizer.get_tokenizer_info()\n",
        "\n",
        "# データの準備\n",
        "inputs = my_tokenizer([\"Statement: Building a wall on the U.S.-Mexico border will take literally years. Metadata: immigration rick-perry Governor Texas republican Radio interview[EXP]\"], return_tensors=\"pt\")\n",
        "input_length = len(inputs[\"input_ids\"][0])\n",
        "print(f\"inputs_len : {input_length}\")\n",
        "\n",
        "outputs = my_model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=5,\n",
        "    num_beams=4,\n",
        "    num_return_sequences=1,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        ")\n",
        "print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "tokens_list = outputs.sequences[0]\n",
        "print(tokens_list)\n",
        "full_text = my_tokenizer.decode(tokens_list)\n",
        "generated = my_tokenizer.decode(generated_tokens[0])\n",
        "\n",
        "print(f\"full text : {full_text}\\n\")\n",
        "print(f\"generated text: {generated}\\n\")\n",
        "print(f\"len(generated) : {len(my_tokenizer.tokenize(generated))}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9On7S6T12TbR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bai7yul1huXr"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, AutoConfig, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
        "input_length = len(inputs[\"input_ids\"][0])\n",
        "print(f\"inputs_len : {input_length}\")\n",
        "\n",
        "# greedy_search\n",
        "# outputs = model.generate(**inputs, max_new_tokens=15, return_dict_in_generate=True, output_scores=True)\n",
        "# beam_search\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=5,\n",
        "    num_beams=4,\n",
        "    num_return_sequences=1,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        ")\n",
        "print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "tokens_list = outputs.sequences[0]\n",
        "print(tokens_list)\n",
        "full_text = tokenizer.decode(tokens_list)\n",
        "generated = tokenizer.decode(generated_tokens[0])\n",
        "\n",
        "print(f\"full text : {full_text}\\n\")\n",
        "print(f\"generated text: {generated}\\n\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRkLjp0h73VM"
      },
      "source": [
        "# T5デモ\n",
        "- `<extra_id_0>`や`</s>`が出力に含まれてしまっている\n",
        "- lm_head.weightsを初期化してもあまり効果が無い\n",
        "- https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py\n",
        "- https://www.kaggle.com/code/kreeshrajani/fine-tune-t5-for-conversational-model\n",
        "- https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Tokenizer.sp_model_kwargs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CSsvMsOyROy"
      },
      "source": [
        "## 公式サイトより"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WZvyBATyUEI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# training\n",
        "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
        "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
        "outputs = model(input_ids=input_ids, labels=labels)\n",
        "loss = outputs.loss\n",
        "logits = outputs.logits\n",
        "\n",
        "# inference\n",
        "input_ids = tokenizer(\n",
        "    \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
        ").input_ids  # Batch size 1\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "# studies have shown that owning a dog is good for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncxz-bFVuQmS"
      },
      "source": [
        "## fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "id": "F2IFfkHWg5jg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05177341-3a13-4ab9-9705-42012a744a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before\n",
            "['</s>', '<unk>', '<pad>', '<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']\n",
            "[1, 2, 0, 32099, 32098, 32097, 32096, 32095, 32094, 32093, 32092, 32091, 32090, 32089, 32088, 32087, 32086, 32085, 32084, 32083, 32082, 32081, 32080, 32079, 32078, 32077, 32076, 32075, 32074, 32073, 32072, 32071, 32070, 32069, 32068, 32067, 32066, 32065, 32064, 32063, 32062, 32061, 32060, 32059, 32058, 32057, 32056, 32055, 32054, 32053, 32052, 32051, 32050, 32049, 32048, 32047, 32046, 32045, 32044, 32043, 32042, 32041, 32040, 32039, 32038, 32037, 32036, 32035, 32034, 32033, 32032, 32031, 32030, 32029, 32028, 32027, 32026, 32025, 32024, 32023, 32022, 32021, 32020, 32019, 32018, 32017, 32016, 32015, 32014, 32013, 32012, 32011, 32010, 32009, 32008, 32007, 32006, 32005, 32004, 32003, 32002, 32001, 32000]\n",
            "After\n",
            "['</s>', '<unk>', '<pad>', '[EXP]']\n",
            "[1, 2, 0, 32100]\n",
            "exp_id : 32100\n",
            "eos_id : 1\n",
            "pad_id : 0\n",
            "\n",
            "len(tokenizer) : 32101\n",
            "before : torch.Size([32128, 512])\n",
            "before resizing : Parameter containing:\n",
            "tensor([[ -2.0156,   0.2236,  -7.0938,  ...,  -0.3535,   2.6406,  -2.8906],\n",
            "        [ 12.6250,   8.1875, -11.6250,  ...,   7.9375,  -7.3125,   0.9453],\n",
            "        [ -8.7500,   7.1875,  27.8750,  ..., -26.7500,   0.8555,  -1.5156],\n",
            "        ...,\n",
            "        [-25.2500, -28.5000, -17.2500,  ..., -17.7500,  -5.2500,  27.3750],\n",
            "        [-25.5000, -29.3750, -18.2500,  ..., -17.7500,  -4.8125,  27.7500],\n",
            "        [-26.7500, -28.3750, -17.8750,  ..., -18.5000,  -7.0000,  27.6250]],\n",
            "       requires_grad=True)\n",
            "after :  torch.Size([32101, 512])\n",
            "after resizing : torch.Size([32101, 512])\n",
            "before changing weights : torch.Size([32101, 512])\n",
            "focus : tensor([[ -2.0156,   0.2236,  -7.0938,  ...,  -0.3535,   2.6406,  -2.8906],\n",
            "        [ 12.6250,   8.1875, -11.6250,  ...,   7.9375,  -7.3125,   0.9453],\n",
            "        [ -8.7500,   7.1875,  27.8750,  ..., -26.7500,   0.8555,  -1.5156],\n",
            "        ...,\n",
            "        [-13.1875,   5.2188, -18.0000,  ...,  14.0625,  19.3750,  -0.2422],\n",
            "        [-15.4375,   8.7500,   6.9688,  ...,   6.7500,   4.2812,  -0.4199],\n",
            "        [  7.8438,   8.5625,  -4.3750,  ...,   0.6719,  -2.4688,  -4.1562]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "focus.shape : torch.Size([32100, 512])\n",
            "after changing weights : Parameter containing:\n",
            "tensor([[-2.0156e+00,  2.2363e-01, -7.0938e+00,  ..., -3.5352e-01,\n",
            "          2.6406e+00, -2.8906e+00],\n",
            "        [ 1.2625e+01,  8.1875e+00, -1.1625e+01,  ...,  7.9375e+00,\n",
            "         -7.3125e+00,  9.4531e-01],\n",
            "        [-8.7500e+00,  7.1875e+00,  2.7875e+01,  ..., -2.6750e+01,\n",
            "          8.5547e-01, -1.5156e+00],\n",
            "        ...,\n",
            "        [-1.5438e+01,  8.7500e+00,  6.9688e+00,  ...,  6.7500e+00,\n",
            "          4.2812e+00, -4.1992e-01],\n",
            "        [ 7.8438e+00,  8.5625e+00, -4.3750e+00,  ...,  6.7188e-01,\n",
            "         -2.4688e+00, -4.1562e+00],\n",
            "        [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
            "         -1.0000e+04, -1.0000e+04]], requires_grad=True)\n",
            "new_weights.shape : torch.Size([32101, 512])\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "\n",
        "print(\"Before\")\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "special_tokens_dict = {\n",
        "    'additional_special_tokens': ['[EXP]']\n",
        "    }\n",
        "\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "exp_id = tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "eos_id = tokenizer.convert_tokens_to_ids('</s>')\n",
        "pad_id = tokenizer.convert_tokens_to_ids('<pad>')\n",
        "print(\"After\")\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "print(f\"exp_id : {exp_id}\")\n",
        "print(f\"eos_id : {eos_id}\")\n",
        "print(f\"pad_id : {pad_id}\\n\")\n",
        "print(f\"len(tokenizer) : {len(tokenizer)}\")\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "print(f\"before : {model.lm_head.weight.shape}\")\n",
        "print(f\"before resizing : {model.lm_head.weight}\")\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(\"after : \", model.lm_head.weight.shape)\n",
        "print(f\"after resizing : {model.lm_head.weight.shape}\")\n",
        "print(f\"before changing weights : {model.lm_head.weight.shape}\")\n",
        "print(f\"focus : {model.lm_head.weight[:-1, :]}\")\n",
        "print(f\"focus.shape : {model.lm_head.weight[:-1, :].shape}\")\n",
        "new_weights = torch.cat([model.lm_head.weight[:-1, :], torch.zeros(1, model.lm_head.weight.shape[1]) -10000])\n",
        "model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "print(f\"after changing weights : {model.lm_head.weight}\")\n",
        "print(f\"new_weights.shape : {model.lm_head.weight.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "4eFQyzQEhVTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21344255-27f1-4bb4-d03b-aa38d92dcb96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized input_text : ['▁Statement', ':', '▁When', '▁did', '▁the', '▁decline', '▁of', '▁coal', '▁start', '?', '▁It', '▁started', '▁when', '▁natural', '▁gas', '▁took', '▁off', '▁that', '▁started', '▁to', '▁begin', '▁in', '▁(', 'P', 'resident', '▁George', '▁W', '.', ')', '▁Bush', 's', '▁administration', '.', '▁Meta', 'data', ':', '▁energy', ',', 'his', 'tory', ',', 'job', '-', 'acco', 'mp', 'l', 'ish', 'ments', '▁', 's', 'cott', '-', 'sur', 'o', 've', 'll', '▁State', '▁de', 'legate', '▁Virginia', '▁', 'democrat', '▁', 'a', '▁floor', '▁speech', '.', '[EXP]']\n",
            "\n",
            "tokenized label_text : ['▁Statement', ':', '▁When', '▁did', '▁the', '▁decline', '▁of', '▁coal', '▁start', '?', '▁It', '▁started', '▁when', '▁natural', '▁gas', '▁took', '▁off', '▁that', '▁started', '▁to', '▁begin', '▁in', '▁(', 'P', 'resident', '▁George', '▁W', '.', ')', '▁Bush', 's', '▁administration', '.', '▁Meta', 'data', ':', '▁energy', ',', 'his', 'tory', ',', 'job', '-', 'acco', 'mp', 'l', 'ish', 'ments', '▁', 's', 'cott', '-', 'sur', 'o', 've', 'll', '▁State', '▁de', 'legate', '▁Virginia', '▁', 'democrat', '▁', 'a', '▁floor', '▁speech', '.', '[EXP]', '▁Sur', 'o', 've', 'll', '▁said', '▁the', '▁decline', '▁of', '▁coal', '▁\"', 'star', 'ted', '▁when', '▁natural', '▁gas', '▁took', '▁off', '▁That', '▁started', '▁to', '▁begin', '▁in', '▁President', '▁(', 'George', '▁W', '.', '▁', ')', '▁Bush', 's', '▁administration', '.', '▁\"', 'No', '▁doubt', ',', '▁natural', '▁gas', '▁has', '▁been', '▁', 'gaining', '▁ground', '▁on', '▁coal', '▁in', '▁', 'generating', '▁electricity', '.', '▁The', '▁trend', '▁started', '▁in', '▁the', '▁1990', 's', '▁but', '▁clearly', '▁gained', '▁speed', '▁during', '▁the', '▁Bush', '▁administration', '▁when', '▁the', '▁production', '▁of', '▁natural', '▁gas', '▁--', '▁', 'a', '▁competitor', '▁of', '▁coal', '▁--', '▁picked', '▁up', '.', '▁But', '▁analysts', '▁give', '▁little', '▁credit', '▁or', '▁blame', '▁to', '▁Bush', '▁for', '▁that', '▁trend', '.', '▁They', '▁note', '▁that', '▁other', '▁factors', ',', '▁such', '▁as', '▁technological', '▁innovation', ',', '▁', 'entrepreneurship', '▁and', '▁policies', '▁of', '▁previous', '▁administration', 's', ',', '▁had', '▁more', '▁to', '▁do', '▁with', '▁', 'laying', '▁the', '▁ground', 'work', '▁for', '▁the', '▁natural', '▁gas', '▁boom', '.', '</s>']\n",
            "input_text_length : 68\n",
            "input_encoding : {'input_ids': tensor([[16836,    10,   366,   410,     8,  7198,    13,  8416,   456,    58,\n",
            "            94,   708,   116,   793,  1807,   808,   326,    24,   708,    12,\n",
            "          1731,    16,    41,   345, 15704,  3080,   549,     5,    61,  8905,\n",
            "             7,  3602,     5, 14204,  6757,    10,   827,     6, 10193, 10972,\n",
            "             6, 16899,    18, 21007,  1167,    40,  1273,  4128,     3,     7,\n",
            "         10405,    18,  3042,    32,   162,   195,  1015,    20,  8791,  5382,\n",
            "             3, 23319,     3,     9,  1501,  5023,     5, 32100,     1,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "\n",
            "label_encoding : {'input_ids': tensor([[16836,    10,   366,   410,     8,  7198,    13,  8416,   456,    58,\n",
            "            94,   708,   116,   793,  1807,   808,   326,    24,   708,    12,\n",
            "          1731,    16,    41,   345, 15704,  3080,   549,     5,    61,  8905,\n",
            "             7,  3602,     5, 14204,  6757,    10,   827,     6, 10193, 10972,\n",
            "             6, 16899,    18, 21007,  1167,    40,  1273,  4128,     3,     7,\n",
            "         10405,    18,  3042,    32,   162,   195,  1015,    20,  8791,  5382,\n",
            "             3, 23319,     3,     9,  1501,  5023,     5, 32100,  3705,    32,\n",
            "           162,   195,   243,     8,  7198,    13,  8416,    96,  3624,  1054,\n",
            "           116,   793,  1807,   808,   326,   466,   708,    12,  1731,    16,\n",
            "          1661,    41, 31317,   549,     5,     3,    61,  8905,     7,  3602,\n",
            "             5,    96,  4168,  3228,     6,   793,  1807,    65,   118,     3,\n",
            "         11866,  1591,    30,  8416,    16,     3, 11600,  6373,     5,    37,\n",
            "          4166,   708,    16,     8,  5541,     7,    68,  3133,  6886,  1634,\n",
            "           383,     8,  8905,  3602,   116,     8,   999,    13,   793,  1807,\n",
            "          1636,     3,     9, 18766,    13,  8416,  1636,  4758,    95,     5,\n",
            "           299, 15639,   428,   385,   998,    42,  9100,    12,  8905,    21,\n",
            "            24,  4166,     5,   328,  2232,    24,   119,  2580,     6,   224,\n",
            "            38,  9974,  4337,     6,     3, 24849,    11,  3101,    13,  1767,\n",
            "          3602,     7,     6,   141,    72,    12,   103,    28,     3, 14720,\n",
            "             8,  1591,  1981,    21,     8,   793,  1807, 13997,     5,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "input prompt : Statement: When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration. Metadata: energy,history,job-accomplishments scott-surovell State delegate Virginia democrat a floor speech.[EXP]</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "label : Statement: When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration. Metadata: energy,history,job-accomplishments scott-surovell State delegate Virginia democrat a floor speech.[EXP] Surovell said the decline of coal \"started when natural gas took off That started to begin in President (George W. ) Bushs administration. \"No doubt, natural gas has been gaining ground on coal in generating electricity. The trend started in the 1990s but clearly gained speed during the Bush administration when the production of natural gas -- a competitor of coal -- picked up. But analysts give little credit or blame to Bush for that trend. They note that other factors, such as technological innovation, entrepreneurship and policies of previous administrations, had more to do with laying the groundwork for the natural gas boom.</s>\n",
            "labels before -100 masking : tensor([[16836,    10,   366,   410,     8,  7198,    13,  8416,   456,    58,\n",
            "            94,   708,   116,   793,  1807,   808,   326,    24,   708,    12,\n",
            "          1731,    16,    41,   345, 15704,  3080,   549,     5,    61,  8905,\n",
            "             7,  3602,     5, 14204,  6757,    10,   827,     6, 10193, 10972,\n",
            "             6, 16899,    18, 21007,  1167,    40,  1273,  4128,     3,     7,\n",
            "         10405,    18,  3042,    32,   162,   195,  1015,    20,  8791,  5382,\n",
            "             3, 23319,     3,     9,  1501,  5023,     5, 32100,  3705,    32,\n",
            "           162,   195,   243,     8,  7198,    13,  8416,    96,  3624,  1054,\n",
            "           116,   793,  1807,   808,   326,   466,   708,    12,  1731,    16,\n",
            "          1661,    41, 31317,   549,     5,     3,    61,  8905,     7,  3602,\n",
            "             5,    96,  4168,  3228,     6,   793,  1807,    65,   118,     3,\n",
            "         11866,  1591,    30,  8416,    16,     3, 11600,  6373,     5,    37,\n",
            "          4166,   708,    16,     8,  5541,     7,    68,  3133,  6886,  1634,\n",
            "           383,     8,  8905,  3602,   116,     8,   999,    13,   793,  1807,\n",
            "          1636,     3,     9, 18766,    13,  8416,  1636,  4758,    95,     5,\n",
            "           299, 15639,   428,   385,   998,    42,  9100,    12,  8905,    21,\n",
            "            24,  4166,     5,   328,  2232,    24,   119,  2580,     6,   224,\n",
            "            38,  9974,  4337,     6,     3, 24849,    11,  3101,    13,  1767,\n",
            "          3602,     7,     6,   141,    72,    12,   103,    28,     3, 14720,\n",
            "             8,  1591,  1981,    21,     8,   793,  1807, 13997,     5,     1]])\n",
            "labels after -100 masking : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  3705,    32,\n",
            "           162,   195,   243,     8,  7198,    13,  8416,    96,  3624,  1054,\n",
            "           116,   793,  1807,   808,   326,   466,   708,    12,  1731,    16,\n",
            "          1661,    41, 31317,   549,     5,     3,    61,  8905,     7,  3602,\n",
            "             5,    96,  4168,  3228,     6,   793,  1807,    65,   118,     3,\n",
            "         11866,  1591,    30,  8416,    16,     3, 11600,  6373,     5,    37,\n",
            "          4166,   708,    16,     8,  5541,     7,    68,  3133,  6886,  1634,\n",
            "           383,     8,  8905,  3602,   116,     8,   999,    13,   793,  1807,\n",
            "          1636,     3,     9, 18766,    13,  8416,  1636,  4758,    95,     5,\n",
            "           299, 15639,   428,   385,   998,    42,  9100,    12,  8905,    21,\n",
            "            24,  4166,     5,   328,  2232,    24,   119,  2580,     6,   224,\n",
            "            38,  9974,  4337,     6,     3, 24849,    11,  3101,    13,  1767,\n",
            "          3602,     7,     6,   141,    72,    12,   103,    28,     3, 14720,\n",
            "             8,  1591,  1981,    21,     8,   793,  1807, 13997,     5,     1]])\n",
            "labels.shape : torch.Size([1, 200])\n"
          ]
        }
      ],
      "source": [
        "input_text = 'Statement: When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration. Metadata: energy,history,job-accomplishments scott-surovell State delegate Virginia democrat a floor speech.'+ tokenizer.convert_ids_to_tokens(exp_id)\n",
        "justification_text = 'Surovell said the decline of coal \"started when natural gas took off  That started to begin in President (George W. ) Bushs administration. \"No doubt, natural gas has been gaining ground on coal in generating electricity. The trend started in the 1990s but clearly gained speed during the Bush administration when the production of natural gas -- a competitor of coal -- picked up. But analysts give little credit or blame to Bush for that trend. They note that other factors, such as technological innovation, entrepreneurship and policies of previous administrations, had more to do with laying the groundwork for the natural gas boom.'\n",
        "label_text = input_text  + justification_text + tokenizer.convert_ids_to_tokens(eos_id)\n",
        "print(f\"tokenized input_text : {tokenizer.tokenize(input_text)}\\n\")\n",
        "print(f\"tokenized label_text : {tokenizer.tokenize(label_text)}\")\n",
        "input_text_length = len(tokenizer.tokenize(input_text))\n",
        "justification_text_length = len(tokenizer.tokenize(justification_text))\n",
        "label_text_length = len(tokenizer.tokenize(label_text))\n",
        "print(f\"input_text_length : {input_text_length}\")\n",
        "\n",
        "input_encoding = tokenizer.encode_plus(\n",
        "    input_text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=label_text_length,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "\n",
        "print(f\"input_encoding : {input_encoding}\\n\")\n",
        "decoded_input_encoding = tokenizer.decode(input_encoding[\"input_ids\"][0])\n",
        "# print(f\"decoded input_encoding : {decoded_input_encoding}\\n\")\n",
        "\n",
        "label_encoding = tokenizer.encode_plus(\n",
        "    label_text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=label_text_length,\n",
        "    return_token_type_ids=False,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "\n",
        "print(f\"label_encoding : {label_encoding}\")\n",
        "# print(f\"label_encoding['input_ids'][0] : {label_encoding['input_ids'][0]}\")\n",
        "decoded_label_encoding = tokenizer.decode(label_encoding[\"input_ids\"][0])\n",
        "# print(f\"decoded label_encoding : {decoded_label_encoding}\")\n",
        "print(f\"input prompt : {decoded_input_encoding}\")\n",
        "print(f\"label : {decoded_label_encoding}\")\n",
        "\n",
        "labels = label_encoding[\"input_ids\"]\n",
        "print(f\"labels before -100 masking : {labels}\")\n",
        "# decoded_labels = tokenizer.decode(labels)\n",
        "# print(f\"decoded labels : {decoded_labels}\")\n",
        "labels[:, :input_text_length] = -100 # cross_entropy_ignore_index\n",
        "labels[:, label_text_length:] = -100\n",
        "print(f\"labels after -100 masking : {labels}\")\n",
        "print(f\"labels.shape : {labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "id": "QYIrs850a-UJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf6349cd-5a36-4dd3-fabb-a162479d409e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model : T5ForConditionalGeneration(\n",
            "  (shared): Embedding(32101, 512)\n",
            "  (encoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32101, 512)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 8)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
            "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-5): 5 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
            "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (decoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32101, 512)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 8)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
            "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-5): 5 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
            "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=512, out_features=32101, bias=False)\n",
            ")\n",
            "type(model) : <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n",
            "type(outputs) : <class 'transformers.modeling_outputs.Seq2SeqLMOutput'>\n",
            "logits : tensor([[[-2.5476e+01, -1.2861e+01, -1.7625e+01,  ..., -1.3640e+01,\n",
            "          -3.2110e+00,  1.0486e+04],\n",
            "         [-2.5476e+01, -1.2861e+01, -1.7625e+01,  ..., -1.3640e+01,\n",
            "          -3.2110e+00,  1.0486e+04],\n",
            "         [-2.5476e+01, -1.2861e+01, -1.7625e+01,  ..., -1.3640e+01,\n",
            "          -3.2110e+00,  1.0486e+04],\n",
            "         ...,\n",
            "         [-5.4455e+01, -4.7042e+00, -2.2898e+01,  ..., -2.8213e+01,\n",
            "          -3.0636e+01,  1.3202e+04],\n",
            "         [-5.0904e+01, -5.3404e+00, -2.2956e+01,  ..., -2.7929e+01,\n",
            "          -2.8834e+01,  1.3415e+04],\n",
            "         [-4.8555e+01, -5.0367e+00, -2.2519e+01,  ..., -2.7969e+01,\n",
            "          -2.8701e+01,  1.3469e+04]]], grad_fn=<UnsafeViewBackward0>)\n",
            "logtis.size() : torch.Size([1, 200, 32101])\n",
            "loss : 6978.9580078125\n",
            "output : tensor([[[1.2420e-05, 1.1084e-06, 7.3320e-07,  ..., 2.0707e-04,\n",
            "          1.4493e-02, 0.0000e+00],\n",
            "         [1.2420e-05, 1.1084e-06, 7.3320e-07,  ..., 2.0707e-04,\n",
            "          1.4493e-02, 0.0000e+00],\n",
            "         [1.2420e-05, 1.1084e-06, 7.3320e-07,  ..., 2.0707e-04,\n",
            "          1.4493e-02, 0.0000e+00],\n",
            "         ...,\n",
            "         [3.2251e-18, 3.8631e-03, 3.7582e-09,  ..., 9.7075e-11,\n",
            "          1.7817e-14, 0.0000e+00],\n",
            "         [1.1244e-16, 2.0448e-03, 3.5469e-09,  ..., 1.2893e-10,\n",
            "          1.0790e-13, 0.0000e+00],\n",
            "         [1.1777e-15, 2.7704e-03, 5.4941e-09,  ..., 1.2380e-10,\n",
            "          1.2329e-13, 0.0000e+00]]], grad_fn=<SoftmaxBackward0>)\n",
            "output.size() : torch.Size([1, 200, 32101])\n",
            "argmax(ouput) : tensor([[32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 23591,\n",
            "         19828,   195,  1015,    10, 24409,    13,  8416,  3511, 12416,  1054,\n",
            "         23562,   410,  1807,   808,   326,   793,   708,    12,  1731,    16,\n",
            "            41,  3080,   345,   549,     5,    61, 12703,  8905,  7289, 10030,\n",
            "             5,  7243, 10555,  8052, 16919,  2199, 24436, 14428,   582,  8154,\n",
            "         31036, 15290, 15362,  6089,  6490, 15777, 27470,     1,  4471, 26958,\n",
            "          7512,  3256,   116, 22965, 15559, 17774,     1,  1703,     1, 27721,\n",
            "         13830, 20417,     1,  6863,     1, 10538, 27512,    13, 30390,  5556,\n",
            "         16646, 31942, 17560, 22587, 18955,     1, 10762, 28448,    95,     1,\n",
            "         30752, 17945, 28270, 26147,  2736,     1, 15620,     1,  1792,    31,\n",
            "             1,    58,     1, 26539,  9098,     1, 25562,  2580,     1,     1,\n",
            "           587,     1, 14500,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1, 32100,     1,     1,     1]])\n",
            "argmax(ouput).size() : torch.Size([1, 200])\n",
            "argmax(output).dtype() : torch.int64\n",
            "tensor([[32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 23591,\n",
            "         19828,   195,  1015,    10, 24409,    13,  8416,  3511, 12416,  1054,\n",
            "         23562,   410,  1807,   808,   326,   793,   708,    12,  1731,    16,\n",
            "            41,  3080,   345,   549,     5,    61, 12703,  8905,  7289, 10030,\n",
            "             5,  7243, 10555,  8052, 16919,  2199, 24436, 14428,   582,  8154,\n",
            "         31036, 15290, 15362,  6089,  6490, 15777, 27470,     1,  4471, 26958,\n",
            "          7512,  3256,   116, 22965, 15559, 17774,     1,  1703,     1, 27721,\n",
            "         13830, 20417,     1,  6863,     1, 10538, 27512,    13, 30390,  5556,\n",
            "         16646, 31942, 17560, 22587, 18955,     1, 10762, 28448,    95,     1,\n",
            "         30752, 17945, 28270, 26147,  2736,     1, 15620,     1,  1792,    31,\n",
            "             1,    58,     1, 26539,  9098,     1, 25562,  2580,     1,     1,\n",
            "           587,     1, 14500,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1, 32100,     1,     1,     1]])\n",
            "generated : <extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0>posonsvettell State: hallway of coal startsdidted 1/2\" did gas took off natural started to begin in ( GeorgeP W.)7) Bushsburg regime.azăWhenthinglessly although gases flows become experiencingbooming momentum clearance behalf suggestssistedianu</s> suppliesinformatiileodor continues when Brig 1930′</s>cher</s> eyebrowively parliament</s> Administration</s> lemn déclin of zahărgas pumpscostingoyeznnouncing thereof</s> puts senzati up</s> investitii yeah speculate speeches detail</s>deal</s> avoid'</s>?</s> Haftung relate</s> pahar factors</s></s> wie</s> advancement</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>[EXP]</s></s></s>\n",
            "len(generated) : 203\n"
          ]
        }
      ],
      "source": [
        "outputs = model(input_ids=input_encoding[\"input_ids\"], attention_mask=input_encoding[\"attention_mask\"], labels=labels)\n",
        "\n",
        "print(f\"model : {model}\")\n",
        "print(f\"type(model) : {type(model)}\")\n",
        "print(f\"type(outputs) : {type(outputs)}\")\n",
        "\n",
        "logits = outputs.logits\n",
        "loss = outputs.loss\n",
        "print(f\"logits : {logits}\")\n",
        "print(f\"logtis.size() : {logits.size()}\")\n",
        "print(f\"loss : {loss}\")\n",
        "\n",
        "m = nn.Softmax(dim=1)\n",
        "output = m(logits)\n",
        "print(f\"output : {output}\")\n",
        "print(f\"output.size() : {output.size()}\")\n",
        "arg = torch.argmax(output, dim=2)\n",
        "print(f\"argmax(ouput) : {arg}\") # --> tensor\n",
        "print(f\"argmax(ouput).size() : {arg.size()}\")\n",
        "print(f\"argmax(output).dtype() : {arg.dtype}\")\n",
        "print(arg)\n",
        "generated = tokenizer.decode(arg[0])\n",
        "print(f\"generated : {generated}\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNWyFtg7uWzL"
      },
      "source": [
        "## 推論"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k20XUiAPuPB9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "inputs = tokenizer([\"summarize: Today is\"], return_tensors=\"pt\")\n",
        "input_length = len(inputs[\"input_ids\"][0])\n",
        "print(f\"inputs_len : {input_length}\")\n",
        "\n",
        "# greedy_search\n",
        "outputs = model.generate(**inputs, max_new_tokens=15, return_dict_in_generate=True, output_scores=True)\n",
        "# beam_search\n",
        "# outputs = model.generate(\n",
        "#     **inputs,\n",
        "#     max_new_tokens=5,\n",
        "#     num_beams=4,\n",
        "#     num_return_sequences=1,\n",
        "#     return_dict_in_generate=True,\n",
        "#     output_scores=True,\n",
        "# )\n",
        "print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "tokens_list = outputs.sequences[0]\n",
        "print(tokens_list)\n",
        "full_text = tokenizer.decode(tokens_list)\n",
        "generated = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"full text : {full_text}\\n\")\n",
        "print(f\"generated text: {generated}\\n\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoDbJrtWKA8k"
      },
      "source": [
        "# BART\n",
        "- add_special_token=Trueにしたとき、先頭に`<s>`が追加されるため-100のマスキング時にinput_length+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H9d_ni0KDdO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "print(\"Before\")\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "special_tokens_dict = {\n",
        "    'additional_special_tokens': ['[EXP]']\n",
        "    }\n",
        "\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "exp_id = tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "eos_id = tokenizer.convert_tokens_to_ids('</s>')\n",
        "pad_id = tokenizer.convert_tokens_to_ids('<pad>')\n",
        "print(\"After\")\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "print(f\"exp_id : {exp_id}\")\n",
        "print(f\"eos_id : {eos_id}\")\n",
        "print(f\"pad_id : {pad_id}\\n\")\n",
        "print(f\"len(tokenizer) : {len(tokenizer)}\")\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
        "# print(f\"before : {model.lm_head.weight.shape}\")\n",
        "# print(f\"before resizing : {model.lm_head.weight}\")\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# print(\"after : \", model.lm_head.weight.shape)\n",
        "# print(f\"after resizing : {model.lm_head.weight.shape}\")\n",
        "# print(f\"before changing weights : {model.lm_head.weight.shape}\")\n",
        "# print(f\"focus : {model.lm_head.weight[:-1, :]}\")\n",
        "# print(f\"focus.shape : {model.lm_head.weight[:-1, :].shape}\")\n",
        "# new_weights = torch.cat([model.lm_head.weight[:-1, :], torch.zeros(1, model.lm_head.weight.shape[1]) -10000])\n",
        "# model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "# print(f\"after changing weights : {model.lm_head.weight}\")\n",
        "# print(f\"new_weights.shape : {model.lm_head.weight.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS-7CkYAKTI0"
      },
      "outputs": [],
      "source": [
        "input_text = 'Statement: When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration. Metadata: energy,history,job-accomplishments scott-surovell State delegate Virginia democrat a floor speech.'+ tokenizer.convert_ids_to_tokens(exp_id)\n",
        "justification_text = 'Surovell said the decline of coal \"started when natural gas took off  That started to begin in President (George W. ) Bushs administration. \"No doubt, natural gas has been gaining ground on coal in generating electricity. The trend started in the 1990s but clearly gained speed during the Bush administration when the production of natural gas -- a competitor of coal -- picked up. But analysts give little credit or blame to Bush for that trend. They note that other factors, such as technological innovation, entrepreneurship and policies of previous administrations, had more to do with laying the groundwork for the natural gas boom.'\n",
        "label_text = input_text  + justification_text + tokenizer.convert_ids_to_tokens(eos_id)\n",
        "print(f\"tokenized input_text : {tokenizer.tokenize(input_text)}\\n\")\n",
        "print(f\"tokenized label_text : {tokenizer.tokenize(label_text)}\")\n",
        "input_text_length = len(tokenizer.tokenize(input_text))\n",
        "justification_text_length = len(tokenizer.tokenize(justification_text))\n",
        "label_text_length = len(tokenizer.tokenize(label_text))\n",
        "print(f\"input_text_length : {input_text_length}\")\n",
        "\n",
        "input_encoding = tokenizer.encode_plus(\n",
        "    input_text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=label_text_length,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "\n",
        "print(f\"input_encoding : {input_encoding}\\n\")\n",
        "decoded_input_encoding = tokenizer.decode(input_encoding[\"input_ids\"][0])\n",
        "# print(f\"decoded input_encoding : {decoded_input_encoding}\\n\")\n",
        "\n",
        "label_encoding = tokenizer.encode_plus(\n",
        "    label_text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=label_text_length,\n",
        "    return_token_type_ids=False,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "\n",
        "print(f\"label_encoding : {label_encoding}\")\n",
        "# print(f\"label_encoding['input_ids'][0] : {label_encoding['input_ids'][0]}\")\n",
        "decoded_label_encoding = tokenizer.decode(label_encoding[\"input_ids\"][0])\n",
        "# print(f\"decoded label_encoding : {decoded_label_encoding}\")\n",
        "print(f\"input prompt : {decoded_input_encoding}\")\n",
        "print(f\"label : {decoded_label_encoding}\")\n",
        "\n",
        "labels = label_encoding[\"input_ids\"]\n",
        "print(f\"labels before -100 masking : {labels}\")\n",
        "# decoded_labels = tokenizer.decode(labels)\n",
        "# print(f\"decoded labels : {decoded_labels}\")\n",
        "labels[:, :input_text_length+1] = -100 # cross_entropy_ignore_index\n",
        "labels[:, label_text_length:] = -100\n",
        "print(f\"labels after -100 masking : {labels}\")\n",
        "print(f\"labels.shape : {labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fy4pRgFbKYRU"
      },
      "outputs": [],
      "source": [
        "outputs = model(input_ids=input_encoding[\"input_ids\"], attention_mask=input_encoding[\"attention_mask\"], labels=labels)\n",
        "\n",
        "print(f\"model : {model}\")\n",
        "print(f\"type(model) : {type(model)}\")\n",
        "print(f\"type(outputs) : {type(outputs)}\")\n",
        "\n",
        "logits = outputs.logits\n",
        "loss = outputs.loss\n",
        "print(f\"logits : {logits}\")\n",
        "print(f\"logtis.size() : {logits.size()}\")\n",
        "print(f\"loss : {loss}\")\n",
        "\n",
        "m = nn.Softmax(dim=1)\n",
        "output = m(logits)\n",
        "print(f\"output : {output}\")\n",
        "print(f\"output.size() : {output.size()}\")\n",
        "arg = torch.argmax(output, dim=2)\n",
        "print(f\"argmax(ouput) : {arg}\") # --> tensor\n",
        "print(f\"argmax(ouput).size() : {arg.size()}\")\n",
        "print(f\"argmax(output).dtype() : {arg.dtype}\")\n",
        "print(arg)\n",
        "generated = tokenizer.decode(arg[0])\n",
        "print(f\"generated : {generated}\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ_6sUIJtXs5"
      },
      "source": [
        "## 推論"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOaCTJIdtZ_R"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOttzmfVIgVDY0BAdiSEat1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d3ac4456643447569282f723a9aec791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8f0f937f79847bb88971c724e52c7ff",
              "IPY_MODEL_784f9b0047d845f5a715b53bf8f1061f",
              "IPY_MODEL_40c642fe9b8440739e545f7553d657e8"
            ],
            "layout": "IPY_MODEL_f25a2ab8c12245938fc0ae61198d0a33"
          }
        },
        "e8f0f937f79847bb88971c724e52c7ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3189440e500d48529fc8abeac3fecdcd",
            "placeholder": "​",
            "style": "IPY_MODEL_bf2f0d82e6d9493699547a3edd89340a",
            "value": "config.json: 100%"
          }
        },
        "784f9b0047d845f5a715b53bf8f1061f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f54117ff0fd4bb19bc72eacb805051c",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77c4a89eddef45a594e74e1406a81e50",
            "value": 665
          }
        },
        "40c642fe9b8440739e545f7553d657e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcbb6532da96487f88d0ba01ee846a80",
            "placeholder": "​",
            "style": "IPY_MODEL_d3b2367a78cd441c9b595e19857e8a1a",
            "value": " 665/665 [00:00&lt;00:00, 57.0kB/s]"
          }
        },
        "f25a2ab8c12245938fc0ae61198d0a33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3189440e500d48529fc8abeac3fecdcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf2f0d82e6d9493699547a3edd89340a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f54117ff0fd4bb19bc72eacb805051c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77c4a89eddef45a594e74e1406a81e50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fcbb6532da96487f88d0ba01ee846a80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3b2367a78cd441c9b595e19857e8a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61385a6a48434cbf8ec69952e7043e73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc7fc98a5a8f479b95f1da78f617ecac",
              "IPY_MODEL_35ddbb83e7d54779affd1286ddeab5e8",
              "IPY_MODEL_506de6ba392c4503957cad9c3da4b1b2"
            ],
            "layout": "IPY_MODEL_db5ac87a8a2b42508cf1bdb07b8f255a"
          }
        },
        "cc7fc98a5a8f479b95f1da78f617ecac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09a6ccb9a88d49a39052249aec027d1d",
            "placeholder": "​",
            "style": "IPY_MODEL_86cf4cac2537494981b524d5d73490a1",
            "value": "vocab.json: 100%"
          }
        },
        "35ddbb83e7d54779affd1286ddeab5e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d4c5c4ef86048c59dd91f4e8880f04c",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b70b7239297a457fb3847641ec5bab00",
            "value": 1042301
          }
        },
        "506de6ba392c4503957cad9c3da4b1b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dcc248d91d54da389ad2e97cbf123b9",
            "placeholder": "​",
            "style": "IPY_MODEL_4d6655472c504244b488723fdc6699a9",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 1.06MB/s]"
          }
        },
        "db5ac87a8a2b42508cf1bdb07b8f255a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09a6ccb9a88d49a39052249aec027d1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86cf4cac2537494981b524d5d73490a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d4c5c4ef86048c59dd91f4e8880f04c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b70b7239297a457fb3847641ec5bab00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9dcc248d91d54da389ad2e97cbf123b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d6655472c504244b488723fdc6699a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc1d03d5bb6248bb95d3f379110e6b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5586f729d5da4a6da931ab79d88f89fa",
              "IPY_MODEL_6aaa4b7b680e4017ad891401c688acb4",
              "IPY_MODEL_67d805ee09944942a0e845c5aa1114bf"
            ],
            "layout": "IPY_MODEL_5a4c2ff6502a44748214808ddce79f87"
          }
        },
        "5586f729d5da4a6da931ab79d88f89fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ac920905a7e490fb04f0f7169591e76",
            "placeholder": "​",
            "style": "IPY_MODEL_ea33d0e9ad8149d5be15d44c6fc1e9f9",
            "value": "merges.txt: 100%"
          }
        },
        "6aaa4b7b680e4017ad891401c688acb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ca92db7b21f4829821ae343350bd6d2",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbafb94bde934eb3bcc74e45705b1961",
            "value": 456318
          }
        },
        "67d805ee09944942a0e845c5aa1114bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fe88c6359cd49b28b908b57e76a36d6",
            "placeholder": "​",
            "style": "IPY_MODEL_5a3b8aa87a95415aac6d2fb0af7984c1",
            "value": " 456k/456k [00:00&lt;00:00, 624kB/s]"
          }
        },
        "5a4c2ff6502a44748214808ddce79f87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ac920905a7e490fb04f0f7169591e76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea33d0e9ad8149d5be15d44c6fc1e9f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ca92db7b21f4829821ae343350bd6d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbafb94bde934eb3bcc74e45705b1961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9fe88c6359cd49b28b908b57e76a36d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a3b8aa87a95415aac6d2fb0af7984c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d029fe4a5e4641f48f7cb3000c64b224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df15377de5974278ac4c2e3b9b13aeac",
              "IPY_MODEL_dea38d38643140e0a9a7f76217ee1764",
              "IPY_MODEL_9f0388d393574c4d8f2b4bb4ae658112"
            ],
            "layout": "IPY_MODEL_2754487c94aa4a448a66566d8d2c266a"
          }
        },
        "df15377de5974278ac4c2e3b9b13aeac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_682a27acf11e4471a4b36c2294596739",
            "placeholder": "​",
            "style": "IPY_MODEL_f47e9931c13443e7a5d7f0bc06428f38",
            "value": "tokenizer.json: 100%"
          }
        },
        "dea38d38643140e0a9a7f76217ee1764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32305e25f36046fd864265ca946a8920",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ccdc78c142f4aeb9140309decbeb168",
            "value": 1355256
          }
        },
        "9f0388d393574c4d8f2b4bb4ae658112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a12b6e5dcc848cebaea566bbd728952",
            "placeholder": "​",
            "style": "IPY_MODEL_01b6ff01a61142d989e93b322e165199",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 5.30MB/s]"
          }
        },
        "2754487c94aa4a448a66566d8d2c266a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "682a27acf11e4471a4b36c2294596739": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f47e9931c13443e7a5d7f0bc06428f38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32305e25f36046fd864265ca946a8920": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ccdc78c142f4aeb9140309decbeb168": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a12b6e5dcc848cebaea566bbd728952": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01b6ff01a61142d989e93b322e165199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}