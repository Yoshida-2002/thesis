{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yoshida-2002/thesis/blob/main/CEG_omo_on_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 389,
      "metadata": {
        "id": "Lefh-Mu8KRcM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c7cb62f-0131-43a5-e5e2-a0c3cd23e1ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 390,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "aSs33HuiK1M6",
        "outputId": "7650b00c-c2fa-4c53-f710-0b3a5978fd01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning\n",
            "' CEG omo on colab'           'data for CEG'     models                \u001b[0m\u001b[01;34mplot_figures\u001b[0m/\n",
            "' CEG omo on colab のコピー'   \u001b[01;34mlightning_logs\u001b[0m/  'models のコピー'      \u001b[01;34mresult_log\u001b[0m/\n",
            " \u001b[01;34mdata\u001b[0m/                         \u001b[01;34mmodel\u001b[0m/            \u001b[01;34mmodel_transformers\u001b[0m/   \u001b[01;34msrc\u001b[0m/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 390
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning\n",
        "%ls\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {
        "id": "jRrJ4uFkLMCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdaad3b1-60db-4c05-890f-ed5866b34da0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch-lightning) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch-lightning) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pathを辞書にまとめて管理する"
      ],
      "metadata": {
        "id": "_fm0w0jc01Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_dict = {\n",
        "    # 生データのパス\n",
        "    'raw_train_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/train2.tsv',\n",
        "    'raw_test_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/test2.tsv',\n",
        "    'raw_val_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/val2.tsv',\n",
        "\n",
        "    # 欠損値処理を行った後のデータのパス\n",
        "    'missing_value_processed_train_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_train2.tsv',\n",
        "    'missing_value_processed_test_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_test2.tsv',\n",
        "    'missing_value_processed_val_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_val2.tsv',\n",
        "\n",
        "    # 欠損値処理を行った後、さらなる整形を行ったデータ\n",
        "    'input_for_lightning_model_train_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_train2.tsv',\n",
        "    'input_for_lightning_model_test_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_test2.tsv',\n",
        "    'input_for_lightning_model_val_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_val2.tsv',\n",
        "\n",
        "    # 集合に分割した後のデータ (main)\n",
        "    'main_input_for_lightning_model_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/train/true_set2.tsv',\n",
        "    'main_input_for_lightning_model_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/train/false_set2.tsv',\n",
        "    'main_input_for_lightning_model_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/test/true_set2.tsv',\n",
        "    'main_input_for_lightning_model_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/test/false_set2.tsv',\n",
        "    'main_input_for_lightning_model_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/val/true_set2.tsv',\n",
        "    'main_input_for_lightning_model_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/val/false_set2.tsv',\n",
        "\n",
        "    # 集合に分割した後のデータ (toy)\n",
        "    'toy_input_for_lightning_model_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/train/true_set2.tsv',\n",
        "    'toy_input_for_lightning_model_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/train/false_set2.tsv',\n",
        "    'toy_input_for_lightning_model_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/test/true_set2.tsv',\n",
        "    'toy_input_for_lightning_model_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/test/false_set2.tsv',\n",
        "    'toy_input_for_lightning_model_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/val/true_set2.tsv',\n",
        "    'toy_input_for_lightning_model_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/val/false_set2.tsv',\n",
        "\n",
        "    # モデルを保存するPath\n",
        "    'saved_model_path' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/src/models',\n",
        "    'TensorBoardLogger': 'lightning_logs',\n",
        "  }\n"
      ],
      "metadata": {
        "id": "VrElytlN06OI"
      },
      "execution_count": 392,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-H8IgiRLeom"
      },
      "source": [
        "# data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2W3EWwDNM-t"
      },
      "source": [
        "## data_reader.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "metadata": {
        "id": "xDRiQFePNLVS"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"data_reader module is written for read files\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def read_csv(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "\n",
        "    :param path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "def read_tsv(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "\n",
        "    :param path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return pd.read_csv(path, sep=\"\\t\", header=None)\n",
        "\n",
        "\n",
        "def read_npy(path: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    load a list of numpy elements into memory\n",
        "    :param path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return np.load(path, allow_pickle=True)\n",
        "\n",
        "\n",
        "def read_excel(path:str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    :param path: where to load data from\n",
        "    :return: pd.DataFrame\n",
        "    \"\"\"\n",
        "    return pd.read_excel(path, engine=\"openpyxl\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sdEJztwNSgt"
      },
      "source": [
        "## data_writer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 394,
      "metadata": {
        "id": "od80SyiCNVaE"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"data_writer module is written for write data in files\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def write_npy(path: str, data: list) -> None:\n",
        "    \"\"\"\n",
        "    save a list of numpy elements into disk\n",
        "    :param path:\n",
        "    :param data:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return np.save(path, data, allow_pickle=True)\n",
        "\n",
        "def write_dataframe_in_tsv(data: pd.DataFrame, path: str) -> None:\n",
        "\n",
        "  \"\"\"\n",
        "  save pd.Dataframe in tsv file\n",
        "  :param path:\n",
        "  :param data:\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  data.to_csv(path, sep='\\t', index=False, header=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lROVJGpnfD8U"
      },
      "source": [
        "## data_missing_value_processor\n",
        "- 欠損値の処理を行う\n",
        "- やること\n",
        "  1. 全ての値がNaNの列を削除する\n",
        "  2. NaNを0で補完する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 395,
      "metadata": {
        "id": "mFLX1BVOfi1v"
      },
      "outputs": [],
      "source": [
        "def data_missing_value_processor(data: pd.DataFrame, path: str) -> pd.DataFrame:\n",
        "\n",
        "  print('input data size : ' + str(len(data)))\n",
        "\n",
        "  data_without_invalid_row = data.dropna(how=\"all\")\n",
        "  print(str(len(data) - len(data_without_invalid_row)) + \" data was dropped because all of the values were NaN. \")\n",
        "\n",
        "  processed_data = data_without_invalid_row.fillna(0)\n",
        "  print(\"NaN in dataframe was filled with 0. \")\n",
        "  print(\"processed input data size : \" + str(len(processed_data)))\n",
        "  print(\"removed data size : \" + str(len(data) - len(processed_data)) )\n",
        "  print('\\n')\n",
        "\n",
        "  write_dataframe_in_tsv(processed_data, path)\n",
        "\n",
        "  return processed_data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phuOrNG7eh6i"
      },
      "source": [
        "##data_pre_processor_for_LIAR_PLUS_Dataset\n",
        "- LIAR_PLUS_Datasetクラスの入力に用いるdataframeを作成する\n",
        "- 作成したdataframeを保存する\n",
        "- statement, metadata, justification, credit_scoreの四つの列からなるdataframeを出力する\n",
        "- やること\n",
        "  1. 値として0(元々は欠損値)を持つ場合、\"None\"で補完する\n",
        "  2. 列を結合してstaetment, metadata, justification, credit_scoreを作成する\n",
        "  3. credit_scoreのnanを0.5で補完する\n",
        "  4. 作成したdataframeを保存する\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 396,
      "metadata": {
        "id": "Y5D6XvWcefqX"
      },
      "outputs": [],
      "source": [
        "def data_pre_processor_for_LIAR_PLUS_Dataset(data: pd.DataFrame, path: str) -> pd.DataFrame:\n",
        "\n",
        "  # 1. 値として0(元々は欠損値)を持つ場合、\"None\"で補完する\n",
        "\n",
        "  data[4].replace(0, 'None', inplace=True)\n",
        "  data[5].replace(0, 'None', inplace=True)\n",
        "  data[6].replace(0, 'None', inplace=True)\n",
        "  data[7].replace(0, 'None', inplace=True)\n",
        "  data[8].replace(0, 'None', inplace=True)\n",
        "  data[14].replace(0, 'None', inplace=True)\n",
        "  data[15].replace(0, 'None', inplace=True)\n",
        "\n",
        "  # 2. 列を結合してstaetment, metadata, justificationを作成する\n",
        "\n",
        "  data_for_LIAR_PLUS_Dataset = pd.DataFrame(\n",
        "      data = {\n",
        "      'id' : data[0],\n",
        "      'label' : data[2],\n",
        "      'statement' : data[3],\n",
        "      'metadata' : data[4]+ ' ' + data[5] + ' ' + data[6] + ' ' + data[7] + ' ' + data[8] + ' ' + data[14],\n",
        "      'justification' : data[15],\n",
        "      'credit_score' : (data[12]*0.2 + data[11]*0.5 + data[9]*0.75 + data[10]*0.9 + data[13]*1)/(data[12] + data[11] + data[9] + data[10] + data[13]),\n",
        "      'unique_id' : data[1]\n",
        "      })\n",
        "\n",
        "  data_for_LIAR_PLUS_Dataset['credit_score'] = data_for_LIAR_PLUS_Dataset['credit_score'].fillna(0.5)\n",
        "\n",
        "  write_dataframe_in_tsv(data_for_LIAR_PLUS_Dataset, path)\n",
        "\n",
        "  print(\"dataframe was saved in\" + path + \"\\n\")\n",
        "\n",
        "  data_for_LIAR_PLUS_Dataset.info()\n",
        "\n",
        "\n",
        "  return data_for_LIAR_PLUS_Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeBeWogIddoI"
      },
      "source": [
        "# dataset_separator_on_labels\n",
        "- プロンプトを作成する前にラベルで条件付けられた集合を作成\n",
        "- 名前以外はgenerated_prompt_separatorと同じ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 397,
      "metadata": {
        "id": "mHRtX4hxdtCE"
      },
      "outputs": [],
      "source": [
        "def dataset_separator_on_labels(path: dict, data: pd.DataFrame) -> dict:\n",
        "\n",
        "  true_set = pd.DataFrame()\n",
        "  false_set = pd.DataFrame()\n",
        "  total_len = len(data)\n",
        "\n",
        "  for i in range(len(data)):\n",
        "\n",
        "    data_row = data.iloc[i]\n",
        "   # print(data_row)\n",
        "\n",
        "    if data_row[1] == \"true\":\n",
        "        true_set = true_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"half-true\":\n",
        "        true_set = true_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"mostly-true\":\n",
        "        true_set = true_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"false\":\n",
        "        false_set = false_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"barely-true\":\n",
        "        false_set = false_set.append(data_row, ignore_index=True)\n",
        "    elif data_row[1] == \"pants-fire\":\n",
        "        false_set = false_set.append(data_row, ignore_index=True)\n",
        "    else:\n",
        "        print(f\"Invalid label was found! {data_row[1]}\")\n",
        "\n",
        "  print(\"===================================================\\n\")\n",
        "  print(f\"total len : {total_len}\")\n",
        "  print(f\"true_set : {len(true_set)}\")\n",
        "  print(f\"false_set : {len(false_set)}\")\n",
        "  print(\"===================================================\\n\")\n",
        "\n",
        "  write_dataframe_in_tsv(true_set, path[\"true_set\"])\n",
        "  write_dataframe_in_tsv(false_set, path[\"false_set\"])\n",
        "\n",
        "\n",
        "  return dict(\n",
        "      true_set=true_set,\n",
        "      false_set=true_set\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbE1iUnbvV1w"
      },
      "source": [
        "# prompt_generator\n",
        "・プロンプトの作成を一任\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 398,
      "metadata": {
        "id": "yghLEX3Uvc53"
      },
      "outputs": [],
      "source": [
        "# def prompt_generator(data: pd.DataFrame, path :str) -> pd.DataFrame :\n",
        "#   \"\"\"\n",
        "#   this function is for generating and saving prompts\n",
        "\n",
        "#   param  : stage(\"fine-tuning\" of \"completion\")\n",
        "#   param  : data(row data for prompt generation)\n",
        "#   param  : path(path to save generated promps dataframe)\n",
        "#   return : prompts(generated prompts)\n",
        "#   \"\"\"\n",
        "#   generated_prompts = \"Statement: \" + data[2] + \" Metadata: \" + data[3]\n",
        "\n",
        "#   prompts_df = pd.DataFrame(\n",
        "#       data = {\n",
        "#       'id' : data[0],\n",
        "#       'label' : data[1],\n",
        "#       'concat' : generated_prompts,\n",
        "#       'justification': data[4],\n",
        "#       'credit_score' : data[5],\n",
        "#       'unique_id' : data[6]\n",
        "#       })\n",
        "\n",
        "#   write_dataframe_in_tsv(prompts_df, path)\n",
        "\n",
        "#   return prompts_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 399,
      "metadata": {
        "id": "XMKn8TNcYi1g"
      },
      "outputs": [],
      "source": [
        "# def prompt_generator(data: pd.DataFrame, path:str, config_dict:dict) -> pd.DataFrame :\n",
        "#   \"\"\"\n",
        "#   this function is for generating and saving prompts\n",
        "\n",
        "#   param  : stage(\"fine-tuning\" of \"completion\")\n",
        "#   param  : data(row data for prompt generation)\n",
        "#   param  : path(path to save generated promps dataframe)\n",
        "#   return : prompts(generated prompts)\n",
        "#   \"\"\"\n",
        "\n",
        "\n",
        "#   generated_prompts = ''\n",
        "\n",
        "#   if 'statement' in config_dict['PROMPT_COMPONENTS']:\n",
        "#     generated_prompts = 'Statement: ' + data[2]\n",
        "\n",
        "#   if 'metadata' in config_dict['PROMPT_COMPONENTS']:\n",
        "#     generated_prompts += \" Metadata: \" + data[3]\n",
        "\n",
        "#   prompts_df = pd.DataFrame(\n",
        "#       data = {\n",
        "#       'id' : data[0],\n",
        "#       'label' : data[1],\n",
        "#       'concat' : generated_prompts,\n",
        "#       'justification': data[4],\n",
        "#       'credit_score' : data[5],\n",
        "#       'unique_id' : data[6]\n",
        "#       })\n",
        "\n",
        "#   return prompts_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UubLg77lkNfo"
      },
      "source": [
        "# concatinated_inputs_generator\n",
        "- concatとfull-promptを作成し諸々の情報を返す\n",
        "- 例外処理のコードはまだ書いていない"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 400,
      "metadata": {
        "id": "WVwpH7jzkvku"
      },
      "outputs": [],
      "source": [
        "def concatinated_inputs_generator(data: pd.Series, config_dict:dict, tokenizer, prepare_tokenizer) -> dict :\n",
        "\n",
        "  concatinated_input = ''\n",
        "\n",
        "  if 'statement' in config_dict['PROMPT_COMPONENTS']:\n",
        "    concatinated_input = 'Statement: ' + data[2]\n",
        "\n",
        "  if 'metadata' in config_dict['PROMPT_COMPONENTS']:\n",
        "    concatinated_input += ' Metadata: ' + data[3]\n",
        "\n",
        "  concatinated_input += prepare_tokenizer.exp_token\n",
        "  concatinated_input_length = len(tokenizer.tokenize(concatinated_input))\n",
        "\n",
        "  if 'justification' in config_dict['PROMPT_COMPONENTS']:\n",
        "    full_input = concatinated_input + data[4] + prepare_tokenizer.eos_token\n",
        "\n",
        "  full_input_length = len(tokenizer.tokenize(full_input))\n",
        "\n",
        "  # bartではspecial_token<s>が文頭に付くため、その分だけ入力長が1大きくなる\n",
        "  if config_dict[\"MODEL_NAME\"] == 'facebook/bart-base':\n",
        "      concatinated_input_length = concatinated_input_length + 1\n",
        "\n",
        "  if concatinated_input_length > config_dict['MAX_LENGTH']:\n",
        "      concatinated_input_length = config_dict['MAX_LENGTH']\n",
        "\n",
        "  if full_input_length > config_dict['MAX_LENGTH']:\n",
        "      full_input_length = config_dict['MAX_LENGTH']\n",
        "\n",
        "\n",
        "  return concatinated_input, full_input, concatinated_input_length, full_input_length\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 401,
      "metadata": {
        "id": "nlnS8xxD3yZv"
      },
      "outputs": [],
      "source": [
        "# train_prompt_path = \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts/main/train_prompts.tsv\"\n",
        "# test_prompt_path = \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts/main/test_prompts.tsv\"\n",
        "# val_prompt_path = \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts/main/val_prompts.tsv\"\n",
        "\n",
        "# train_prompts = read_tsv(\"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/main/input_train2.tsv\")\n",
        "# test_prompts = read_tsv(\"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/main/input_test2.tsv\")\n",
        "# val_prompts = read_tsv(\"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/main/input_val2.tsv\")\n",
        "\n",
        "# train_prompts = prompt_generator(train_prompts, train_prompt_path)\n",
        "# test_prompts = prompt_generator(test_prompts, test_prompt_path)\n",
        "# val_prompts = prompt_generator(val_prompts, val_prompt_path)\n",
        "\n",
        "# print(len(train_prompts))\n",
        "# train_prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 402,
      "metadata": {
        "id": "pQTF6thEEeYI"
      },
      "outputs": [],
      "source": [
        "# x = read_tsv(train_prompt_path)\n",
        "# x.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMUYg-qvy_Yw"
      },
      "source": [
        "# generated_prompts_separator\n",
        "- ラベルごとに分類する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 403,
      "metadata": {
        "id": "Rz-tzHX9B1yv"
      },
      "outputs": [],
      "source": [
        "# def generated_prompts_separator(path: dict, data: pd.DataFrame) -> dict:\n",
        "\n",
        "#   true_set = pd.DataFrame()\n",
        "#   false_set = pd.DataFrame()\n",
        "#   total_len = len(data)\n",
        "\n",
        "#   for i in range(len(data)):\n",
        "\n",
        "#     data_row = data.iloc[i]\n",
        "#    # print(data_row)\n",
        "\n",
        "#     if data_row[1] == \"true\":\n",
        "#         true_set = true_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"half-true\":\n",
        "#         true_set = true_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"mostly-true\":\n",
        "#         true_set = true_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"false\":\n",
        "#         false_set = false_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"barely-true\":\n",
        "#         false_set = false_set.append(data_row, ignore_index=True)\n",
        "#     elif data_row[1] == \"pants-fire\":\n",
        "#         false_set = false_set.append(data_row, ignore_index=True)\n",
        "#     else:\n",
        "#         print(f\"Invalid label was found! {data_row[1]}\")\n",
        "\n",
        "#   print(\"===================================================\\n\")\n",
        "#   print(f\"total len : {total_len}\")\n",
        "#   print(f\"true_set : {len(true_set)}\")\n",
        "#   print(f\"false_set : {len(false_set)}\")\n",
        "#   print(\"===================================================\\n\")\n",
        "\n",
        "#   write_dataframe_in_tsv(true_set, path[\"true_set\"])\n",
        "#   write_dataframe_in_tsv(false_set, path[\"false_set\"])\n",
        "\n",
        "\n",
        "#   return dict(\n",
        "#       true_set=true_set,\n",
        "#       false_set=true_set\n",
        "#   )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 404,
      "metadata": {
        "id": "RGPWmYuKW7AT"
      },
      "outputs": [],
      "source": [
        "# fine_main_train_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/train/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/train/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_main_test_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/test/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/test/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_main_val_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/val/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/val/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_toy_train_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/train/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/train/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_toy_test_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/test/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/test/false_sets.tsv\"\n",
        "# }\n",
        "\n",
        "# fine_toy_val_path_dict = {\n",
        "#     \"true_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/val/true_sets.tsv\",\n",
        "#     \"false_set\" : \"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/toy/val/false_sets.tsv\"\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 405,
      "metadata": {
        "id": "OGnq4oOKMIN-"
      },
      "outputs": [],
      "source": [
        "# main_train_df = read_tsv(train_prompt_path)\n",
        "# main_test_df = read_tsv(test_prompt_path)\n",
        "# main_val_df = read_tsv(val_prompt_path)\n",
        "\n",
        "# toy_train_df = main_train_df[:600]\n",
        "# toy_test_df = main_test_df[:200]\n",
        "# toy_val_df = main_val_df[:200]\n",
        "\n",
        "\n",
        "# main_train_set = generated_prompts_separator(fine_main_train_path_dict, main_train_df)\n",
        "# main_test_set = generated_prompts_separator(fine_main_test_path_dict, main_test_df)\n",
        "# main_val_set = generated_prompts_separator(fine_main_val_path_dict, main_val_df)\n",
        "\n",
        "# toy_train_set = generated_prompts_separator(fine_toy_train_path_dict, toy_train_df)\n",
        "# toy_test_set = generated_prompts_separator(fine_toy_test_path_dict, toy_test_df)\n",
        "# toy_val_set = generated_prompts_separator(fine_toy_val_path_dict, toy_val_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 406,
      "metadata": {
        "id": "-pyPfFDRkUFP"
      },
      "outputs": [],
      "source": [
        "# y = read_tsv(\"/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/prompts_set/main/train/false_sets.tsv\")\n",
        "# y.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# データ前処理の全行程"
      ],
      "metadata": {
        "id": "SxZNpMBC4_xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path_dict = {\n",
        "#     # 生データのパス\n",
        "#     'raw_train_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/train2.tsv',\n",
        "#     'raw_test_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/test2.tsv',\n",
        "#     'raw_val_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/val2.tsv',\n",
        "\n",
        "#     # 欠損値処理を行った後のデータのパス\n",
        "#     'missing_value_processed_train_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_train2.tsv',\n",
        "#     'missing_value_processed_test_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_test2.tsv',\n",
        "#     'missing_value_processed_val_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_val2.tsv',\n",
        "\n",
        "#     # 欠損値処理を行った後、さらなる整形を行ったデータ\n",
        "#     'input_for_lightning_model_train_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_train2.tsv',\n",
        "#     'input_for_lightning_model_test_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_test2.tsv',\n",
        "#     'input_for_lightning_model_val_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_val2.tsv',\n",
        "\n",
        "#     # 集合に分割した後のデータ (main)\n",
        "#     'main_input_for_lightning_model_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/train/true_set2.tsv',\n",
        "#     'main_input_for_lightning_model_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/train/false_set2.tsv',\n",
        "#     'main_input_for_lightning_model_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/test/true_set2.tsv',\n",
        "#     'main_input_for_lightning_model_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/test/false_set2.tsv',\n",
        "#     'main_input_for_lightning_model_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/val/true_set2.tsv',\n",
        "#     'main_input_for_lightning_model_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/val/false_set2.tsv',\n",
        "\n",
        "#     # 集合に分割した後のデータ (toy)\n",
        "#     'toy_input_for_lightning_model_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/train/true_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/train/false_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/test/true_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/test/false_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/val/true_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/val/false_set2.tsv',\n",
        "\n",
        "#   }\n"
      ],
      "metadata": {
        "id": "1xrPuNRyL4Sv"
      },
      "execution_count": 407,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 生データの読み込み\n",
        "# raw_train_data = read_tsv(path_dict['raw_train_data'])\n",
        "# raw_test_data = read_tsv(path_dict['raw_test_data'])\n",
        "# raw_val_data = read_tsv(path_dict['raw_val_data'])\n",
        "\n",
        "# # 基本的な前処理\n",
        "# processed_train_data = data_missing_value_processor(\n",
        "#     raw_train_data,\n",
        "#     path_dict['missing_value_processed_train_data']\n",
        "# )\n",
        "# processed_test_data = data_missing_value_processor(\n",
        "#     raw_test_data,\n",
        "#     path_dict['missing_value_processed_test_data']\n",
        "# )\n",
        "# processed_val_data = data_missing_value_processor(\n",
        "#     raw_val_data,\n",
        "#     path_dict[\"missing_value_processed_val_data\"]\n",
        "# )\n",
        "\n",
        "# # タスクに向けた前処理と保存\n",
        "# train_data_for_LIAR_PLUS_Dataset = data_pre_processor_for_LIAR_PLUS_Dataset(\n",
        "#     processed_train_data,\n",
        "#     path_dict['input_for_lightning_model_train_base']\n",
        "#     )\n",
        "# test_data_for_LIAR_PLUS_Dataset = data_pre_processor_for_LIAR_PLUS_Dataset(\n",
        "#     processed_test_data,\n",
        "#     path_dict['input_for_lightning_model_test_base']\n",
        "#     )\n",
        "# val_data_for_LIAR_PLUS_Dataset = data_pre_processor_for_LIAR_PLUS_Dataset(\n",
        "#     processed_val_data,\n",
        "#     path_dict['input_for_lightning_model_val_base']\n",
        "#     )\n",
        "\n",
        "# # 集合に分割し保存 (main)\n",
        "# main_train_true_set, main_train_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['main_input_for_lightning_model_train_data_true'],\n",
        "#      'false_set': path_dict['main_input_for_lightning_model_train_data_false']},\n",
        "#     train_data_for_LIAR_PLUS_Dataset\n",
        "# )\n",
        "\n",
        "# main_test_true_set, main_test_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['main_input_for_lightning_model_test_data_true'],\n",
        "#      'false_set': path_dict['main_input_for_lightning_model_test_data_false']},\n",
        "#     test_data_for_LIAR_PLUS_Dataset\n",
        "# )\n",
        "\n",
        "# main_val_true_set, main_val_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['main_input_for_lightning_model_val_data_true'],\n",
        "#      'false_set': path_dict['main_input_for_lightning_model_val_data_false']},\n",
        "#     val_data_for_LIAR_PLUS_Dataset\n",
        "# )\n",
        "\n",
        "\n",
        "# # 集合に分割し保存 (toy)\n",
        "# toy_train_true_set, toy_train_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "#      'false_set': path_dict['toy_input_for_lightning_model_train_data_false']},\n",
        "#     train_data_for_LIAR_PLUS_Dataset[:120]\n",
        "# )\n",
        "\n",
        "# toy_test_true_set, toy_test_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "#      'false_set': path_dict['toy_input_for_lightning_model_test_data_false']},\n",
        "#     test_data_for_LIAR_PLUS_Dataset[:40]\n",
        "# )\n",
        "\n",
        "# toy_val_true_set, toy_val_false_set = dataset_separator_on_labels(\n",
        "#     {'true_set' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "#      'false_set': path_dict['toy_input_for_lightning_model_val_data_false']},\n",
        "#     val_data_for_LIAR_PLUS_Dataset[:40]\n",
        "# )"
      ],
      "metadata": {
        "id": "SBQLyeNu4_Lw"
      },
      "execution_count": 408,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ_ZwRAby_hP"
      },
      "source": [
        "# Google Driveへ接続しディレクトリを移動"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 409,
      "metadata": {
        "id": "L31wefW7zMo9"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 410,
      "metadata": {
        "id": "vpFyWy0BzU0v"
      },
      "outputs": [],
      "source": [
        "# %cd /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning\n",
        "# %ls\n",
        "# %pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB5rz2m-y_uq"
      },
      "source": [
        "# TransformersとPytorch Lightningをインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 411,
      "metadata": {
        "id": "izeLGyCTznqQ"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 412,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X4Le2kXY8K3",
        "outputId": "c7e9c0da-663b-415a-afa9-045564fd8992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.35.2\n",
            "2.1.3\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "import pytorch_lightning as pl\n",
        "import pandas as pd\n",
        "print(transformers.__version__)\n",
        "print(pl.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciFDDUpS572n"
      },
      "source": [
        "# class LIAR_PLUS_Dataset_For_CEG(Dataset):を定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 413,
      "metadata": {
        "id": "Zf6kfB036jme"
      },
      "outputs": [],
      "source": [
        "# # -*- coding: utf-8 -*-\n",
        "# # pylint: disable-msg=import-error\n",
        "# # pylint: disable-msg=no-member\n",
        "# # ========================================================\n",
        "# \"\"\"dataset module is written for create data module\"\"\"\n",
        "# # ========================================================\n",
        "\n",
        "\n",
        "# # ========================================================\n",
        "# # Imports\n",
        "# # ========================================================\n",
        "# import pandas as pd\n",
        "# import pytorch_lightning as pl\n",
        "# import torch\n",
        "# import time\n",
        "\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# from typing import Optional\n",
        "\n",
        "# class LIAR_PLUS_Dataset_For_CEG(Dataset):\n",
        "#     \"\"\"\n",
        "#     this class is for encoding input dataframe for GPT2\n",
        "\n",
        "#     Input dataframe needs to be consist of 5 columns, \"id\", \"label\", \"prompt\", \"justification\" and \"credit_score\"\n",
        "\n",
        "#     later we wrap a lightning data module around it.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, data: pd.DataFrame, config_dict: dict):\n",
        "#       # max_token_length should be flexible depending on input sequence so it needs to be changed later.\n",
        "#         self.data = data\n",
        "#         self.config_dict = config_dict\n",
        "#         self.prepare_tokenizer = Prepare_Tokenizer(self.config_dict)\n",
        "#         self.tokenizer = self.prepare_tokenizer.get_tokenizer()\n",
        "#         self.prepare_tokenizer.get_tokenizer_info()\n",
        "#         self.max_token_len = config_dict['MAX_LENGTH']\n",
        "#         # print(\"Before\")\n",
        "#         # print(self.tokenizer.all_special_tokens)\n",
        "#         # print(self.tokenizer.all_special_ids)\n",
        "#         # special_tokens_dict = {\n",
        "#         # 'additional_special_tokens': ['[EXP]'],\n",
        "#         # \"eos_token\" : \"<|endoftext|>\",\n",
        "#         # \"pad_token\" : \"<|endoftext|>\"\n",
        "#         # }\n",
        "#         # self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "#         # self.exp_id = tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "#         # self.eos_id = tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "#         # self.pad_id = tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "#         # print(\"After\")\n",
        "#         # print(tokenizer.all_special_tokens)\n",
        "#         # print(tokenizer.all_special_ids)\n",
        "#         # print(f\"exp_id : {self.exp_id}\")\n",
        "#         # print(f\"eos_id : {self.eos_id}\")\n",
        "#         # print(f\"pad_id : {self.pad_id}\\n\")\n",
        "#         # print(f\"len(tokenizer) : {len(self.tokenizer)}\")\n",
        "\n",
        "#          # it could be 512\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, index: int):\n",
        "\n",
        "#         data_row = self.data.iloc[index]\n",
        "\n",
        "#         concat_text = data_row[2] + \"[EXP]\"\n",
        "#         justification_text = data_row[3]\n",
        "#         full_prompt_text = concat_text + justification_text + \"[EOS]\"\n",
        "\n",
        "#         concat_text_length = len(self.tokenizer.tokenize(concat_text))\n",
        "#         # bartではspecial_token<s>が文頭に付くため、その分だけ入力長が1大きくなる\n",
        "#         if self.config_dict[\"MODEL_NAME\"] == 'facebook/bart-base':\n",
        "#             concat_text_length += concat_text_length\n",
        "\n",
        "#         if concat_text_length > self.max_token_len:\n",
        "#             concat_text_length = self.max_token_len\n",
        "#         justification_text_length = len(self.tokenizer.tokenize(justification_text))\n",
        "#         if justification_text_length > self.max_token_len:\n",
        "#             justification_text_length = self.max_token_len\n",
        "#         full_prompt_text_length = len(self.tokenizer.tokenize(full_prompt_text))\n",
        "#         if full_prompt_text_length > self.max_token_len:\n",
        "#             full_prompt_text_length = self.max_token_len\n",
        "\n",
        "#         if data_row[1] == \"true\":\n",
        "#             label = torch.tensor([1,0], dtype=torch.float32)\n",
        "#         elif data_row[1] == \"half-true\":\n",
        "#             label = torch.tensor([1,0], dtype=torch.float32)\n",
        "#         elif data_row[1] == \"mostly-true\":\n",
        "#             label = torch.tensor([1,0], dtype=torch.float32)\n",
        "#         elif data_row[1] == \"false\":\n",
        "#             label = torch.tensor([0,1], dtype=torch.float32)\n",
        "#         elif data_row[1] == \"barely-true\":\n",
        "#             label = torch.tensor([0,1], dtype=torch.float32)\n",
        "#         elif data_row[1] == \"pants-fire\":\n",
        "#             label = torch.tensor([0,1], dtype=torch.float32)\n",
        "#         else:\n",
        "#             print(f\"Invalid label was found! {data_row[1]}\")\n",
        "#             time.sleep(30)\n",
        "\n",
        "#         concat_encoding = self.tokenizer.encode_plus(\n",
        "#             concat_text,\n",
        "#             add_special_tokens=True,\n",
        "#             max_length = self.max_token_len,\n",
        "#             return_token_type_ids=False,\n",
        "#             padding=\"max_length\",\n",
        "#             truncation=True,\n",
        "#             return_attention_mask=True,\n",
        "#             return_tensors='pt',\n",
        "#         )\n",
        "\n",
        "#         full_prompt_encoding = self.tokenizer.encode_plus(\n",
        "#             full_prompt_text,\n",
        "#             add_special_tokens=True,\n",
        "#             max_length = self.max_token_len,\n",
        "#             return_token_type_ids=False,\n",
        "#             padding=\"max_length\",\n",
        "#             truncation=True,\n",
        "#             return_attention_mask=True,\n",
        "#             return_tensors='pt',\n",
        "#         )\n",
        "\n",
        "#         # labels = full_prompt_encoding[\"input_ids\"].flatten()\n",
        "#         # labels[:concat_text_length] = -100 # cross_entropy_ignore_index\n",
        "\n",
        "#         # print(\"=================================================================\\n\")\n",
        "#         # print(f\"concat_text : {concat_text}\\n\")\n",
        "#         # print(f\"concat_text_length : {concat_text_length}\\n\")\n",
        "#         # print(f\"justification_text : {justification_text}\\n\")\n",
        "#         # print(f\"justification_text_length : {justification_text_length}\\n\")\n",
        "#         # print(f\"full_prompt_text : {full_prompt_text}\\n\")\n",
        "#         # print(f\"full_prompt_text_length : {full_prompt_text_length}\\n\")\n",
        "#         # print(f\"labels before padding : {labels}\\n\")\n",
        "#         # print(f\"labels after padding: {labels}\\n\")\n",
        "#         # print(\"=================================================================\\n\")\n",
        "\n",
        "#         return dict(\n",
        "#             input_ids1=concat_encoding[\"input_ids\"].flatten(),\n",
        "#             attention_mask1=concat_encoding[\"attention_mask\"].flatten(),\n",
        "#             input_ids2=full_prompt_encoding[\"input_ids\"].flatten(),\n",
        "#             attention_mask2=full_prompt_encoding[\"attention_mask\"].flatten(),\n",
        "#             concat_text_length=concat_text_length,\n",
        "#             full_prompt_text_length=full_prompt_text_length,\n",
        "#             # labels = labels\n",
        "#         )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 414,
      "metadata": {
        "id": "NnU6wf8TntZG"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# pylint: disable-msg=import-error\n",
        "# pylint: disable-msg=no-member\n",
        "# ========================================================\n",
        "\"\"\"dataset module is written for create data module\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import time\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "class LIAR_PLUS_Dataset_For_CEG(Dataset):\n",
        "    \"\"\"\n",
        "    this class is for encoding input dataframe for GPT2\n",
        "\n",
        "    Input dataframe needs to be consist of 5 columns, \"id\", \"label\", \"prompt\", \"justification\" and \"credit_score\"\n",
        "\n",
        "    later we wrap a lightning data module around it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data: pd.DataFrame, config_dict: dict):\n",
        "      # max_token_length should be flexible depending on input sequence so it needs to be changed later.\n",
        "        self.data = data\n",
        "        self.config_dict = config_dict\n",
        "        self.prepare_tokenizer = Prepare_Tokenizer(self.config_dict)\n",
        "        self.tokenizer = self.prepare_tokenizer.get_tokenizer()\n",
        "        self.prepare_tokenizer.get_tokenizer_info()\n",
        "        self.max_token_len = config_dict['MAX_LENGTH']\n",
        "        # print(\"Before\")\n",
        "        # print(self.tokenizer.all_special_tokens)\n",
        "        # print(self.tokenizer.all_special_ids)\n",
        "        # special_tokens_dict = {\n",
        "        # 'additional_special_tokens': ['[EXP]'],\n",
        "        # \"eos_token\" : \"<|endoftext|>\",\n",
        "        # \"pad_token\" : \"<|endoftext|>\"\n",
        "        # }\n",
        "        # self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "        # self.exp_id = tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "        # self.eos_id = tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        # self.pad_id = tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        # print(\"After\")\n",
        "        # print(tokenizer.all_special_tokens)\n",
        "        # print(tokenizer.all_special_ids)\n",
        "        # print(f\"exp_id : {self.exp_id}\")\n",
        "        # print(f\"eos_id : {self.eos_id}\")\n",
        "        # print(f\"pad_id : {self.pad_id}\\n\")\n",
        "        # print(f\"len(tokenizer) : {len(self.tokenizer)}\")\n",
        "\n",
        "         # it could be 512\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "\n",
        "        data_row = self.data.iloc[index]\n",
        "        # print('------------------------------------------------------------------\\n')\n",
        "        # print(f'data_row : {data_row}\\n')\n",
        "        # print('------------------------------------------------------------------\\n')\n",
        "        concat_text, full_prompt_text, concat_text_length, full_prompt_text_length = concatinated_inputs_generator(data_row, self.config_dict, self.tokenizer, self.prepare_tokenizer )\n",
        "\n",
        "        # print('------------------------------------------------------------------\\n')\n",
        "        # print(f'concat_text : {concat_text}\\n')\n",
        "        # print(f'full_prompt_text : {full_prompt_text}\\n')\n",
        "        # print(f'concat_text_length : {concat_text_length}\\n')\n",
        "        # print(f'full_prompt_text_length : {full_prompt_text_length}\\n')\n",
        "        # print('------------------------------------------------------------------\\n')\n",
        "        concat_encoding = self.tokenizer.encode_plus(\n",
        "            concat_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length = self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        full_prompt_encoding = self.tokenizer.encode_plus(\n",
        "            full_prompt_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length = self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        # labels = full_prompt_encoding[\"input_ids\"].flatten()\n",
        "        # labels[:concat_text_length] = -100 # cross_entropy_ignore_index\n",
        "\n",
        "        # print(\"=================================================================\\n\")\n",
        "        # print(f\"concat_text : {concat_text}\\n\")\n",
        "        # print(f\"concat_text_length : {concat_text_length}\\n\")\n",
        "        # print(f\"justification_text : {justification_text}\\n\")\n",
        "        # print(f\"justification_text_length : {justification_text_length}\\n\")\n",
        "        # print(f\"full_prompt_text : {full_prompt_text}\\n\")\n",
        "        # print(f\"full_prompt_text_length : {full_prompt_text_length}\\n\")\n",
        "        # print(f\"labels before padding : {labels}\\n\")\n",
        "        # print(f\"labels after padding: {labels}\\n\")\n",
        "        # print(\"=================================================================\\n\")\n",
        "\n",
        "        return dict(\n",
        "            input_ids1=concat_encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask1=concat_encoding[\"attention_mask\"].flatten(),\n",
        "            input_ids2=full_prompt_encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask2=full_prompt_encoding[\"attention_mask\"].flatten(),\n",
        "            concat_text_length=concat_text_length,\n",
        "            full_prompt_text_length=full_prompt_text_length,\n",
        "            # labels = labels\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf6XZBBC58F9"
      },
      "source": [
        "# class LIAR_PLUS_DataModule_For_CEG(pl.LightningDataModule)を定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 415,
      "metadata": {
        "id": "gFsOZ6LW6zLM"
      },
      "outputs": [],
      "source": [
        "class LIAR_PLUS_DataModule_For_CEG(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, config_dict, train_df: pd.DataFrame, test_df: pd.DataFrame, val_df: pd.DataFrame):\n",
        "        super().__init__()\n",
        "        self.config_dict = config_dict\n",
        "        self.train_df = train_df\n",
        "        self.test_df = test_df\n",
        "        self.val_df = val_df\n",
        "        self.training_batch_size = config_dict[\"TRAINING_BATCH_SIZE\"]\n",
        "        self.validation_batch_size = config_dict[\"VALIDATION_BATCH_SIZE\"]\n",
        "        self.test_batch_size = config_dict[\"TEST_BATCH_SIZE\"]\n",
        "        self.max_token_len = self.config_dict[\"MAX_LENGTH\"]\n",
        "        self.train_dataset, self.test_dataset, self.val_dataset = None, None, None\n",
        "\n",
        "        # 以下の処理はDataset内でやってくれるのかも\n",
        "        # print(\"Before\")\n",
        "        # print(self.tokenizer.all_special_tokens)\n",
        "        # print(self.tokenizer.all_special_ids)\n",
        "        # special_tokens_dict = {'additional_special_tokens': ['[EXP]'], \"eos_token\" : \"[EOS]\"}\n",
        "        # self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "        # self.exp_id = tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "        # self.eos_id = tokenizer.convert_tokens_to_ids('[EOS]')\n",
        "        # print(\"After\")\n",
        "        # print(tokenizer.all_special_tokens)\n",
        "        # print(tokenizer.all_special_ids)\n",
        "        # print(f\"exp_id : {self.exp_id}\")\n",
        "        # print(f\"eos_id : {self.eos_id}\\n\")\n",
        "\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None) -> None:\n",
        "        self.train_dataset = LIAR_PLUS_Dataset_For_CEG(self.train_df, self.config_dict)\n",
        "        print(f\"train_dataset size : {len(self.train_dataset)}\\n\")\n",
        "        self.test_dataset = LIAR_PLUS_Dataset_For_CEG(self.test_df, self.config_dict)\n",
        "        print(f\"test_dataset size : {len(self.test_dataset)}\\n\")\n",
        "        self.val_dataset = LIAR_PLUS_Dataset_For_CEG(self.val_df, self.config_dict)\n",
        "        print(f\"val_dataset size : {len(self.val_dataset)}\\n\")\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.training_batch_size,\n",
        "            shuffle=True,\n",
        "            drop_last=True,\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.validation_batch_size,\n",
        "            drop_last=True,\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            drop_last=True,\n",
        "            num_workers=2\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FKESVDD58Nr"
      },
      "source": [
        "# build_checkpoint_callbackを定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 416,
      "metadata": {
        "id": "zVqrSIBr7hYX"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"helper module is written for write useful function in indexer package\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "def build_checkpoint_callback(config_dict, base_time, batch_size, filename=\"QTag-{epoch:02d}-{val_loss:.2f}\",\n",
        "                              monitor=\"val_loss\"):\n",
        "    \"\"\"\n",
        "\n",
        "    :param save_top_k:\n",
        "    :param filename:\n",
        "    :param monitor:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # dirpath = config_dict['SAVED_MODEL_PATH'] + f'/{base_time}/' + config_dict['TensorBoardLogger_NAME'] + f'/batch_size={batch_size}'\n",
        "    dirpath = config_dict['SAVED_MODEL_PATH'] + '/' + config_dict['TensorBoardLogger_NAME'] + f'/{base_time}/batch_size={batch_size}'\n",
        "    # saves a file like: input/QTag-epoch=02-val_loss=0.32.ckpt\n",
        "    checkpoint_callback = ModelCheckpoint(monitor=monitor,  # monitored quantity\n",
        "                                          filename=filename,\n",
        "                                          save_top_k=config_dict['SAVE_TOP_K'],  # save the top k models\n",
        "                                          dirpath=dirpath,\n",
        "                                          mode=\"min\",  # mode of the monitored quantity for optimization\n",
        "                                          )\n",
        "    print(f\"checkpoint_callback : {checkpoint_callback}\")\n",
        "    return checkpoint_callback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mqQ2UHy58SM"
      },
      "source": [
        "# class CEG(pl.LightningModule)を定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 417,
      "metadata": {
        "id": "E9axfgge7tV3"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# pylint: disable=too-many-arguments\n",
        "# pylint: disable=import-error\n",
        "# ========================================================\n",
        "\"\"\"This module is written for write BERT classifier.\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "from typing import List\n",
        "import pytorch_lightning as pl\n",
        "from torch import nn\n",
        "import torch\n",
        "import torchmetrics\n",
        "from transformers import GPT2Tokenizer, AutoModel, AutoConfig, AdamW, get_linear_schedule_with_warmup, GPT2LMHeadModel, T5ForConditionalGeneration, BartForConditionalGeneration\n",
        "\n",
        "class CEG(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    creates a pytorch lightning model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config_dict,\n",
        "                 n_warmup_steps: int = None,\n",
        "                 n_training_steps: int = None,\n",
        "                 n_classes: int = None,\n",
        "                 result_manager = None):\n",
        "        super().__init__()\n",
        "        self.config_dict = config_dict\n",
        "        self.result_manager = result_manager\n",
        "        self.prepare_tokenizer = Prepare_Tokenizer(self.config_dict)\n",
        "        self.tokenizer = self.prepare_tokenizer.get_tokenizer()\n",
        "        self.prepare_tokenizer.get_tokenizer_info()\n",
        "        self.prepare_model = Prepare_Model(config=self.config_dict, prepare_tokenizer=self.prepare_tokenizer)\n",
        "        self.model = self.prepare_model.get_model()\n",
        "        self.prepare_model.get_model_info()\n",
        "        # self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "        # print(\"Before\")\n",
        "        # print(self.tokenizer.all_special_tokens)\n",
        "        # print(self.tokenizer.all_special_ids)\n",
        "        # special_tokens_dict = {\n",
        "        #     'additional_special_tokens': ['[EXP]'],\n",
        "        #     \"eos_token\" : \"<|endoftext|>\",\n",
        "        #     \"pad_token\" : \"<|endoftext|>\"\n",
        "        #     }\n",
        "        # self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "        # exp_id = self.tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "        # eos_id = self.tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        # pad_id = self.tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        # print(\"After\")\n",
        "        # print(self.tokenizer.all_special_tokens)\n",
        "        # print(self.tokenizer.all_special_ids)\n",
        "        # print(f\"exp_id : {exp_id}\")\n",
        "        # print(f\"eos_id : {eos_id}\")\n",
        "        # print(f\"pad_id : {pad_id}\\n\")\n",
        "        # print(f\"len(self.tokenizer) : {len(self.tokenizer)}\")\n",
        "\n",
        "        # gpt2config = AutoConfig.from_pretrained('gpt2', bos_token_id=self.tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id, additional_special_tokens_id = exp_id, output_hidden_states=False)\n",
        "\n",
        "        # self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=gpt2config)\n",
        "        # self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "        # self.lm_head = self.model.lm_head\n",
        "        # new_weights = torch.cat([self.lm_head.weight[:-1, :], torch.zeros(1, self.lm_head.weight.shape[1]) -10000])\n",
        "        # self.model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.batch_size = config_dict['TRAINING_BATCH_SIZE']\n",
        "        self.n_training_steps = n_training_steps\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.config_dict['CROSS_ENTROPY_IGNORE_INDEX'])\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, concat_text_length, full_prompt_text_length):\n",
        "\n",
        "        # print(\"===============================forward=================================\\n\")\n",
        "        labels = input_ids2\n",
        "        # print(f\"labels : {labels}\")\n",
        "        # print(f\"len(labels) : {len(labels)}\")\n",
        "        # print(full_prompt_text_length)\n",
        "        batch_max_length = max(full_prompt_text_length)\n",
        "        # print(f\"batch_max_length : {batch_max_length}\")\n",
        "\n",
        "\n",
        "        for i in range(len(labels)):\n",
        "            labels[i][:concat_text_length[i]] = -100 # cross_entropy_ignore_index\n",
        "            labels[i][full_prompt_text_length[i]:] = -100\n",
        "        # print(f\"input_ids1 before reshaping : {input_ids1.shape}\")\n",
        "        # print(f\"labels before reshaping : {labels.shape}\")\n",
        "        # print(f\"attention_mask1 before reshaping : {attention_mask1.shape}\")\n",
        "        labels = labels[:,:batch_max_length]\n",
        "        input_ids1 = input_ids1[:,:batch_max_length]\n",
        "        attention_mask1 = attention_mask1[:,:batch_max_length]\n",
        "        # print(f\"input_ids1 after reshaping : {input_ids1.shape}\")\n",
        "        # print(f\"labels after reshaping : {labels.shape}\")\n",
        "        # print(f\"attention_mask1 after reshaping : {attention_mask1.shape}\")\n",
        "            # print(f\"labels after -100 padding : {labels[0]}\")\n",
        "        # print(f\"len(labels[0]) before shifting : {len(labels[0])}\\n\")\n",
        "        # print(f\"input_ids1 : {input_ids1}\\n\")\n",
        "        # print(f\"attention_mask1 : {attention_mask1}\\n\")\n",
        "        # print(f\"input_ids2 : {input_ids2}\\n\")\n",
        "        # print(f\"concat_text_length : {concat_text_length}\\n\")\n",
        "        # print(f\"full_prompt_length : {full_prompt_text_length}\\n\")\n",
        "        # print(f\"labels : {labels}\\n\")\n",
        "        # print(f\"labels before shifting : {labels}, labels.size() : {labels.size()}\\n\")\n",
        "        # labels = labels[0][1:]\n",
        "        # labels = torch.Tensor(labels)\n",
        "        # print(f\"labels after shifting : {labels}, labels.size() : {labels.size()}\\n\")\n",
        "        # additional_pad = tokenizer.tokenize(\"[PAD]\")\n",
        "        # additional_pad = torch.Tensor(tokenizer.convert_tokens_to_ids(additional_pad))\n",
        "        # additional_pad = torch.Tensor(additional_pad)\n",
        "        # labels = torch.cat((labels, additional_pad))\n",
        "        # print(f\"labels after shifting : {labels}, labels.size() : {labels.size()}\\n\")\n",
        "\n",
        "        #outputs = self.model(input_ids=input_ids1, attention_mask=attention_mask1, labels=labels)\n",
        "\n",
        "        outputs = self.model(input_ids=input_ids1, attention_mask=attention_mask1, labels=labels)\n",
        "        logits = outputs.logits\n",
        "        loss = 0\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # print(f\"outputs : {outputs}\\n\")\n",
        "        # print(f\"outputs type : {type(outputs)}\")\n",
        "        # print(f\"input.shape : {outputs.shape}\")\n",
        "        # print(f\"logits : {logits}\")\n",
        "        # print(f\"logits type : {type(logits)}\")\n",
        "        # print(f\"logits shape : {logits.shape}\")\n",
        "        # print(f\"loss type : {type(loss)}\")\n",
        "        # print(f\"loss : {loss}\")\n",
        "        softmax_output = self.softmax(logits)\n",
        "        arg = torch.argmax(softmax_output, dim=2)\n",
        "        # print(f\"argmax(ouput) : {arg}\") # --> tensor\n",
        "        # print(f\"argmax(ouput).size() : {arg.size()}\")\n",
        "        # print(f\"argmax(output).dtype() : {arg.dtype}\")\n",
        "        # print(arg)\n",
        "        generated = self.tokenizer.decode(arg[0])\n",
        "        # print(f\"generated text: {generated}\\n\")\n",
        "        # print(f\"len(generated) : {len(self.tokenizer.tokenize(generated))}\\n\")\n",
        "        # print(f\"loss type : {type(loss)}\\n\")\n",
        "        # print(f\"loss shape : {loss.shape}\\n\")\n",
        "\n",
        "        return loss, outputs\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "\n",
        "        :param batch:\n",
        "        :param batch_idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # print(f\"training batch : {batch}\")\n",
        "        input_ids1 = batch[\"input_ids1\"]\n",
        "        attention_mask1 = batch[\"attention_mask1\"]\n",
        "        input_ids2 = batch[\"input_ids2\"]\n",
        "        attention_mask2 = batch[\"attention_mask2\"]\n",
        "        concat_text_length = batch[\"concat_text_length\"]\n",
        "        full_prompt_text_length = batch[\"full_prompt_text_length\"]\n",
        "        # labels = batch[\"labels\"]\n",
        "\n",
        "        loss, output = self(input_ids1, attention_mask1, input_ids2, attention_mask2, concat_text_length, full_prompt_text_length)\n",
        "\n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "        # print(f'type(np_loss) : {type(np_loss)}')\n",
        "        # print(f'np_loss : {np_loss}')\n",
        "\n",
        "        self.result_manager.trainLoss_batch.append(np_loss)\n",
        "        # print(\"===============================training=================================\\n\")\n",
        "        # print(f\"output : {outputs}\\n\")\n",
        "        # print(f\"output type : {type(outputs)}\\n\")\n",
        "        # print(f\"loss : {loss}\\n\")\n",
        "        # print(f\"loss type : {type(loss)}\\n\")\n",
        "        # print(f\"loss shape : {loss.shape}\\n\")\n",
        "\n",
        "        return {\"loss\": loss, \"predictions\": output}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "\n",
        "        :param batch:\n",
        "        :param batch_idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # print(f\"validation batch : {batch}\")\n",
        "        input_ids1 = batch[\"input_ids1\"]\n",
        "        attention_mask1 = batch[\"attention_mask1\"]\n",
        "        input_ids2 = batch[\"input_ids2\"]\n",
        "        attention_mask2 = batch[\"attention_mask2\"]\n",
        "        concat_text_length = batch[\"concat_text_length\"]\n",
        "        full_prompt_text_length = batch[\"full_prompt_text_length\"]\n",
        "        # labels = batch[\"labels\"]\n",
        "\n",
        "        loss, output = self(input_ids1, attention_mask1, input_ids2, attention_mask2, concat_text_length, full_prompt_text_length)\n",
        "\n",
        "        self.log(\"val_loss\", loss, prog_bar=True, logger=True, on_step=False)\n",
        "\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "        # print(f'type(np_loss) : {type(np_loss)}')\n",
        "        # print(f'np_loss : {np_loss}')\n",
        "        self.result_manager.valLoss_batch.append(np_loss)\n",
        "        # print(\"===============================validating=================================\\n\")\n",
        "        # print(f\"output : {outputs}\\n\")\n",
        "        # print(f\"output type : {type(outputs)}\\n\")\n",
        "        # print(f\"loss : {loss}\\n\")\n",
        "        # print(f\"loss type : {type(loss)}\\n\")\n",
        "        # print(f\"loss shape : {loss.shape}\\n\")\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "\n",
        "        :param batch:\n",
        "        :param batch_idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        # print(\"test batch : {batch}\")\n",
        "        input_ids1 = batch[\"input_ids1\"]\n",
        "        attention_mask1 = batch[\"attention_mask1\"]\n",
        "        input_ids2 = batch[\"input_ids2\"]\n",
        "        attention_mask2 = batch[\"attention_mask2\"]\n",
        "        concat_text_length = batch[\"concat_text_length\"]\n",
        "        full_prompt_text_length = batch[\"full_prompt_text_length\"]\n",
        "        # labels = batch[\"labels\"]\n",
        "\n",
        "        loss, output = self(input_ids1, attention_mask1, input_ids2, attention_mask2, concat_text_length, full_prompt_text_length)\n",
        "\n",
        "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
        "\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "        # print(f'type(np_loss) : {type(np_loss)}')\n",
        "        # print(f'np_loss : {np_loss}')\n",
        "        self.result_manager.testLoss_batch.append(np_loss)\n",
        "\n",
        "        # print(\"===============================testing=================================\\n\")\n",
        "        # print(f\"output : {outputs}\\n\")\n",
        "        # print(f\"output type : {type(outputs)}\\n\")\n",
        "        # print(f\"loss : {loss}\\n\")\n",
        "        # print(f\"loss type : {type(loss)}\\n\")\n",
        "        # print(f\"loss shape : {loss.shape}\\n\")\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        optimizer = AdamW(self.parameters(), lr=self.config_dict['LR'])\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.n_warmup_steps,\n",
        "                                                    num_training_steps=self.n_training_steps)\n",
        "        return dict(optimizer=optimizer, lr_scheduler=dict(scheduler=scheduler, interval=\"step\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA2UW3Xf58Vh"
      },
      "source": [
        "# calculate_warmup_stepsを定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 418,
      "metadata": {
        "id": "4ljK1VdE8BP5"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"This module is written for write useful function.\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_warmup_steps(train_df: pd.DataFrame, num_epochs: int, batch_size: int):\n",
        "    steps_per_epoch = len(train_df) // batch_size\n",
        "    total_training_steps = steps_per_epoch * num_epochs\n",
        "    warmup_steps = total_training_steps // 5\n",
        "    return total_training_steps, warmup_steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8eAFYfq3-CD"
      },
      "source": [
        "# ModelConfigを定義する\n",
        "- colab上では辞書\n",
        "- モジュール分けするときはjsonに"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dPiXl1ej7b3"
      },
      "source": [
        "## GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 419,
      "metadata": {
        "id": "kpgeqGQOkCpL"
      },
      "outputs": [],
      "source": [
        "# GPT2-small\n",
        "gpt2_small_config = {\n",
        "    'MODEL_NAME' : 'gpt2',\n",
        "    'TOKENIZER_NAME' : 'gpt2',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/small',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'small']\n",
        "}\n",
        "\n",
        "# GPT2-medium\n",
        "gpt2_medium_config = {\n",
        "    'MODEL_NAME' : 'gpt2-medium',\n",
        "    'TOKENIZER_NAME' : 'gpt2-medium',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 'gpt2/medium',\n",
        "    'MODEL_FOLDER' : ['gpt2', 'medium']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXnvK6fjj9uf"
      },
      "source": [
        "## T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 420,
      "metadata": {
        "id": "27MGdEADkWyL"
      },
      "outputs": [],
      "source": [
        "# T5-small\n",
        "t5_small_config = {\n",
        "    'MODEL_NAME' : 't5-small',\n",
        "    'TOKENIZER_NAME' : 't5-small',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 't5/small',\n",
        "    'MODEL_FOLDER' : ['t5', 'small']\n",
        "}\n",
        "\n",
        "# T5-base\n",
        "t5_base_config = {\n",
        "    'MODEL_NAME' : 't5-base',\n",
        "    'TOKENIZER_NAME' : 't5-base',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 't5/base',\n",
        "    'MODEL_FOLDER' : ['t5', 'base']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy9zBiRqkAC3"
      },
      "source": [
        "## BART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 421,
      "metadata": {
        "id": "5d4CXx8T4BWp"
      },
      "outputs": [],
      "source": [
        "# BART_base\n",
        "bart_base_config = {\n",
        "    'MODEL_NAME' : 'facebook/bart-base',\n",
        "    'TOKENIZER_NAME' : 'facebook/bart-base',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'CEG',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 'bart/base',\n",
        "    'MODEL_FOLDER' : ['bart', 'base']\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model_Dict"
      ],
      "metadata": {
        "id": "QoWojRc90I8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model_Config_Dict = {\n",
        "    'gpt2_small_config' : gpt2_small_config,\n",
        "    'gpt2_medium_config' : gpt2_medium_config,\n",
        "    't5_small_config' : t5_small_config,\n",
        "    't5_base_config' : t5_base_config,\n",
        "    'bart_base_config' : bart_base_config\n",
        "}"
      ],
      "metadata": {
        "id": "IILYrc640NFr"
      },
      "execution_count": 422,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shph_NDW3HpB"
      },
      "source": [
        "# class Prepare_tokenizerを定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 423,
      "metadata": {
        "id": "D0mJ9pIG3MHe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import warnings\n",
        "\n",
        "class Prepare_Tokenizer():\n",
        "    def __init__(self, config:dict):\n",
        "        self.config = config\n",
        "        self.TOKENIZER_NAME = self.config['TOKENIZER_NAME']\n",
        "        self.tokenizer = None\n",
        "        self.tokenizer_length = None\n",
        "        self.additional_special_tokens = None\n",
        "        self.eos_token = None\n",
        "        self.eos_token_id = None\n",
        "        self.pad_token = None\n",
        "        self.pad_token_id = None\n",
        "        self.exp_token = '[EXP]'\n",
        "        self.exp_token_id = None\n",
        "        self.initialization_flag = 0\n",
        "\n",
        "    def get_tokenizer(self):\n",
        "        if self.initialization_flag != 0:\n",
        "            raise ValueError(f\"tokenizer is already initialized. This instance is for {self.tokenizer}\")\n",
        "\n",
        "        if self.TOKENIZER_NAME in ['gpt2', 'gpt2-medium']:\n",
        "            self.initialization_flag = 1\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.TOKENIZER_NAME)\n",
        "            print(f\"tokenizer : AutoTokenizer.from_pretrained({self.TOKENIZER_NAME})\")\n",
        "            print(\"Before adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            special_tokens_dict = {\n",
        "            'additional_special_tokens': ['[EXP]'],\n",
        "            \"eos_token\" : \"<|endoftext|>\",\n",
        "            \"pad_token\" : \"<|endoftext|>\"\n",
        "            }\n",
        "            self.eos_token = \"<|endoftext|>\"\n",
        "            self.pad_token = \"<|endoftext|>\"\n",
        "            self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "            self.exp_token_id = self.tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "            self.eos_token_id = self.tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "            self.pad_token_id = self.tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "            print(\"After adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            print(f\"exp_id : {self.exp_token_id}\")\n",
        "            print(f\"eos_id : {self.eos_token_id}\")\n",
        "            print(f\"pad_id : {self.pad_token_id}\")\n",
        "            self.tokenizer_length = len(self.tokenizer)\n",
        "            print(f\"len(tokenizer) : {self.tokenizer_length}\")\n",
        "\n",
        "            return self.tokenizer\n",
        "\n",
        "        elif self.TOKENIZER_NAME in ['t5-small','t5-base']:\n",
        "            self.initialization_flag = 1\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.TOKENIZER_NAME)\n",
        "            print(f\"tokenizer : AutoTokenizer.from_pretrained({self.TOKENIZER_NAME})\")\n",
        "            print(\"Before adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            special_tokens_dict = {\n",
        "            'additional_special_tokens': ['[EXP]'],\n",
        "            }\n",
        "            self.eos_token = '</s>'\n",
        "            self.pad_token = '<pad>'\n",
        "            self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "            self.exp_token_id = self.tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "            self.eos_token_id = self.tokenizer.convert_tokens_to_ids('</s>')\n",
        "            self.pad_token_id = self.tokenizer.convert_tokens_to_ids('<pad>')\n",
        "            print(\"After adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            print(f\"exp_id : {self.exp_token_id}\")\n",
        "            print(f\"eos_id : {self.eos_token_id}\")\n",
        "            print(f\"pad_id : {self.pad_token_id}\")\n",
        "            self.tokenizer_length = len(self.tokenizer)\n",
        "            print(f\"len(tokenizer) : {self.tokenizer_length}\")\n",
        "\n",
        "            return self.tokenizer\n",
        "\n",
        "        elif self.TOKENIZER_NAME in ['facebook/bart-base']:\n",
        "            self.initialization_flag = 1\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.TOKENIZER_NAME)\n",
        "            print(f\"tokenizer : AutoTokenizer.from_pretrained({self.TOKENIZER_NAME})\")\n",
        "            print(\"Before adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            special_tokens_dict = {\n",
        "            'additional_special_tokens': ['[EXP]'],\n",
        "            }\n",
        "            self.eos_token = '</s>'\n",
        "            self.pad_token = '<pad>'\n",
        "            self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "            self.exp_token_id = self.tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "            self.eos_token_id = self.tokenizer.convert_tokens_to_ids('</s>')\n",
        "            self.pad_token_id = self.tokenizer.convert_tokens_to_ids('<pad>')\n",
        "            print(\"After adding additional_special_tokens\")\n",
        "            print(self.tokenizer.all_special_tokens)\n",
        "            print(self.tokenizer.all_special_ids)\n",
        "            print(f\"exp_id : {self.exp_token_id}\")\n",
        "            print(f\"eos_id : {self.eos_token_id}\")\n",
        "            print(f\"pad_id : {self.pad_token_id}\")\n",
        "            self.tokenizer_length = len(self.tokenizer)\n",
        "            print(f\"len(tokenizer) : {self.tokenizer_length}\")\n",
        "\n",
        "            return self.tokenizer\n",
        "        else:\n",
        "            raise ValueError('Wrong Tokenizer Type : you have to indicate tokenizer type out of the folloing list.\\n [\"gpt2\", \"gpt2-medium\", \"t5-small\", \"t5-base\", \"bart-base\"]\\n')\n",
        "\n",
        "    def get_tokenizer_info(self):\n",
        "        if self.initialization_flag == 0:\n",
        "            print(\"there is no information. you need to call get_tokenizer() first.\")\n",
        "        else:\n",
        "            print(\"---------- tokenizer information ----------\")\n",
        "            print(f\"self.TOKENIZER_NAME : {self.TOKENIZER_NAME}\")\n",
        "            print(f\"self.tokenizer : {self.tokenizer}\")\n",
        "            print(f\"self.tokenizer_length : {self.tokenizer_length}\")\n",
        "            print(f\"self.additional_special_tokens : {self.additional_special_tokens}\")\n",
        "            print(f\"self.eos_token : {self.eos_token}\")\n",
        "            print(f\"self.eos_token_id : {self.eos_token_id}\")\n",
        "            print(f\"self.pad_token : {self.pad_token}\")\n",
        "            print(f\"self.pad_token_id : {self.pad_token_id}\")\n",
        "            print(f\"self.exp_token : {self.exp_token}\")\n",
        "            print(f\"self.exp_token_id : {self.exp_token_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkBEZqVYGCj7"
      },
      "source": [
        "# class Prepare_Modelを定義する\n",
        "- 今のところ、モデルごとに操作の大きな違いはなさそう\n",
        "- BARTに関して,文の頭に`<s>`が付く"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 424,
      "metadata": {
        "id": "HE6f_5MZGr0k"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, T5ForConditionalGeneration, BartForConditionalGeneration, AutoConfig\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Prepare_Model():\n",
        "\n",
        "    def __init__(self, config:dict, prepare_tokenizer):\n",
        "        self.MODEL_NAME = config['MODEL_NAME']\n",
        "        self.TOKENIZER_LENGTH = prepare_tokenizer.tokenizer_length\n",
        "        self.additional_model_config = None\n",
        "        self.prepare_tokenizer = prepare_tokenizer\n",
        "        self.model = None\n",
        "        self.initialization_flag = 0\n",
        "\n",
        "\n",
        "    def get_model(self):\n",
        "        if self.initialization_flag == 1:\n",
        "            raise ValueError('You cannot call get_model() because this is already initialized.\\n You are supossed to instantiate Prepare_Model_Class and then call get_model() again.')\n",
        "\n",
        "        if self.MODEL_NAME in ['gpt2', 'gpt2-medium']:\n",
        "\n",
        "            if self.prepare_tokenizer is None:\n",
        "                raise ValueError('prepare_tokenizer is None and this is not good for gpt2!')\n",
        "            self.initialization_flag = 1\n",
        "            self.additional_model_config = AutoConfig.from_pretrained(self.MODEL_NAME, eos_token_id=self.prepare_tokenizer.eos_token_id, pad_token_id=self.prepare_tokenizer.pad_token_id, additional_special_tokens_id = self.prepare_tokenizer.exp_token_id, output_hidden_states=False)\n",
        "            self.model = GPT2LMHeadModel.from_pretrained(self.MODEL_NAME, config=self.additional_model_config)\n",
        "            self.model.resize_token_embeddings(self.TOKENIZER_LENGTH)\n",
        "            self.lm_head = self.model.lm_head\n",
        "            new_weights = torch.cat([self.lm_head.weight[:-1, :], torch.zeros(1, self.lm_head.weight.shape[1]) -10000])\n",
        "            self.model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "\n",
        "            return self.model\n",
        "\n",
        "        elif self.MODEL_NAME in ['t5-small','t5-base']:\n",
        "\n",
        "            self.initialization_flag = 1\n",
        "            self.model = T5ForConditionalGeneration.from_pretrained(self.MODEL_NAME)\n",
        "            self.model.resize_token_embeddings(self.TOKENIZER_LENGTH)\n",
        "            # not sure the following 2 lines are actually necessary\n",
        "            print(f\"weights.shape : {self.model.lm_head.weight.shape}\")\n",
        "            # new_weights = torch.cat([self.model.lm_head.weight[:-1, :], torch.zeros(1, self.model.lm_head.weight.shape[1]) -10000])\n",
        "            # self.model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "\n",
        "\n",
        "            # print(f\"new_weights.shape : {self.model.lm_head.weight.shape}\")\n",
        "\n",
        "            return self.model\n",
        "\n",
        "        elif self.MODEL_NAME in ['facebook/bart-base']:\n",
        "\n",
        "            self.initialization_flag = 1\n",
        "            self.model = BartForConditionalGeneration.from_pretrained(self.MODEL_NAME)\n",
        "            self.model.resize_token_embeddings(self.TOKENIZER_LENGTH)\n",
        "            # not sure the following 2 lines are actually necessary\n",
        "            new_weights = torch.cat([self.model.lm_head.weight[:-1, :], torch.zeros(1, self.model.lm_head.weight.shape[1]) -10000])\n",
        "            self.model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "            return self.model\n",
        "        else:\n",
        "            raise ValueError('Wrong Model Type : you have to indicate model type out of the folloing list.\\n [\"gpt2\", \"gpt2-medium\", \"t5-small\", \"t5-base\", \"bart-base\"]\\n')\n",
        "\n",
        "\n",
        "    def get_model_info(self):\n",
        "        if self.initialization_flag == 0:\n",
        "            print(\"there is no information. you need to call get_model() first.\")\n",
        "        else:\n",
        "            print(\"---------- model information ----------\")\n",
        "            print(f\"self.MODEL_NAME : {self.MODEL_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 425,
      "metadata": {
        "id": "2WCJwC68PT4G"
      },
      "outputs": [],
      "source": [
        "# my_prepare_tokenizer = Prepare_Tokenizer(bart_base_config)\n",
        "# my_prepare_tokenizer.get_tokenizer_info()\n",
        "# my_tokenizer = my_prepare_tokenizer.get_tokenizer()\n",
        "# my_prepare_tokenizer.get_tokenizer_info()\n",
        "# my_prepare_model = Prepare_Model(config=bart_base_config, prepare_tokenizer=my_prepare_tokenizer)\n",
        "# my_prepare_model.get_model_info()\n",
        "# my_model = my_prepare_model.get_model()\n",
        "# my_prepare_model.get_model_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0v_Z78O729U"
      },
      "source": [
        "# Trainerを定義する\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 426,
      "metadata": {
        "id": "s6bUD-2e8ZaY"
      },
      "outputs": [],
      "source": [
        "# import pytorch_lightning as pl\n",
        "# import torch\n",
        "# from pytorch_lightning.callbacks import EarlyStopping\n",
        "# from pytorch_lightning.loggers import TensorBoardLogger\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from transformers import BertTokenizer\n",
        "\n",
        "# random_seed = gpt2_small_config[\"RANDOM_SEED\"]\n",
        "# pl.seed_everything(random_seed)\n",
        "\n",
        "# print(\"train_data_path : \" + gpt2_small_config[\"TRAINING_DATA_PATH\"])\n",
        "# print(\"test_data_path : \" + gpt2_small_config[\"TEST_DATA_PATH\"])\n",
        "# print(\"val_data_path : \" + gpt2_small_config[\"VALIDATION_DATA_PATH\"])\n",
        "# train_data = read_tsv(gpt2_small_config[\"TRAINING_DATA_PATH\"])\n",
        "# test_data = read_tsv(gpt2_small_config[\"TEST_DATA_PATH\"])\n",
        "# val_data = read_tsv(gpt2_small_config[\"VALIDATION_DATA_PATH\"])\n",
        "\n",
        "\n",
        "# data_module =  LIAR_PLUS_DataModule_For_CEG(\n",
        "#     gpt2_small_config,\n",
        "#     train_data,\n",
        "#     test_data,\n",
        "#     val_data\n",
        "#     )\n",
        "\n",
        "# total_training_steps, warmup_steps = calculate_warmup_steps(train_data, gpt2_small_config[\"TRAINING_EPOCHS\"],\n",
        "#                                                             gpt2_small_config[\"TRAINING_BATCH_SIZE\"])\n",
        "\n",
        "# print(f\"total_training_steps : {total_training_steps} , warmup_steps : {warmup_steps} \")\n",
        "\n",
        "# model = CEG(\n",
        "#     config_dict=gpt2_small_config,\n",
        "#     n_warmup_steps=warmup_steps,\n",
        "#     n_training_steps=total_training_steps,\n",
        "#     n_classes=2\n",
        "#     )\n",
        "\n",
        "# model\n",
        "# checkpoint_callback = build_checkpoint_callback(gpt2_small_config)\n",
        "# logger = TensorBoardLogger(path_dict['TensorBoardLogger'], name=gpt2_small_config['TensorBoardLogger_NAME'])\n",
        "# early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "# trainer = pl.Trainer(\n",
        "#     logger=logger,\n",
        "#     callbacks=[checkpoint_callback, early_stopping_callback],\n",
        "#     max_epochs=gpt2_small_config['TRAINING_EPOCHS'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOSy57F573Kk"
      },
      "source": [
        "# trainer.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 427,
      "metadata": {
        "id": "HV1Tx7GO-u-c"
      },
      "outputs": [],
      "source": [
        "# trainer.fit(model, datamodule=data_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Np19lF73OV"
      },
      "source": [
        "# trainer.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 428,
      "metadata": {
        "id": "-DxYnT8B-25b"
      },
      "outputs": [],
      "source": [
        "# trainer.test(datamodule=data_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1HyKsIv73SD"
      },
      "source": [
        "# 結果を確認する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 429,
      "metadata": {
        "id": "xRHqmgR0F59h"
      },
      "outputs": [],
      "source": [
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir=lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "157MXEFW9qNZ"
      },
      "source": [
        "# class Fine_Tuned_Model\n",
        "- fine-tuning済みのモデルをロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 430,
      "metadata": {
        "id": "EmN_mlcS9xxq"
      },
      "outputs": [],
      "source": [
        "class Fine_Tuned_Model():\n",
        "    def __init__(self, config_dict, ckpt_file_name, file_name:str):\n",
        "\n",
        "        self.config_dict = config_dict\n",
        "        self.ckpt_file_name = ckpt_file_name\n",
        "        self.file_name = file_name\n",
        "        self.component_name = config_dict['COMPONENT_NAME']\n",
        "        self.MODEL_NAME = config_dict[\"MODEL_NAME\"]\n",
        "        self.transformers_model_path = f'./model_transformers/{self.MODEL_NAME}/{self.file_name}'\n",
        "        self.initialization_flag = 0\n",
        "        self.model = None\n",
        "        self.pretrained_model = None\n",
        "\n",
        "    def create_directory_for_transformers_model(self):\n",
        "        if self.initialization_flag == 1:\n",
        "            raise ValueError('self.initialization_flag == 1 : you are supposed to instantiate Fine_Tuned_Model once and call it again.')\n",
        "        else:\n",
        "            self.initialization_flag = 1\n",
        "\n",
        "        if self.component_name == 'CEG':\n",
        "            self.model = CEG.load_from_checkpoint(gpt2_small_config['SAVED_MODEL_PATH']+ f'/{self.ckpt_file_name}')\n",
        "            self.model.model.save_pretrained(self.transformers_model_path)\n",
        "            print(f'{self.MODEL_NAME} was saved as {self.component_name} in {self.transformers_model_path}.')\n",
        "        elif self.component_name == 'EP':\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(f'self.component_name : {self.component_name} is not supported.')\n",
        "\n",
        "\n",
        "    def load_model(self):\n",
        "        if self.initialization_flag == 0:\n",
        "            raise ValueError('self.initialization_flag == 0 : you are supposed to call create_directory_for_transformer_model() first.')\n",
        "\n",
        "        if self.MODEL_NAME in ['gpt2', 'gpt2-medium']:\n",
        "            self.pretrained_model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/' + f'{self.MODEL_NAME}/{self.file_name}')\n",
        "            print(self.pretrained_model)\n",
        "\n",
        "            return self.pretrained_model\n",
        "\n",
        "        elif self.MODEL_NAME in ['t5-small','t5-base']:\n",
        "            self.pretrained_model = T5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/' + f'{self.MODEL_NAME}/{self.file_name}')\n",
        "            print(self.pretrained_model)\n",
        "\n",
        "            return self.pretrained_model\n",
        "\n",
        "        elif self.MODEL_NAME in ['facebook/bart-base']:\n",
        "            self.pretrained_model = BartForConditionalGeneration.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/' + f'{self.MODEL_NAME}/{self.file_name}')\n",
        "            print(self.pretrained_model)\n",
        "\n",
        "            return self.pretrained_model\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Wrong Model Type : you have to indicate model type out of the folloing list.\\n [\"gpt2\", \"gpt2-medium\", \"t5-small\", \"t5-base\", \"bart-base\"]\\n')\n",
        "\n",
        "\n",
        "    def get_model_info(self):\n",
        "        if self.initialization_flag == 0:\n",
        "            print(\"there is no information. you need to call load_model() first.\")\n",
        "        else:\n",
        "            print(\"---------- model information ----------\")\n",
        "            print(f'self.config_dict : {self.config_dict}')\n",
        "            print(f'self.ckpt_file_name : {self.ckpt_file_name}')\n",
        "            print(f'self.file_name : {self.file_name}')\n",
        "            print(f'self.component_name : {self.component_name}')\n",
        "            print(f'self.MODEL_NAME : {self.MODEL_NAME}')\n",
        "            print(f'self.transformers_model_path : {self.transformers_model_path}')\n",
        "            print(f'self.initialization_flag : {self.initialization_flag}')\n",
        "            print(f'self.model : {self.model}')\n",
        "            print(f'self.pretrained_model : {self.pretrained_model}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class Result_Manager\n",
        "- 結果の保存や表示を行う\n",
        "- 次の指標を保存する\n",
        "## バッチごと\n",
        "- trainAcc\n",
        "- trainF1\n",
        "- trainloss\n",
        "- valAcc\n",
        "- valF1\n",
        "- valloss\n",
        "- testAcc\n",
        "- testF1\n",
        "- testloss\n",
        "\n",
        "\n",
        "## エポックごと\n",
        "- trainAcc\n",
        "- trainF1\n",
        "- trainloss\n",
        "- valAcc\n",
        "- valF1\n",
        "- valoss\n"
      ],
      "metadata": {
        "id": "ylG8KaRfYSd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "import datetime\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "yHeo6y028rRe"
      },
      "execution_count": 431,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class  Result_Manager():\n",
        "    def __init__(self, base_time, batch_size, model_config):\n",
        "        self.model_config = model_config\n",
        "        self.batch_size = batch_size\n",
        "        self.epoch_size = None\n",
        "        self.dt_now = base_time\n",
        "        self.metrics_name_list = [\n",
        "            'trainAcc_batch',\n",
        "            'trainF1_batch',\n",
        "            'trainLoss_batch',\n",
        "            'valAcc_batch',\n",
        "            'valF1_batch',\n",
        "            'valLoss_batch',\n",
        "            'testAcc_batch',\n",
        "            'testF1_batch',\n",
        "            'testLoss_batch',\n",
        "            'trainAcc_epoch',\n",
        "            'trainF1_epoch',\n",
        "            'trainLoss_epoch',\n",
        "            'valAcc_epoch',\n",
        "            'valF1_epoch',\n",
        "            'valLoss_epoch',\n",
        "        ]\n",
        "\n",
        "        self.trainAcc_batch = []\n",
        "        self.trainF1_batch = []\n",
        "        self.trainLoss_batch = []\n",
        "        self.valAcc_batch = []\n",
        "        self.valF1_batch = []\n",
        "        self.valLoss_batch = []\n",
        "        self.testAcc_batch = []\n",
        "        self.testF1_batch = []\n",
        "        self.testLoss_batch = []\n",
        "\n",
        "        self.trainAcc_epoch = []\n",
        "        self.trainF1_epoch = []\n",
        "        self.trainLoss_epoch = []\n",
        "        self.valAcc_epoch = []\n",
        "        self.valF1_epoch = []\n",
        "        self.valLoss_epoch = []\n",
        "\n",
        "        self.metrics_list = [\n",
        "            self.trainAcc_batch,\n",
        "            self.trainF1_batch,\n",
        "            self.trainLoss_batch,\n",
        "            self.valAcc_batch,\n",
        "            self.valF1_batch,\n",
        "            self.valLoss_batch,\n",
        "            self.testAcc_batch,\n",
        "            self.testF1_batch,\n",
        "            self.testLoss_batch,\n",
        "            self.trainAcc_epoch,\n",
        "            self.trainF1_epoch,\n",
        "            self.trainLoss_epoch,\n",
        "            self.valAcc_epoch,\n",
        "            self.valF1_epoch,\n",
        "            self.valLoss_epoch,\n",
        "        ]\n",
        "\n",
        "    def _build_folder(self, base_path: str):\n",
        "\n",
        "        for folder in self.model_config['MODEL_FOLDER']:\n",
        "            base_path = f'{base_path}/{folder}'\n",
        "            if not os.path.exists(base_path):\n",
        "                os.mkdir(base_path)\n",
        "\n",
        "        return base_path\n",
        "\n",
        "\n",
        "    def save_result_log(self, base_path: str):\n",
        "\n",
        "        base_path = f'{base_path}/result_log/{self.dt_now}'\n",
        "        if not os.path.exists(base_path):\n",
        "            os.mkdir(base_path)\n",
        "\n",
        "        model_path = self._build_folder(base_path)\n",
        "\n",
        "        folder = f'{model_path}/batch_size:{self.batch_size}'\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "\n",
        "        for idx in range(len(self.metrics_list)):\n",
        "\n",
        "            if len(self.metrics_list[idx]) != 0:\n",
        "                file_name = self.metrics_name_list[idx]+'.csv'\n",
        "                # print(f'file_name : {file_name}')\n",
        "                # np_metric = [item.detach().cpu().numpy().astype(float) for item in self.metrics_list[idx]]\n",
        "                np_metric = self.metrics_list[idx]\n",
        "                print(self.metrics_name_list[idx])\n",
        "                print(f'len(np_metric) : {len(np_metric)}')\n",
        "                data = pd.DataFrame({self.metrics_name_list[idx]:np_metric})\n",
        "                data.to_csv(f'{folder}/{file_name}', header=True, index=False)\n",
        "\n",
        "\n",
        "        # pd.Series(self.trainAcc_batch).to_csv(folder+'/trainAcc_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainF1_batch).to_csv(folder+'/trainF1_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainLoss_batch).to_csv(folder+'/trainLoss_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valAcc_batch).to_csv(folder+'/valAcc_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valF1_batch).to_csv(folder+'/valF1_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valLoss_batch).to_csv(folder+'/valLoss_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.testAcc_batch).to_csv(folder+'/testAcc_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.testF1_batch).to_csv(folder+'/testF1_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.testLoss_batch).to_csv(folder+'/testLoss_batch.csv', header=False, index=False)\n",
        "\n",
        "        # pd.Series(self.trainAcc_epoch).to_csv(folder+'/trainAcc_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainF1_epoch).to_csv(folder+'/trainF1_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainLoss_epoch).to_csv(folder+'/trainLoss_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valAcc_epoch).to_csv(folder+'/valAcc_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valF1_epoch).to_csv(folder+'/valF1_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valLoss_epoch).to_csv(folder+'/valLoss_epoch.csv', header=False, index=False)\n",
        "\n",
        "\n",
        "    def make_single_result_img(self, base_path:str):\n",
        "\n",
        "        base_path = f'{base_path}/plot_figures/{self.dt_now}'\n",
        "        if not os.path.exists(base_path):\n",
        "            os.mkdir(base_path)\n",
        "\n",
        "        model_path = self._build_folder(base_path)\n",
        "\n",
        "        folder = f'{model_path}/batch_size:{self.batch_size}'\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "\n",
        "        for idx in range(len(self.metrics_name_list)):\n",
        "\n",
        "            save_fig_path = f'/{folder}/' + self.metrics_name_list[idx] + '.png'\n",
        "\n",
        "            if len(self.metrics_list[idx]) == 0:\n",
        "                continue\n",
        "            else:\n",
        "                plt.figure()\n",
        "                np_metric = self.metrics_list[idx]\n",
        "                # print(f'type(np_metric) : {type(np_metric)}')\n",
        "                # print(self.metrics_name_list[idx])\n",
        "                # print(f'np_metric : {np_metric}')\n",
        "                data = pd.DataFrame({self.metrics_name_list[idx]:np_metric})\n",
        "                # print(f'type(data) : {type(data)}')\n",
        "                data = data.astype(float)\n",
        "                # print(f'type(data) : {type(data)}')\n",
        "                data.plot(\n",
        "                title=self.metrics_name_list[idx],\n",
        "                legend=True\n",
        "                )\n",
        "                plt.savefig(save_fig_path)\n",
        "                # plt.close('all')\n",
        "\n",
        "\n",
        "            # plt.figure()\n",
        "            # if len(self.metrics_list[idx]) != 0:\n",
        "            #     np_metric = self.metrics_list[idx]\n",
        "            #     print(f'type(np_metric) : {type(np_metric)}')\n",
        "            #     print(self.metrics_name_list[idx])\n",
        "            #     print(f'np_metric : {np_metric}')\n",
        "            #     data = pd.Series(np_metric)\n",
        "            #     print(f'type(data) : {type(data)}')\n",
        "            #     data = data.astype(float)\n",
        "            #     print(f'type(data) : {type(data)}')\n",
        "            #     data.plot(\n",
        "            #     title=self.metrics_name_list[idx],\n",
        "            #     legend=True\n",
        "            #     )\n",
        "            # plt.savefig(save_fig_path)\n",
        "            # plt.close('all')\n",
        "\n",
        "    # def make_imgs_into_one(self, base_path:str, metrics: list, figure_name:str):\n",
        "\n",
        "    #     base_path = base_path + '/plot_figures/' + self.dt_now + '/'\n",
        "    #     imgs = []\n",
        "\n",
        "    #     for i in range(len(metrics)):\n",
        "    #         j = base_path + metrics[i] + '.png'\n",
        "    #         imgs.append(j)\n",
        "    #         j = ''\n",
        "\n",
        "    #     save_fig_path = base_path + figure_name +'.png'\n",
        "    #     fig, axs = plt.subplots(1, len(metrics), figsize=(10, 5))\n",
        "\n",
        "    #     for i, im in zip(range(len(axs)), imgs):\n",
        "    #         img = Image.open(im)\n",
        "    #         axs[i].imshow(img)\n",
        "    #         axs[i].set_title(metrics[i])\n",
        "    #         axs[i].axis('off')\n",
        "\n",
        "    #     plt.show()\n",
        "    #     plt.savefig(save_fig_path)\n",
        "\n",
        "    # def make_data_into_one(self, base_path:str, metrics: list, figure_name:str):\n",
        "\n",
        "    #     base_path = base_path + '/result_log/' + self.dt_now + '/'\n",
        "    #     data = []\n",
        "\n",
        "    #     for i in range(len(metrics)):\n",
        "    #         data_path = base_path + i + '.csv'\n",
        "    #         data.append(pd.read_csv(data_path))\n",
        "    #         data_path = ''\n",
        "\n",
        "    #     save_fig_path = base_path + figure_name +'.png'\n",
        "    #     fig, axs = plt.subplots(1, len(metrics), figsize=(10, 5))\n",
        "\n",
        "    #     for i, im in zip(range(len(axs)), imgs):\n",
        "    #         img = Image.open(im)\n",
        "    #         axs[i].imshow(img) # cmap=カラーマップの選択\n",
        "    #         axs[i].set_title(metrics[i])\n",
        "    #         axs[i].axis('off')\n",
        "\n",
        "    #     plt.show()\n",
        "    #     plt.savefig(save_fig_path)\n",
        "\n",
        "    # def get_result_img(self, base_path:str, image_name:str):\n",
        "    #     imgs = base_path + '/plot_figures/' + self.dt_now + '/' + image_name\n",
        "    #     img = imread(fname=imgs, format='png')\n",
        "    #     plt.imshow(\n",
        "    #         img,\n",
        "    #     )\n",
        "    #     plt.show()"
      ],
      "metadata": {
        "id": "s0FmH8BGY5Zw"
      },
      "execution_count": 432,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# base_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning'\n",
        "\n",
        "# my_Result_Manager = Result_Manager()\n",
        "# my_Result_Manager.save_result_log(base_path)\n",
        "# my_Result_Manager.make_single_result_img(base_path)\n",
        "# my_Result_Manager.make_imgs_into_one(base_path, ['trainLoss_batch', 'valLoss_batch','trainLoss_batch', 'valLoss_batch', 'trainLoss_batch', 'valLoss_batch'], 'example_image')"
      ],
      "metadata": {
        "id": "zx_rIKKtDDqm"
      },
      "execution_count": 433,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class myclass():\n",
        "    def __init__(self, num:int):\n",
        "        self.num = num\n",
        "\n",
        "    def _add_one(self):\n",
        "        return self.num +1\n",
        "\n",
        "    def add_one(self):\n",
        "        return self._add_one()\n"
      ],
      "metadata": {
        "id": "G3lf9p7sLyYO"
      },
      "execution_count": 434,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mc = myclass(99)\n",
        "print(mc.add_one())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdXGjnkbMm9s",
        "outputId": "7a6f99d1-85db-4bff-b7f2-e060fa8c8544"
      },
      "execution_count": 435,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train_and_test.py"
      ],
      "metadata": {
        "id": "yJThITDcX55U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer"
      ],
      "metadata": {
        "id": "sBM_FaGPz8Bm"
      },
      "execution_count": 436,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_test_config =  {\n",
        "    'MODEL_NAME' : ['gpt2_small_config', 'gpt2_medium_config', 't5_small_config', 't5_base_config', 'bart_base_config'],\n",
        "    'BATCH_SIZE' : [1, 2, 4, 8], # train\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'TRAINING_DATA_PATH' : path_dict['toy_input_for_lightning_model_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH' : path_dict['toy_input_for_lightning_model_val_data_true'],\n",
        "    'TEST_DATA_PATH' : path_dict['toy_input_for_lightning_model_test_data_true'],\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "lwiMog4M1qv9"
      },
      "execution_count": 437,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def train_and_test_CEG\n",
        "- バッチをいろいろ変えて実験を行う"
      ],
      "metadata": {
        "id": "F8DU2vwmi62H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_test_CEG(model_config, train_and_test_config, batch_size:int, base_time):\n",
        "\n",
        "    print(f'----------- training and testing has just started (batch_size : {batch_size}) -----------')\n",
        "    pl.seed_everything(train_and_test_config['RANDOM_SEED'])\n",
        "\n",
        "    train_data = read_tsv(train_and_test_config[\"TRAINING_DATA_PATH\"])\n",
        "    test_data = read_tsv(train_and_test_config[\"TEST_DATA_PATH\"])\n",
        "    val_data = read_tsv(train_and_test_config[\"VALIDATION_DATA_PATH\"])\n",
        "    model_config['TRAINING_BATCH_SIZE'] = batch_size\n",
        "    result_manager = Result_Manager(base_time, batch_size, model_config)\n",
        "    base_path = path_dict['TensorBoardLogger'] + f'/{base_time}'\n",
        "    folder = model_config['TensorBoardLogger_NAME'] + f'/batch_size:{batch_size}'\n",
        "\n",
        "    data_module =  LIAR_PLUS_DataModule_For_CEG(\n",
        "                    model_config,\n",
        "                    train_data,\n",
        "                    test_data,\n",
        "                    val_data\n",
        "                    )\n",
        "\n",
        "    total_training_steps, warmup_steps = calculate_warmup_steps(train_data, model_config[\"TRAINING_EPOCHS\"],\n",
        "                                                        model_config[\"TRAINING_BATCH_SIZE\"])\n",
        "    print(f\"total_training_steps : {total_training_steps} , warmup_steps : {warmup_steps} \")\n",
        "    model = CEG(\n",
        "            config_dict=model_config,\n",
        "            n_warmup_steps=warmup_steps,\n",
        "            n_training_steps=total_training_steps,\n",
        "            n_classes=2,\n",
        "            result_manager=result_manager)\n",
        "\n",
        "    checkpoint_callback = build_checkpoint_callback(model_config, base_time, batch_size)\n",
        "    logger = TensorBoardLogger(base_path, name=folder)\n",
        "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        logger=logger,\n",
        "        callbacks=[checkpoint_callback, early_stopping_callback],\n",
        "        max_epochs=model_config['TRAINING_EPOCHS'])\n",
        "\n",
        "    trainer.fit(model, datamodule=data_module)\n",
        "    trainer.test(datamodule=data_module)\n",
        "\n",
        "    model.result_manager.save_result_log('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning')\n",
        "    model.result_manager.make_single_result_img('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning')\n",
        "\n"
      ],
      "metadata": {
        "id": "gYTebhHDjLvC"
      },
      "execution_count": 438,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def get_base_time\n",
        "- ベースタイムを取得する"
      ],
      "metadata": {
        "id": "4gN4z8_DGBp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_time():\n",
        "\n",
        "    now_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "    base_time = str(now_time)[:-16]\n",
        "\n",
        "    print(f'you got base_time : {base_time}')\n",
        "\n",
        "    return base_time\n"
      ],
      "metadata": {
        "id": "g97VCxNfGZFk"
      },
      "execution_count": 439,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_time = get_base_time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxXZI1HDmqbd",
        "outputId": "befbd05e-cc21-4170-c0bd-a633be93b96e"
      },
      "execution_count": 445,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you got base_time : 2024-01-15 19:11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_and_test_CEG(\n",
        "    model_config=gpt2_small_config,\n",
        "    train_and_test_config=train_and_test_config,\n",
        "    batch_size=1,\n",
        "    base_time=base_time)"
      ],
      "metadata": {
        "id": "5uW-MK4zqqvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train_and_test_CEGの実行による各モデルの訓練"
      ],
      "metadata": {
        "id": "60u4GTAT0Cvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # gpt2-small\n",
        "# for batch_size in [1, 2, 4, 8]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_small_config,\n",
        "#         train_and_test_config=train_and_test_config,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time)"
      ],
      "metadata": {
        "id": "2LoaFzU1zb37"
      },
      "execution_count": 441,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # gpt2-medium\n",
        "# for batch_size in [1, 2, 4, 8]:\n",
        "\n",
        "#     train_and_test_CEG(\n",
        "#         model_config=gpt2_medium_config,\n",
        "#         train_and_test_config=train_and_test_config,\n",
        "#         batch_size=batch_size,\n",
        "#         base_time=base_time)"
      ],
      "metadata": {
        "id": "IbWO94Io0RLO"
      },
      "execution_count": 442,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# t5-small\n",
        "for batch_size in [1, 2, 4, 8]:\n",
        "\n",
        "    train_and_test_CEG(\n",
        "        model_config=t5_small_config,\n",
        "        train_and_test_config=train_and_test_config,\n",
        "        batch_size=batch_size,\n",
        "        base_time=base_time)"
      ],
      "metadata": {
        "id": "sHL8J0lI0Rlx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b5395d1c6d2e4d0bbfe2f730d5ba5903",
            "2437af86be70480b983248e50a931a32",
            "f48a91dc6b7946a7b8eb76f2381cbb74",
            "2a106f10fa84401fb4ea97beab6f5f24",
            "1a1e37058555418ab5c36ff7b9a45248",
            "9297b0ad25fc461c9c8a4b9fbbbb4de6",
            "2970346c22ca4d998149e31b63d6ba4f",
            "8ac9b442659640fa847fe82096539f00",
            "4beb68466b184160b314f1d4e3ff6a4c",
            "452fff34c777421187bea9c3c13228c4",
            "af666b0ce46345ecb166149b4e73e294"
          ]
        },
        "outputId": "1ddab600-29a2-439c-a7f7-712641947b01"
      },
      "execution_count": 443,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- training and testing has just started (batch_size : 1) -----------\n",
            "total_training_steps : 138 , warmup_steps : 27 \n",
            "tokenizer : AutoTokenizer.from_pretrained(t5-small)\n",
            "Before adding additional_special_tokens\n",
            "['</s>', '<unk>', '<pad>', '<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']\n",
            "[1, 2, 0, 32099, 32098, 32097, 32096, 32095, 32094, 32093, 32092, 32091, 32090, 32089, 32088, 32087, 32086, 32085, 32084, 32083, 32082, 32081, 32080, 32079, 32078, 32077, 32076, 32075, 32074, 32073, 32072, 32071, 32070, 32069, 32068, 32067, 32066, 32065, 32064, 32063, 32062, 32061, 32060, 32059, 32058, 32057, 32056, 32055, 32054, 32053, 32052, 32051, 32050, 32049, 32048, 32047, 32046, 32045, 32044, 32043, 32042, 32041, 32040, 32039, 32038, 32037, 32036, 32035, 32034, 32033, 32032, 32031, 32030, 32029, 32028, 32027, 32026, 32025, 32024, 32023, 32022, 32021, 32020, 32019, 32018, 32017, 32016, 32015, 32014, 32013, 32012, 32011, 32010, 32009, 32008, 32007, 32006, 32005, 32004, 32003, 32002, 32001, 32000]\n",
            "After adding additional_special_tokens\n",
            "['</s>', '<unk>', '<pad>', '[EXP]']\n",
            "[1, 2, 0, 32100]\n",
            "exp_id : 32100\n",
            "eos_id : 1\n",
            "pad_id : 0\n",
            "len(tokenizer) : 32101\n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : t5-small\n",
            "self.tokenizer : T5TokenizerFast(name_or_path='t5-small', vocab_size=32100, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['[EXP]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32000: AddedToken(\"<extra_id_99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32001: AddedToken(\"<extra_id_98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32002: AddedToken(\"<extra_id_97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32003: AddedToken(\"<extra_id_96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32004: AddedToken(\"<extra_id_95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32005: AddedToken(\"<extra_id_94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32006: AddedToken(\"<extra_id_93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32007: AddedToken(\"<extra_id_92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32008: AddedToken(\"<extra_id_91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32009: AddedToken(\"<extra_id_90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32010: AddedToken(\"<extra_id_89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32011: AddedToken(\"<extra_id_88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32012: AddedToken(\"<extra_id_87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32013: AddedToken(\"<extra_id_86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32014: AddedToken(\"<extra_id_85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32015: AddedToken(\"<extra_id_84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32016: AddedToken(\"<extra_id_83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32017: AddedToken(\"<extra_id_82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32018: AddedToken(\"<extra_id_81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32019: AddedToken(\"<extra_id_80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32020: AddedToken(\"<extra_id_79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32021: AddedToken(\"<extra_id_78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32022: AddedToken(\"<extra_id_77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32023: AddedToken(\"<extra_id_76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32024: AddedToken(\"<extra_id_75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32025: AddedToken(\"<extra_id_74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32026: AddedToken(\"<extra_id_73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32027: AddedToken(\"<extra_id_72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32028: AddedToken(\"<extra_id_71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32029: AddedToken(\"<extra_id_70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32030: AddedToken(\"<extra_id_69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32031: AddedToken(\"<extra_id_68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32032: AddedToken(\"<extra_id_67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32033: AddedToken(\"<extra_id_66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32034: AddedToken(\"<extra_id_65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32035: AddedToken(\"<extra_id_64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32036: AddedToken(\"<extra_id_63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32037: AddedToken(\"<extra_id_62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32038: AddedToken(\"<extra_id_61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32039: AddedToken(\"<extra_id_60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32040: AddedToken(\"<extra_id_59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32041: AddedToken(\"<extra_id_58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32042: AddedToken(\"<extra_id_57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32043: AddedToken(\"<extra_id_56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32044: AddedToken(\"<extra_id_55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32045: AddedToken(\"<extra_id_54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32046: AddedToken(\"<extra_id_53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32047: AddedToken(\"<extra_id_52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32048: AddedToken(\"<extra_id_51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32049: AddedToken(\"<extra_id_50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32050: AddedToken(\"<extra_id_49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32051: AddedToken(\"<extra_id_48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32052: AddedToken(\"<extra_id_47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32053: AddedToken(\"<extra_id_46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32054: AddedToken(\"<extra_id_45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32055: AddedToken(\"<extra_id_44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32056: AddedToken(\"<extra_id_43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32057: AddedToken(\"<extra_id_42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32058: AddedToken(\"<extra_id_41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32059: AddedToken(\"<extra_id_40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32060: AddedToken(\"<extra_id_39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32061: AddedToken(\"<extra_id_38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32062: AddedToken(\"<extra_id_37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32063: AddedToken(\"<extra_id_36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32064: AddedToken(\"<extra_id_35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32065: AddedToken(\"<extra_id_34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32066: AddedToken(\"<extra_id_33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32067: AddedToken(\"<extra_id_32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32068: AddedToken(\"<extra_id_31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32069: AddedToken(\"<extra_id_30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32070: AddedToken(\"<extra_id_29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32071: AddedToken(\"<extra_id_28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32072: AddedToken(\"<extra_id_27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32073: AddedToken(\"<extra_id_26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32074: AddedToken(\"<extra_id_25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32075: AddedToken(\"<extra_id_24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32076: AddedToken(\"<extra_id_23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32077: AddedToken(\"<extra_id_22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32078: AddedToken(\"<extra_id_21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32079: AddedToken(\"<extra_id_20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32080: AddedToken(\"<extra_id_19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32081: AddedToken(\"<extra_id_18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32082: AddedToken(\"<extra_id_17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32083: AddedToken(\"<extra_id_16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32084: AddedToken(\"<extra_id_15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32085: AddedToken(\"<extra_id_14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32086: AddedToken(\"<extra_id_13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32087: AddedToken(\"<extra_id_12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32088: AddedToken(\"<extra_id_11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32089: AddedToken(\"<extra_id_10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32090: AddedToken(\"<extra_id_9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32091: AddedToken(\"<extra_id_8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32092: AddedToken(\"<extra_id_7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32093: AddedToken(\"<extra_id_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32094: AddedToken(\"<extra_id_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32095: AddedToken(\"<extra_id_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32096: AddedToken(\"<extra_id_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32097: AddedToken(\"<extra_id_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32098: AddedToken(\"<extra_id_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32099: AddedToken(\"<extra_id_0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32100: AddedToken(\"[EXP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : 32101\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : </s>\n",
            "self.eos_token_id : 1\n",
            "self.pad_token : <pad>\n",
            "self.pad_token_id : 0\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : 32100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: lightning_logs/2024-01-15 19:10/t5/small/batch_size:1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights.shape : torch.Size([32101, 512])\n",
            "---------- model information ----------\n",
            "self.MODEL_NAME : t5-small\n",
            "checkpoint_callback : <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7a64a50b0970>\n",
            "tokenizer : AutoTokenizer.from_pretrained(t5-small)\n",
            "Before adding additional_special_tokens\n",
            "['</s>', '<unk>', '<pad>', '<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']\n",
            "[1, 2, 0, 32099, 32098, 32097, 32096, 32095, 32094, 32093, 32092, 32091, 32090, 32089, 32088, 32087, 32086, 32085, 32084, 32083, 32082, 32081, 32080, 32079, 32078, 32077, 32076, 32075, 32074, 32073, 32072, 32071, 32070, 32069, 32068, 32067, 32066, 32065, 32064, 32063, 32062, 32061, 32060, 32059, 32058, 32057, 32056, 32055, 32054, 32053, 32052, 32051, 32050, 32049, 32048, 32047, 32046, 32045, 32044, 32043, 32042, 32041, 32040, 32039, 32038, 32037, 32036, 32035, 32034, 32033, 32032, 32031, 32030, 32029, 32028, 32027, 32026, 32025, 32024, 32023, 32022, 32021, 32020, 32019, 32018, 32017, 32016, 32015, 32014, 32013, 32012, 32011, 32010, 32009, 32008, 32007, 32006, 32005, 32004, 32003, 32002, 32001, 32000]\n",
            "After adding additional_special_tokens\n",
            "['</s>', '<unk>', '<pad>', '[EXP]']\n",
            "[1, 2, 0, 32100]\n",
            "exp_id : 32100\n",
            "eos_id : 1\n",
            "pad_id : 0\n",
            "len(tokenizer) : 32101\n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : t5-small\n",
            "self.tokenizer : T5TokenizerFast(name_or_path='t5-small', vocab_size=32100, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['[EXP]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32000: AddedToken(\"<extra_id_99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32001: AddedToken(\"<extra_id_98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32002: AddedToken(\"<extra_id_97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32003: AddedToken(\"<extra_id_96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32004: AddedToken(\"<extra_id_95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32005: AddedToken(\"<extra_id_94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32006: AddedToken(\"<extra_id_93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32007: AddedToken(\"<extra_id_92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32008: AddedToken(\"<extra_id_91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32009: AddedToken(\"<extra_id_90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32010: AddedToken(\"<extra_id_89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32011: AddedToken(\"<extra_id_88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32012: AddedToken(\"<extra_id_87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32013: AddedToken(\"<extra_id_86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32014: AddedToken(\"<extra_id_85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32015: AddedToken(\"<extra_id_84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32016: AddedToken(\"<extra_id_83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32017: AddedToken(\"<extra_id_82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32018: AddedToken(\"<extra_id_81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32019: AddedToken(\"<extra_id_80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32020: AddedToken(\"<extra_id_79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32021: AddedToken(\"<extra_id_78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32022: AddedToken(\"<extra_id_77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32023: AddedToken(\"<extra_id_76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32024: AddedToken(\"<extra_id_75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32025: AddedToken(\"<extra_id_74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32026: AddedToken(\"<extra_id_73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32027: AddedToken(\"<extra_id_72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32028: AddedToken(\"<extra_id_71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32029: AddedToken(\"<extra_id_70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32030: AddedToken(\"<extra_id_69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32031: AddedToken(\"<extra_id_68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32032: AddedToken(\"<extra_id_67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32033: AddedToken(\"<extra_id_66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32034: AddedToken(\"<extra_id_65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32035: AddedToken(\"<extra_id_64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32036: AddedToken(\"<extra_id_63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32037: AddedToken(\"<extra_id_62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32038: AddedToken(\"<extra_id_61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32039: AddedToken(\"<extra_id_60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32040: AddedToken(\"<extra_id_59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32041: AddedToken(\"<extra_id_58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32042: AddedToken(\"<extra_id_57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32043: AddedToken(\"<extra_id_56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32044: AddedToken(\"<extra_id_55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32045: AddedToken(\"<extra_id_54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32046: AddedToken(\"<extra_id_53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32047: AddedToken(\"<extra_id_52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32048: AddedToken(\"<extra_id_51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32049: AddedToken(\"<extra_id_50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32050: AddedToken(\"<extra_id_49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32051: AddedToken(\"<extra_id_48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32052: AddedToken(\"<extra_id_47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32053: AddedToken(\"<extra_id_46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32054: AddedToken(\"<extra_id_45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32055: AddedToken(\"<extra_id_44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32056: AddedToken(\"<extra_id_43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32057: AddedToken(\"<extra_id_42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32058: AddedToken(\"<extra_id_41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32059: AddedToken(\"<extra_id_40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32060: AddedToken(\"<extra_id_39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32061: AddedToken(\"<extra_id_38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32062: AddedToken(\"<extra_id_37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32063: AddedToken(\"<extra_id_36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32064: AddedToken(\"<extra_id_35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32065: AddedToken(\"<extra_id_34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32066: AddedToken(\"<extra_id_33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32067: AddedToken(\"<extra_id_32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32068: AddedToken(\"<extra_id_31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32069: AddedToken(\"<extra_id_30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32070: AddedToken(\"<extra_id_29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32071: AddedToken(\"<extra_id_28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32072: AddedToken(\"<extra_id_27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32073: AddedToken(\"<extra_id_26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32074: AddedToken(\"<extra_id_25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32075: AddedToken(\"<extra_id_24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32076: AddedToken(\"<extra_id_23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32077: AddedToken(\"<extra_id_22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32078: AddedToken(\"<extra_id_21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32079: AddedToken(\"<extra_id_20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32080: AddedToken(\"<extra_id_19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32081: AddedToken(\"<extra_id_18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32082: AddedToken(\"<extra_id_17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32083: AddedToken(\"<extra_id_16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32084: AddedToken(\"<extra_id_15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32085: AddedToken(\"<extra_id_14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32086: AddedToken(\"<extra_id_13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32087: AddedToken(\"<extra_id_12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32088: AddedToken(\"<extra_id_11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32089: AddedToken(\"<extra_id_10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32090: AddedToken(\"<extra_id_9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32091: AddedToken(\"<extra_id_8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32092: AddedToken(\"<extra_id_7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32093: AddedToken(\"<extra_id_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32094: AddedToken(\"<extra_id_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32095: AddedToken(\"<extra_id_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32096: AddedToken(\"<extra_id_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32097: AddedToken(\"<extra_id_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32098: AddedToken(\"<extra_id_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32099: AddedToken(\"<extra_id_0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32100: AddedToken(\"[EXP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : 32101\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : </s>\n",
            "self.eos_token_id : 1\n",
            "self.pad_token : <pad>\n",
            "self.pad_token_id : 0\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : 32100\n",
            "train_dataset size : 69\n",
            "\n",
            "tokenizer : AutoTokenizer.from_pretrained(t5-small)\n",
            "Before adding additional_special_tokens\n",
            "['</s>', '<unk>', '<pad>', '<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']\n",
            "[1, 2, 0, 32099, 32098, 32097, 32096, 32095, 32094, 32093, 32092, 32091, 32090, 32089, 32088, 32087, 32086, 32085, 32084, 32083, 32082, 32081, 32080, 32079, 32078, 32077, 32076, 32075, 32074, 32073, 32072, 32071, 32070, 32069, 32068, 32067, 32066, 32065, 32064, 32063, 32062, 32061, 32060, 32059, 32058, 32057, 32056, 32055, 32054, 32053, 32052, 32051, 32050, 32049, 32048, 32047, 32046, 32045, 32044, 32043, 32042, 32041, 32040, 32039, 32038, 32037, 32036, 32035, 32034, 32033, 32032, 32031, 32030, 32029, 32028, 32027, 32026, 32025, 32024, 32023, 32022, 32021, 32020, 32019, 32018, 32017, 32016, 32015, 32014, 32013, 32012, 32011, 32010, 32009, 32008, 32007, 32006, 32005, 32004, 32003, 32002, 32001, 32000]\n",
            "After adding additional_special_tokens\n",
            "['</s>', '<unk>', '<pad>', '[EXP]']\n",
            "[1, 2, 0, 32100]\n",
            "exp_id : 32100\n",
            "eos_id : 1\n",
            "pad_id : 0\n",
            "len(tokenizer) : 32101\n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : t5-small\n",
            "self.tokenizer : T5TokenizerFast(name_or_path='t5-small', vocab_size=32100, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['[EXP]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32000: AddedToken(\"<extra_id_99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32001: AddedToken(\"<extra_id_98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32002: AddedToken(\"<extra_id_97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32003: AddedToken(\"<extra_id_96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32004: AddedToken(\"<extra_id_95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32005: AddedToken(\"<extra_id_94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32006: AddedToken(\"<extra_id_93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32007: AddedToken(\"<extra_id_92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32008: AddedToken(\"<extra_id_91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32009: AddedToken(\"<extra_id_90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32010: AddedToken(\"<extra_id_89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32011: AddedToken(\"<extra_id_88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32012: AddedToken(\"<extra_id_87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32013: AddedToken(\"<extra_id_86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32014: AddedToken(\"<extra_id_85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32015: AddedToken(\"<extra_id_84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32016: AddedToken(\"<extra_id_83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32017: AddedToken(\"<extra_id_82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32018: AddedToken(\"<extra_id_81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32019: AddedToken(\"<extra_id_80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32020: AddedToken(\"<extra_id_79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32021: AddedToken(\"<extra_id_78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32022: AddedToken(\"<extra_id_77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32023: AddedToken(\"<extra_id_76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32024: AddedToken(\"<extra_id_75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32025: AddedToken(\"<extra_id_74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32026: AddedToken(\"<extra_id_73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32027: AddedToken(\"<extra_id_72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32028: AddedToken(\"<extra_id_71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32029: AddedToken(\"<extra_id_70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32030: AddedToken(\"<extra_id_69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32031: AddedToken(\"<extra_id_68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32032: AddedToken(\"<extra_id_67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32033: AddedToken(\"<extra_id_66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32034: AddedToken(\"<extra_id_65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32035: AddedToken(\"<extra_id_64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32036: AddedToken(\"<extra_id_63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32037: AddedToken(\"<extra_id_62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32038: AddedToken(\"<extra_id_61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32039: AddedToken(\"<extra_id_60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32040: AddedToken(\"<extra_id_59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32041: AddedToken(\"<extra_id_58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32042: AddedToken(\"<extra_id_57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32043: AddedToken(\"<extra_id_56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32044: AddedToken(\"<extra_id_55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32045: AddedToken(\"<extra_id_54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32046: AddedToken(\"<extra_id_53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32047: AddedToken(\"<extra_id_52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32048: AddedToken(\"<extra_id_51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32049: AddedToken(\"<extra_id_50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32050: AddedToken(\"<extra_id_49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32051: AddedToken(\"<extra_id_48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32052: AddedToken(\"<extra_id_47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32053: AddedToken(\"<extra_id_46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32054: AddedToken(\"<extra_id_45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32055: AddedToken(\"<extra_id_44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32056: AddedToken(\"<extra_id_43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32057: AddedToken(\"<extra_id_42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32058: AddedToken(\"<extra_id_41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32059: AddedToken(\"<extra_id_40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32060: AddedToken(\"<extra_id_39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32061: AddedToken(\"<extra_id_38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32062: AddedToken(\"<extra_id_37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32063: AddedToken(\"<extra_id_36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32064: AddedToken(\"<extra_id_35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32065: AddedToken(\"<extra_id_34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32066: AddedToken(\"<extra_id_33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32067: AddedToken(\"<extra_id_32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32068: AddedToken(\"<extra_id_31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32069: AddedToken(\"<extra_id_30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32070: AddedToken(\"<extra_id_29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32071: AddedToken(\"<extra_id_28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32072: AddedToken(\"<extra_id_27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32073: AddedToken(\"<extra_id_26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32074: AddedToken(\"<extra_id_25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32075: AddedToken(\"<extra_id_24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32076: AddedToken(\"<extra_id_23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32077: AddedToken(\"<extra_id_22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32078: AddedToken(\"<extra_id_21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32079: AddedToken(\"<extra_id_20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32080: AddedToken(\"<extra_id_19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32081: AddedToken(\"<extra_id_18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32082: AddedToken(\"<extra_id_17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32083: AddedToken(\"<extra_id_16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32084: AddedToken(\"<extra_id_15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32085: AddedToken(\"<extra_id_14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32086: AddedToken(\"<extra_id_13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32087: AddedToken(\"<extra_id_12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32088: AddedToken(\"<extra_id_11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32089: AddedToken(\"<extra_id_10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32090: AddedToken(\"<extra_id_9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32091: AddedToken(\"<extra_id_8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32092: AddedToken(\"<extra_id_7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32093: AddedToken(\"<extra_id_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32094: AddedToken(\"<extra_id_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32095: AddedToken(\"<extra_id_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32096: AddedToken(\"<extra_id_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32097: AddedToken(\"<extra_id_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32098: AddedToken(\"<extra_id_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32099: AddedToken(\"<extra_id_0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32100: AddedToken(\"[EXP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : 32101\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : </s>\n",
            "self.eos_token_id : 1\n",
            "self.pad_token : <pad>\n",
            "self.pad_token_id : 0\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : 32100\n",
            "test_dataset size : 23\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name      | Type                       | Params\n",
            "---------------------------------------------------------\n",
            "0 | model     | T5ForConditionalGeneration | 60.5 M\n",
            "1 | softmax   | Softmax                    | 0     \n",
            "2 | criterion | CrossEntropyLoss           | 0     \n",
            "---------------------------------------------------------\n",
            "60.5 M    Trainable params\n",
            "0         Non-trainable params\n",
            "60.5 M    Total params\n",
            "241.971   Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer : AutoTokenizer.from_pretrained(t5-small)\n",
            "Before adding additional_special_tokens\n",
            "['</s>', '<unk>', '<pad>', '<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']\n",
            "[1, 2, 0, 32099, 32098, 32097, 32096, 32095, 32094, 32093, 32092, 32091, 32090, 32089, 32088, 32087, 32086, 32085, 32084, 32083, 32082, 32081, 32080, 32079, 32078, 32077, 32076, 32075, 32074, 32073, 32072, 32071, 32070, 32069, 32068, 32067, 32066, 32065, 32064, 32063, 32062, 32061, 32060, 32059, 32058, 32057, 32056, 32055, 32054, 32053, 32052, 32051, 32050, 32049, 32048, 32047, 32046, 32045, 32044, 32043, 32042, 32041, 32040, 32039, 32038, 32037, 32036, 32035, 32034, 32033, 32032, 32031, 32030, 32029, 32028, 32027, 32026, 32025, 32024, 32023, 32022, 32021, 32020, 32019, 32018, 32017, 32016, 32015, 32014, 32013, 32012, 32011, 32010, 32009, 32008, 32007, 32006, 32005, 32004, 32003, 32002, 32001, 32000]\n",
            "After adding additional_special_tokens\n",
            "['</s>', '<unk>', '<pad>', '[EXP]']\n",
            "[1, 2, 0, 32100]\n",
            "exp_id : 32100\n",
            "eos_id : 1\n",
            "pad_id : 0\n",
            "len(tokenizer) : 32101\n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : t5-small\n",
            "self.tokenizer : T5TokenizerFast(name_or_path='t5-small', vocab_size=32100, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['[EXP]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32000: AddedToken(\"<extra_id_99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32001: AddedToken(\"<extra_id_98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32002: AddedToken(\"<extra_id_97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32003: AddedToken(\"<extra_id_96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32004: AddedToken(\"<extra_id_95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32005: AddedToken(\"<extra_id_94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32006: AddedToken(\"<extra_id_93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32007: AddedToken(\"<extra_id_92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32008: AddedToken(\"<extra_id_91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32009: AddedToken(\"<extra_id_90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32010: AddedToken(\"<extra_id_89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32011: AddedToken(\"<extra_id_88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32012: AddedToken(\"<extra_id_87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32013: AddedToken(\"<extra_id_86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32014: AddedToken(\"<extra_id_85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32015: AddedToken(\"<extra_id_84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32016: AddedToken(\"<extra_id_83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32017: AddedToken(\"<extra_id_82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32018: AddedToken(\"<extra_id_81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32019: AddedToken(\"<extra_id_80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32020: AddedToken(\"<extra_id_79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32021: AddedToken(\"<extra_id_78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32022: AddedToken(\"<extra_id_77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32023: AddedToken(\"<extra_id_76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32024: AddedToken(\"<extra_id_75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32025: AddedToken(\"<extra_id_74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32026: AddedToken(\"<extra_id_73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32027: AddedToken(\"<extra_id_72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32028: AddedToken(\"<extra_id_71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32029: AddedToken(\"<extra_id_70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32030: AddedToken(\"<extra_id_69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32031: AddedToken(\"<extra_id_68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32032: AddedToken(\"<extra_id_67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32033: AddedToken(\"<extra_id_66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32034: AddedToken(\"<extra_id_65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32035: AddedToken(\"<extra_id_64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32036: AddedToken(\"<extra_id_63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32037: AddedToken(\"<extra_id_62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32038: AddedToken(\"<extra_id_61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32039: AddedToken(\"<extra_id_60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32040: AddedToken(\"<extra_id_59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32041: AddedToken(\"<extra_id_58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32042: AddedToken(\"<extra_id_57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32043: AddedToken(\"<extra_id_56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32044: AddedToken(\"<extra_id_55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32045: AddedToken(\"<extra_id_54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32046: AddedToken(\"<extra_id_53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32047: AddedToken(\"<extra_id_52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32048: AddedToken(\"<extra_id_51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32049: AddedToken(\"<extra_id_50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32050: AddedToken(\"<extra_id_49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32051: AddedToken(\"<extra_id_48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32052: AddedToken(\"<extra_id_47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32053: AddedToken(\"<extra_id_46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32054: AddedToken(\"<extra_id_45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32055: AddedToken(\"<extra_id_44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32056: AddedToken(\"<extra_id_43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32057: AddedToken(\"<extra_id_42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32058: AddedToken(\"<extra_id_41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32059: AddedToken(\"<extra_id_40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32060: AddedToken(\"<extra_id_39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32061: AddedToken(\"<extra_id_38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32062: AddedToken(\"<extra_id_37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32063: AddedToken(\"<extra_id_36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32064: AddedToken(\"<extra_id_35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32065: AddedToken(\"<extra_id_34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32066: AddedToken(\"<extra_id_33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32067: AddedToken(\"<extra_id_32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32068: AddedToken(\"<extra_id_31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32069: AddedToken(\"<extra_id_30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32070: AddedToken(\"<extra_id_29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32071: AddedToken(\"<extra_id_28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32072: AddedToken(\"<extra_id_27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32073: AddedToken(\"<extra_id_26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32074: AddedToken(\"<extra_id_25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32075: AddedToken(\"<extra_id_24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32076: AddedToken(\"<extra_id_23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32077: AddedToken(\"<extra_id_22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32078: AddedToken(\"<extra_id_21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32079: AddedToken(\"<extra_id_20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32080: AddedToken(\"<extra_id_19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32081: AddedToken(\"<extra_id_18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32082: AddedToken(\"<extra_id_17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32083: AddedToken(\"<extra_id_16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32084: AddedToken(\"<extra_id_15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32085: AddedToken(\"<extra_id_14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32086: AddedToken(\"<extra_id_13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32087: AddedToken(\"<extra_id_12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32088: AddedToken(\"<extra_id_11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32089: AddedToken(\"<extra_id_10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32090: AddedToken(\"<extra_id_9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32091: AddedToken(\"<extra_id_8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32092: AddedToken(\"<extra_id_7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32093: AddedToken(\"<extra_id_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32094: AddedToken(\"<extra_id_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32095: AddedToken(\"<extra_id_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32096: AddedToken(\"<extra_id_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32097: AddedToken(\"<extra_id_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32098: AddedToken(\"<extra_id_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32099: AddedToken(\"<extra_id_0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32100: AddedToken(\"[EXP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : 32101\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : </s>\n",
            "self.eos_token_id : 1\n",
            "self.pad_token : <pad>\n",
            "self.pad_token_id : 0\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : 32100\n",
            "val_dataset size : 20\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5395d1c6d2e4d0bbfe2f730d5ba5903"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels : tensor([[16836,    10, 10403,  ...,     0,     0,     0],\n",
            "        [16836,    10,   461,  ...,     0,     0,     0],\n",
            "        [16836,    10, 10403,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [16836,    10, 25230,  ...,     0,     0,     0],\n",
            "        [16836,    10,   101,  ...,     0,     0,     0],\n",
            "        [16836,    10, 25290,  ...,     0,     0,     0]], device='cuda:0')\n",
            "len(labels) : 16\n",
            "tensor([152, 144, 228, 203, 166, 238, 109, 158, 148, 181, 163, 190, 106, 172,\n",
            "        136, 126], device='cuda:0')\n",
            "batch_max_length : 238\n",
            "input_ids1 before reshaping : torch.Size([16, 512])\n",
            "labels before reshaping : torch.Size([16, 512])\n",
            "attention_mask1 before reshaping : torch.Size([16, 512])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-443-1a88c12582c7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     train_and_test_CEG(\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mmodel_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt5_small_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtrain_and_test_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_and_test_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-438-4c77f7949008>\u001b[0m in \u001b[0;36mtrain_and_test_CEG\u001b[0;34m(model_config, train_and_test_config, batch_size, base_time)\u001b[0m\n\u001b[1;32m     38\u001b[0m         max_epochs=model_config['TRAINING_EPOCHS'])\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    545\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         )\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m             \u001b[0mval_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_sanity_check_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# run step hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         )\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstep_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_processed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_redirection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-417-e84cec6fe960>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# labels = batch[\"labels\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_text_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_prompt_text_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-417-e84cec6fe960>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids1, attention_mask1, input_ids2, attention_mask2, concat_text_length, full_prompt_text_length)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m#outputs = self.model(input_ids=input_ids1, attention_mask=attention_mask1, labels=labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1779\u001b[0m             \u001b[0;31m# move labels to correct device to enable PP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1781\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1782\u001b[0m             \u001b[0;31m# TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# t5-base\n",
        "for batch_size in [1, 2, 4, 8]:\n",
        "\n",
        "    train_and_test_CEG(\n",
        "        model_config=t5_base_config,\n",
        "        train_and_test_config=train_and_test_config,\n",
        "        batch_size=batch_size,\n",
        "        base_time=base_time)"
      ],
      "metadata": {
        "id": "a9ZSBLd60Rs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bart-base\n",
        "for batch_size in [1, 2, 4, 8]:\n",
        "\n",
        "    train_and_test_CEG(\n",
        "        model_config=bart_base_config,\n",
        "        train_and_test_config=train_and_test_config,\n",
        "        batch_size=batch_size,\n",
        "        base_time=base_time)"
      ],
      "metadata": {
        "id": "SVbTeLvm0Rwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('----------- training and testing has just started -----------')\n",
        "pl.seed_everything(train_and_test_config['RANDOM_SEED'])\n",
        "\n",
        "model_num = len(train_and_test_config['MODEL_NAME'])\n",
        "batch_num = len(train_and_test_config['BATCH_SIZE'])\n",
        "\n",
        "print(\"train_data_path : \" + train_and_test_config[\"TRAINING_DATA_PATH\"])\n",
        "print(\"test_data_path : \" + train_and_test_config[\"TEST_DATA_PATH\"])\n",
        "print(\"val_data_path : \" + train_and_test_config[\"VALIDATION_DATA_PATH\"])\n",
        "train_data = read_tsv(train_and_test_config[\"TRAINING_DATA_PATH\"])\n",
        "test_data = read_tsv(train_and_test_config[\"TEST_DATA_PATH\"])\n",
        "val_data = read_tsv(train_and_test_config[\"VALIDATION_DATA_PATH\"])\n",
        "\n",
        "for model_name in train_and_test_config['MODEL_NAME']:\n",
        "    for batch_size in train_and_test_config['BATCH_SIZE']:\n",
        "\n",
        "        # print(f'model name : {model_name},  batch_size : {batch_size}' )\n",
        "\n",
        "        model_config = Model_Config_Dict[model_name]\n",
        "        result_manager = Result_Manager()\n",
        "        model_config['TRAINING_BATCH_SIZE'] = batch_size\n",
        "        data_module =  LIAR_PLUS_DataModule_For_CEG(\n",
        "                        model_config,\n",
        "                        train_data,\n",
        "                        test_data,\n",
        "                        val_data\n",
        "                        )\n",
        "        total_training_steps, warmup_steps = calculate_warmup_steps(train_data, model_config[\"TRAINING_EPOCHS\"],\n",
        "                                                            model_config[\"TRAINING_BATCH_SIZE\"])\n",
        "        print(f\"total_training_steps : {total_training_steps} , warmup_steps : {warmup_steps} \")\n",
        "        model = CEG(\n",
        "                config_dict=model_config,\n",
        "                n_warmup_steps=warmup_steps,\n",
        "                n_training_steps=total_training_steps,\n",
        "                n_classes=2,\n",
        "                result_manager=result_manager)\n",
        "\n",
        "        checkpoint_callback = build_checkpoint_callback(model_config)\n",
        "        logger = TensorBoardLogger(path_dict['TensorBoardLogger'], name=model_config['TensorBoardLogger_NAME'])\n",
        "        early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "        trainer = pl.Trainer(\n",
        "            logger=logger,\n",
        "            callbacks=[checkpoint_callback, early_stopping_callback],\n",
        "            max_epochs=model_config['TRAINING_EPOCHS'])\n",
        "\n",
        "        trainer.fit(model, datamodule=data_module)\n",
        "        trainer.test(datamodule=data_module)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zCGO8KOo4myK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load_and_save_pretrained_CEG.py"
      ],
      "metadata": {
        "id": "LncI05v0HhkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_save_pretrained_CEG(config_dict:dict, ckpt_name:str, file_type:str, base_path:str, model_name:str):\n",
        "\n",
        "    saved_model_path = f'./model_transformers/{file_type}'\n",
        "    model = CEG.load_from_checkpoint(config_dict['SAVED_MODEL_PATH']+f'/{model_name}')\n",
        "    model.model.save_pretrained(saved_model_path)\n",
        "\n",
        "    if model_name in ['gpt2', 'gpt2-medium']:\n",
        "        model_from_pretrained = GPT2LMHeadModel.from_pretrained(f'{base_path}{saved_model_path[1:]}')\n",
        "    elif model_name in ['t5-small','t5-base']:\n",
        "        model_from_pretrained = T5ForConditionalGeneration.from_pretrained(f'{base_path}{saved_model_path[1:]}')\n",
        "    elif model_name in ['facebook/bart-base']:\n",
        "        model_from_pretrained = BartForConditionalGeneration.from_pretrained(f'{base_path}{saved_model_path[1:]}')\n",
        "    else:\n",
        "        raise ValueError('Wrong Model Type')\n",
        "\n",
        "    print(f'pretrained model was saved in {saved_model_path}')\n",
        "\n",
        "    return model_from_pretrained"
      ],
      "metadata": {
        "id": "4s2E6URYH0uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# predict_CEG.py"
      ],
      "metadata": {
        "id": "M6eFhN6gYCIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model_from_pretrained, model_name:str, input:str):\n",
        "\n",
        "    full_text, generated_text = ''\n",
        "    generated_token_length = 0\n",
        "\n",
        "    if model_name in ['gpt2', 'gpt2-medium']:\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "        inputs = tokenizer([input], return_tensors=\"pt\")\n",
        "        input_length = len(inputs[\"input_ids\"][0])\n",
        "        outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=5,\n",
        "                    num_beams=4,\n",
        "                    num_return_sequences=1,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True,\n",
        "                    )\n",
        "\n",
        "    elif model_name in ['t5-small','t5-base']:\n",
        "        model_from_pretrained = T5ForConditionalGeneration.from_pretrained(f'{base_path}{saved_model_path[1:]}')\n",
        "    elif model_name in ['facebook/bart-base']:\n",
        "        model_from_pretrained = BartForConditionalGeneration.from_pretrained(f'{base_path}{saved_model_path[1:]}')\n",
        "    else:\n",
        "        raise ValueError('Wrong Model Type')\n",
        "\n",
        "\n",
        "    print(f\"full text : {full_text}\\n\")\n",
        "    print(f\"generated text: {generated}\\n\")\n",
        "    print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\\n\")\n",
        "\n",
        "    return full_text, generated_text, generated_text_length\n"
      ],
      "metadata": {
        "id": "qCQsJDlcYObg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 以降メモ"
      ],
      "metadata": {
        "id": "ljIKzJBCXzur"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIcmjm_ShvOG"
      },
      "source": [
        "# GPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebbsKHDktRwJ"
      },
      "source": [
        "## 推論"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9VQsfvyI7Gw"
      },
      "outputs": [],
      "source": [
        "## modelのロード\n",
        "my_model = CEG.load_from_checkpoint(gpt2_small_config['SAVED_MODEL_PATH']+'/QTag-epoch=20-val_loss=7.58.ckpt')\n",
        "my_model.model.save_pretrained('./model_transformers/gpt2_small')\n",
        "my_model_from_pretrained = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/gpt2_small')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUo_aStv4pZA"
      },
      "outputs": [],
      "source": [
        "my_model_from_pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAR1j9UE6y_W"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, AutoConfig, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/model_transformers/gpt2_small')\n",
        "\n",
        "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
        "input_length = len(inputs[\"input_ids\"][0])\n",
        "print(f\"inputs_len : {input_length}\")\n",
        "\n",
        "# greedy_search\n",
        "# outputs = model.generate(**inputs, max_new_tokens=15, return_dict_in_generate=True, output_scores=True)\n",
        "# beam_search\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=5,\n",
        "    num_beams=4,\n",
        "    num_return_sequences=1,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        ")\n",
        "print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "tokens_list = outputs.sequences[0]\n",
        "print(tokens_list)\n",
        "full_text = tokenizer.decode(tokens_list)\n",
        "generated = tokenizer.decode(generated_tokens[0])\n",
        "\n",
        "print(f\"full text : {full_text}\\n\")\n",
        "print(f\"generated text: {generated}\\n\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2_lpCRDuYqo"
      },
      "outputs": [],
      "source": [
        "## tokenzierの準備\n",
        "my_prepare_tokenizer = Prepare_Tokenizer(gpt2_small_config)\n",
        "my_prepare_tokenizer.get_tokenizer_info()\n",
        "my_tokenizer = my_prepare_tokenizer.get_tokenizer()\n",
        "my_prepare_tokenizer.get_tokenizer_info()\n",
        "\n",
        "# データの準備\n",
        "inputs = my_tokenizer([\"Statement: Building a wall on the U.S.-Mexico border will take literally years. Metadata: immigration rick-perry Governor Texas republican Radio interview[EXP]\"], return_tensors=\"pt\")\n",
        "input_length = len(inputs[\"input_ids\"][0])\n",
        "print(f\"inputs_len : {input_length}\")\n",
        "\n",
        "outputs = my_model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=5,\n",
        "    num_beams=4,\n",
        "    num_return_sequences=1,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        ")\n",
        "print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "tokens_list = outputs.sequences[0]\n",
        "print(tokens_list)\n",
        "full_text = my_tokenizer.decode(tokens_list)\n",
        "generated = my_tokenizer.decode(generated_tokens[0])\n",
        "\n",
        "print(f\"full text : {full_text}\\n\")\n",
        "print(f\"generated text: {generated}\\n\")\n",
        "print(f\"len(generated) : {len(my_tokenizer.tokenize(generated))}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9On7S6T12TbR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bai7yul1huXr"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, AutoConfig, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
        "input_length = len(inputs[\"input_ids\"][0])\n",
        "print(f\"inputs_len : {input_length}\")\n",
        "\n",
        "# greedy_search\n",
        "# outputs = model.generate(**inputs, max_new_tokens=15, return_dict_in_generate=True, output_scores=True)\n",
        "# beam_search\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=5,\n",
        "    num_beams=4,\n",
        "    num_return_sequences=1,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        ")\n",
        "print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "tokens_list = outputs.sequences[0]\n",
        "print(tokens_list)\n",
        "full_text = tokenizer.decode(tokens_list)\n",
        "generated = tokenizer.decode(generated_tokens[0])\n",
        "\n",
        "print(f\"full text : {full_text}\\n\")\n",
        "print(f\"generated text: {generated}\\n\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRkLjp0h73VM"
      },
      "source": [
        "# T5デモ\n",
        "- `<extra_id_0>`や`</s>`が出力に含まれてしまっている\n",
        "- lm_head.weightsを初期化してもあまり効果が無い\n",
        "- https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py\n",
        "- https://www.kaggle.com/code/kreeshrajani/fine-tune-t5-for-conversational-model\n",
        "- https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Tokenizer.sp_model_kwargs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CSsvMsOyROy"
      },
      "source": [
        "## 公式サイトより"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WZvyBATyUEI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# training\n",
        "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
        "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
        "outputs = model(input_ids=input_ids, labels=labels)\n",
        "loss = outputs.loss\n",
        "logits = outputs.logits\n",
        "\n",
        "# inference\n",
        "input_ids = tokenizer(\n",
        "    \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
        ").input_ids  # Batch size 1\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "# studies have shown that owning a dog is good for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncxz-bFVuQmS"
      },
      "source": [
        "## fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "id": "F2IFfkHWg5jg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05177341-3a13-4ab9-9705-42012a744a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before\n",
            "['</s>', '<unk>', '<pad>', '<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']\n",
            "[1, 2, 0, 32099, 32098, 32097, 32096, 32095, 32094, 32093, 32092, 32091, 32090, 32089, 32088, 32087, 32086, 32085, 32084, 32083, 32082, 32081, 32080, 32079, 32078, 32077, 32076, 32075, 32074, 32073, 32072, 32071, 32070, 32069, 32068, 32067, 32066, 32065, 32064, 32063, 32062, 32061, 32060, 32059, 32058, 32057, 32056, 32055, 32054, 32053, 32052, 32051, 32050, 32049, 32048, 32047, 32046, 32045, 32044, 32043, 32042, 32041, 32040, 32039, 32038, 32037, 32036, 32035, 32034, 32033, 32032, 32031, 32030, 32029, 32028, 32027, 32026, 32025, 32024, 32023, 32022, 32021, 32020, 32019, 32018, 32017, 32016, 32015, 32014, 32013, 32012, 32011, 32010, 32009, 32008, 32007, 32006, 32005, 32004, 32003, 32002, 32001, 32000]\n",
            "After\n",
            "['</s>', '<unk>', '<pad>', '[EXP]']\n",
            "[1, 2, 0, 32100]\n",
            "exp_id : 32100\n",
            "eos_id : 1\n",
            "pad_id : 0\n",
            "\n",
            "len(tokenizer) : 32101\n",
            "before : torch.Size([32128, 512])\n",
            "before resizing : Parameter containing:\n",
            "tensor([[ -2.0156,   0.2236,  -7.0938,  ...,  -0.3535,   2.6406,  -2.8906],\n",
            "        [ 12.6250,   8.1875, -11.6250,  ...,   7.9375,  -7.3125,   0.9453],\n",
            "        [ -8.7500,   7.1875,  27.8750,  ..., -26.7500,   0.8555,  -1.5156],\n",
            "        ...,\n",
            "        [-25.2500, -28.5000, -17.2500,  ..., -17.7500,  -5.2500,  27.3750],\n",
            "        [-25.5000, -29.3750, -18.2500,  ..., -17.7500,  -4.8125,  27.7500],\n",
            "        [-26.7500, -28.3750, -17.8750,  ..., -18.5000,  -7.0000,  27.6250]],\n",
            "       requires_grad=True)\n",
            "after :  torch.Size([32101, 512])\n",
            "after resizing : torch.Size([32101, 512])\n",
            "before changing weights : torch.Size([32101, 512])\n",
            "focus : tensor([[ -2.0156,   0.2236,  -7.0938,  ...,  -0.3535,   2.6406,  -2.8906],\n",
            "        [ 12.6250,   8.1875, -11.6250,  ...,   7.9375,  -7.3125,   0.9453],\n",
            "        [ -8.7500,   7.1875,  27.8750,  ..., -26.7500,   0.8555,  -1.5156],\n",
            "        ...,\n",
            "        [-13.1875,   5.2188, -18.0000,  ...,  14.0625,  19.3750,  -0.2422],\n",
            "        [-15.4375,   8.7500,   6.9688,  ...,   6.7500,   4.2812,  -0.4199],\n",
            "        [  7.8438,   8.5625,  -4.3750,  ...,   0.6719,  -2.4688,  -4.1562]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "focus.shape : torch.Size([32100, 512])\n",
            "after changing weights : Parameter containing:\n",
            "tensor([[-2.0156e+00,  2.2363e-01, -7.0938e+00,  ..., -3.5352e-01,\n",
            "          2.6406e+00, -2.8906e+00],\n",
            "        [ 1.2625e+01,  8.1875e+00, -1.1625e+01,  ...,  7.9375e+00,\n",
            "         -7.3125e+00,  9.4531e-01],\n",
            "        [-8.7500e+00,  7.1875e+00,  2.7875e+01,  ..., -2.6750e+01,\n",
            "          8.5547e-01, -1.5156e+00],\n",
            "        ...,\n",
            "        [-1.5438e+01,  8.7500e+00,  6.9688e+00,  ...,  6.7500e+00,\n",
            "          4.2812e+00, -4.1992e-01],\n",
            "        [ 7.8438e+00,  8.5625e+00, -4.3750e+00,  ...,  6.7188e-01,\n",
            "         -2.4688e+00, -4.1562e+00],\n",
            "        [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
            "         -1.0000e+04, -1.0000e+04]], requires_grad=True)\n",
            "new_weights.shape : torch.Size([32101, 512])\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "\n",
        "print(\"Before\")\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "special_tokens_dict = {\n",
        "    'additional_special_tokens': ['[EXP]']\n",
        "    }\n",
        "\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "exp_id = tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "eos_id = tokenizer.convert_tokens_to_ids('</s>')\n",
        "pad_id = tokenizer.convert_tokens_to_ids('<pad>')\n",
        "print(\"After\")\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "print(f\"exp_id : {exp_id}\")\n",
        "print(f\"eos_id : {eos_id}\")\n",
        "print(f\"pad_id : {pad_id}\\n\")\n",
        "print(f\"len(tokenizer) : {len(tokenizer)}\")\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "print(f\"before : {model.lm_head.weight.shape}\")\n",
        "print(f\"before resizing : {model.lm_head.weight}\")\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(\"after : \", model.lm_head.weight.shape)\n",
        "print(f\"after resizing : {model.lm_head.weight.shape}\")\n",
        "print(f\"before changing weights : {model.lm_head.weight.shape}\")\n",
        "print(f\"focus : {model.lm_head.weight[:-1, :]}\")\n",
        "print(f\"focus.shape : {model.lm_head.weight[:-1, :].shape}\")\n",
        "new_weights = torch.cat([model.lm_head.weight[:-1, :], torch.zeros(1, model.lm_head.weight.shape[1]) -10000])\n",
        "model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "print(f\"after changing weights : {model.lm_head.weight}\")\n",
        "print(f\"new_weights.shape : {model.lm_head.weight.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "4eFQyzQEhVTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21344255-27f1-4bb4-d03b-aa38d92dcb96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized input_text : ['▁Statement', ':', '▁When', '▁did', '▁the', '▁decline', '▁of', '▁coal', '▁start', '?', '▁It', '▁started', '▁when', '▁natural', '▁gas', '▁took', '▁off', '▁that', '▁started', '▁to', '▁begin', '▁in', '▁(', 'P', 'resident', '▁George', '▁W', '.', ')', '▁Bush', 's', '▁administration', '.', '▁Meta', 'data', ':', '▁energy', ',', 'his', 'tory', ',', 'job', '-', 'acco', 'mp', 'l', 'ish', 'ments', '▁', 's', 'cott', '-', 'sur', 'o', 've', 'll', '▁State', '▁de', 'legate', '▁Virginia', '▁', 'democrat', '▁', 'a', '▁floor', '▁speech', '.', '[EXP]']\n",
            "\n",
            "tokenized label_text : ['▁Statement', ':', '▁When', '▁did', '▁the', '▁decline', '▁of', '▁coal', '▁start', '?', '▁It', '▁started', '▁when', '▁natural', '▁gas', '▁took', '▁off', '▁that', '▁started', '▁to', '▁begin', '▁in', '▁(', 'P', 'resident', '▁George', '▁W', '.', ')', '▁Bush', 's', '▁administration', '.', '▁Meta', 'data', ':', '▁energy', ',', 'his', 'tory', ',', 'job', '-', 'acco', 'mp', 'l', 'ish', 'ments', '▁', 's', 'cott', '-', 'sur', 'o', 've', 'll', '▁State', '▁de', 'legate', '▁Virginia', '▁', 'democrat', '▁', 'a', '▁floor', '▁speech', '.', '[EXP]', '▁Sur', 'o', 've', 'll', '▁said', '▁the', '▁decline', '▁of', '▁coal', '▁\"', 'star', 'ted', '▁when', '▁natural', '▁gas', '▁took', '▁off', '▁That', '▁started', '▁to', '▁begin', '▁in', '▁President', '▁(', 'George', '▁W', '.', '▁', ')', '▁Bush', 's', '▁administration', '.', '▁\"', 'No', '▁doubt', ',', '▁natural', '▁gas', '▁has', '▁been', '▁', 'gaining', '▁ground', '▁on', '▁coal', '▁in', '▁', 'generating', '▁electricity', '.', '▁The', '▁trend', '▁started', '▁in', '▁the', '▁1990', 's', '▁but', '▁clearly', '▁gained', '▁speed', '▁during', '▁the', '▁Bush', '▁administration', '▁when', '▁the', '▁production', '▁of', '▁natural', '▁gas', '▁--', '▁', 'a', '▁competitor', '▁of', '▁coal', '▁--', '▁picked', '▁up', '.', '▁But', '▁analysts', '▁give', '▁little', '▁credit', '▁or', '▁blame', '▁to', '▁Bush', '▁for', '▁that', '▁trend', '.', '▁They', '▁note', '▁that', '▁other', '▁factors', ',', '▁such', '▁as', '▁technological', '▁innovation', ',', '▁', 'entrepreneurship', '▁and', '▁policies', '▁of', '▁previous', '▁administration', 's', ',', '▁had', '▁more', '▁to', '▁do', '▁with', '▁', 'laying', '▁the', '▁ground', 'work', '▁for', '▁the', '▁natural', '▁gas', '▁boom', '.', '</s>']\n",
            "input_text_length : 68\n",
            "input_encoding : {'input_ids': tensor([[16836,    10,   366,   410,     8,  7198,    13,  8416,   456,    58,\n",
            "            94,   708,   116,   793,  1807,   808,   326,    24,   708,    12,\n",
            "          1731,    16,    41,   345, 15704,  3080,   549,     5,    61,  8905,\n",
            "             7,  3602,     5, 14204,  6757,    10,   827,     6, 10193, 10972,\n",
            "             6, 16899,    18, 21007,  1167,    40,  1273,  4128,     3,     7,\n",
            "         10405,    18,  3042,    32,   162,   195,  1015,    20,  8791,  5382,\n",
            "             3, 23319,     3,     9,  1501,  5023,     5, 32100,     1,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "\n",
            "label_encoding : {'input_ids': tensor([[16836,    10,   366,   410,     8,  7198,    13,  8416,   456,    58,\n",
            "            94,   708,   116,   793,  1807,   808,   326,    24,   708,    12,\n",
            "          1731,    16,    41,   345, 15704,  3080,   549,     5,    61,  8905,\n",
            "             7,  3602,     5, 14204,  6757,    10,   827,     6, 10193, 10972,\n",
            "             6, 16899,    18, 21007,  1167,    40,  1273,  4128,     3,     7,\n",
            "         10405,    18,  3042,    32,   162,   195,  1015,    20,  8791,  5382,\n",
            "             3, 23319,     3,     9,  1501,  5023,     5, 32100,  3705,    32,\n",
            "           162,   195,   243,     8,  7198,    13,  8416,    96,  3624,  1054,\n",
            "           116,   793,  1807,   808,   326,   466,   708,    12,  1731,    16,\n",
            "          1661,    41, 31317,   549,     5,     3,    61,  8905,     7,  3602,\n",
            "             5,    96,  4168,  3228,     6,   793,  1807,    65,   118,     3,\n",
            "         11866,  1591,    30,  8416,    16,     3, 11600,  6373,     5,    37,\n",
            "          4166,   708,    16,     8,  5541,     7,    68,  3133,  6886,  1634,\n",
            "           383,     8,  8905,  3602,   116,     8,   999,    13,   793,  1807,\n",
            "          1636,     3,     9, 18766,    13,  8416,  1636,  4758,    95,     5,\n",
            "           299, 15639,   428,   385,   998,    42,  9100,    12,  8905,    21,\n",
            "            24,  4166,     5,   328,  2232,    24,   119,  2580,     6,   224,\n",
            "            38,  9974,  4337,     6,     3, 24849,    11,  3101,    13,  1767,\n",
            "          3602,     7,     6,   141,    72,    12,   103,    28,     3, 14720,\n",
            "             8,  1591,  1981,    21,     8,   793,  1807, 13997,     5,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "input prompt : Statement: When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration. Metadata: energy,history,job-accomplishments scott-surovell State delegate Virginia democrat a floor speech.[EXP]</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "label : Statement: When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration. Metadata: energy,history,job-accomplishments scott-surovell State delegate Virginia democrat a floor speech.[EXP] Surovell said the decline of coal \"started when natural gas took off That started to begin in President (George W. ) Bushs administration. \"No doubt, natural gas has been gaining ground on coal in generating electricity. The trend started in the 1990s but clearly gained speed during the Bush administration when the production of natural gas -- a competitor of coal -- picked up. But analysts give little credit or blame to Bush for that trend. They note that other factors, such as technological innovation, entrepreneurship and policies of previous administrations, had more to do with laying the groundwork for the natural gas boom.</s>\n",
            "labels before -100 masking : tensor([[16836,    10,   366,   410,     8,  7198,    13,  8416,   456,    58,\n",
            "            94,   708,   116,   793,  1807,   808,   326,    24,   708,    12,\n",
            "          1731,    16,    41,   345, 15704,  3080,   549,     5,    61,  8905,\n",
            "             7,  3602,     5, 14204,  6757,    10,   827,     6, 10193, 10972,\n",
            "             6, 16899,    18, 21007,  1167,    40,  1273,  4128,     3,     7,\n",
            "         10405,    18,  3042,    32,   162,   195,  1015,    20,  8791,  5382,\n",
            "             3, 23319,     3,     9,  1501,  5023,     5, 32100,  3705,    32,\n",
            "           162,   195,   243,     8,  7198,    13,  8416,    96,  3624,  1054,\n",
            "           116,   793,  1807,   808,   326,   466,   708,    12,  1731,    16,\n",
            "          1661,    41, 31317,   549,     5,     3,    61,  8905,     7,  3602,\n",
            "             5,    96,  4168,  3228,     6,   793,  1807,    65,   118,     3,\n",
            "         11866,  1591,    30,  8416,    16,     3, 11600,  6373,     5,    37,\n",
            "          4166,   708,    16,     8,  5541,     7,    68,  3133,  6886,  1634,\n",
            "           383,     8,  8905,  3602,   116,     8,   999,    13,   793,  1807,\n",
            "          1636,     3,     9, 18766,    13,  8416,  1636,  4758,    95,     5,\n",
            "           299, 15639,   428,   385,   998,    42,  9100,    12,  8905,    21,\n",
            "            24,  4166,     5,   328,  2232,    24,   119,  2580,     6,   224,\n",
            "            38,  9974,  4337,     6,     3, 24849,    11,  3101,    13,  1767,\n",
            "          3602,     7,     6,   141,    72,    12,   103,    28,     3, 14720,\n",
            "             8,  1591,  1981,    21,     8,   793,  1807, 13997,     5,     1]])\n",
            "labels after -100 masking : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  3705,    32,\n",
            "           162,   195,   243,     8,  7198,    13,  8416,    96,  3624,  1054,\n",
            "           116,   793,  1807,   808,   326,   466,   708,    12,  1731,    16,\n",
            "          1661,    41, 31317,   549,     5,     3,    61,  8905,     7,  3602,\n",
            "             5,    96,  4168,  3228,     6,   793,  1807,    65,   118,     3,\n",
            "         11866,  1591,    30,  8416,    16,     3, 11600,  6373,     5,    37,\n",
            "          4166,   708,    16,     8,  5541,     7,    68,  3133,  6886,  1634,\n",
            "           383,     8,  8905,  3602,   116,     8,   999,    13,   793,  1807,\n",
            "          1636,     3,     9, 18766,    13,  8416,  1636,  4758,    95,     5,\n",
            "           299, 15639,   428,   385,   998,    42,  9100,    12,  8905,    21,\n",
            "            24,  4166,     5,   328,  2232,    24,   119,  2580,     6,   224,\n",
            "            38,  9974,  4337,     6,     3, 24849,    11,  3101,    13,  1767,\n",
            "          3602,     7,     6,   141,    72,    12,   103,    28,     3, 14720,\n",
            "             8,  1591,  1981,    21,     8,   793,  1807, 13997,     5,     1]])\n",
            "labels.shape : torch.Size([1, 200])\n"
          ]
        }
      ],
      "source": [
        "input_text = 'Statement: When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration. Metadata: energy,history,job-accomplishments scott-surovell State delegate Virginia democrat a floor speech.'+ tokenizer.convert_ids_to_tokens(exp_id)\n",
        "justification_text = 'Surovell said the decline of coal \"started when natural gas took off  That started to begin in President (George W. ) Bushs administration. \"No doubt, natural gas has been gaining ground on coal in generating electricity. The trend started in the 1990s but clearly gained speed during the Bush administration when the production of natural gas -- a competitor of coal -- picked up. But analysts give little credit or blame to Bush for that trend. They note that other factors, such as technological innovation, entrepreneurship and policies of previous administrations, had more to do with laying the groundwork for the natural gas boom.'\n",
        "label_text = input_text  + justification_text + tokenizer.convert_ids_to_tokens(eos_id)\n",
        "print(f\"tokenized input_text : {tokenizer.tokenize(input_text)}\\n\")\n",
        "print(f\"tokenized label_text : {tokenizer.tokenize(label_text)}\")\n",
        "input_text_length = len(tokenizer.tokenize(input_text))\n",
        "justification_text_length = len(tokenizer.tokenize(justification_text))\n",
        "label_text_length = len(tokenizer.tokenize(label_text))\n",
        "print(f\"input_text_length : {input_text_length}\")\n",
        "\n",
        "input_encoding = tokenizer.encode_plus(\n",
        "    input_text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=label_text_length,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "\n",
        "print(f\"input_encoding : {input_encoding}\\n\")\n",
        "decoded_input_encoding = tokenizer.decode(input_encoding[\"input_ids\"][0])\n",
        "# print(f\"decoded input_encoding : {decoded_input_encoding}\\n\")\n",
        "\n",
        "label_encoding = tokenizer.encode_plus(\n",
        "    label_text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=label_text_length,\n",
        "    return_token_type_ids=False,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "\n",
        "print(f\"label_encoding : {label_encoding}\")\n",
        "# print(f\"label_encoding['input_ids'][0] : {label_encoding['input_ids'][0]}\")\n",
        "decoded_label_encoding = tokenizer.decode(label_encoding[\"input_ids\"][0])\n",
        "# print(f\"decoded label_encoding : {decoded_label_encoding}\")\n",
        "print(f\"input prompt : {decoded_input_encoding}\")\n",
        "print(f\"label : {decoded_label_encoding}\")\n",
        "\n",
        "labels = label_encoding[\"input_ids\"]\n",
        "print(f\"labels before -100 masking : {labels}\")\n",
        "# decoded_labels = tokenizer.decode(labels)\n",
        "# print(f\"decoded labels : {decoded_labels}\")\n",
        "labels[:, :input_text_length] = -100 # cross_entropy_ignore_index\n",
        "labels[:, label_text_length:] = -100\n",
        "print(f\"labels after -100 masking : {labels}\")\n",
        "print(f\"labels.shape : {labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "id": "QYIrs850a-UJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf6349cd-5a36-4dd3-fabb-a162479d409e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model : T5ForConditionalGeneration(\n",
            "  (shared): Embedding(32101, 512)\n",
            "  (encoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32101, 512)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 8)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
            "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-5): 5 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
            "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (decoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32101, 512)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 8)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
            "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-5): 5 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
            "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
            "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=512, out_features=32101, bias=False)\n",
            ")\n",
            "type(model) : <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n",
            "type(outputs) : <class 'transformers.modeling_outputs.Seq2SeqLMOutput'>\n",
            "logits : tensor([[[-2.5476e+01, -1.2861e+01, -1.7625e+01,  ..., -1.3640e+01,\n",
            "          -3.2110e+00,  1.0486e+04],\n",
            "         [-2.5476e+01, -1.2861e+01, -1.7625e+01,  ..., -1.3640e+01,\n",
            "          -3.2110e+00,  1.0486e+04],\n",
            "         [-2.5476e+01, -1.2861e+01, -1.7625e+01,  ..., -1.3640e+01,\n",
            "          -3.2110e+00,  1.0486e+04],\n",
            "         ...,\n",
            "         [-5.4455e+01, -4.7042e+00, -2.2898e+01,  ..., -2.8213e+01,\n",
            "          -3.0636e+01,  1.3202e+04],\n",
            "         [-5.0904e+01, -5.3404e+00, -2.2956e+01,  ..., -2.7929e+01,\n",
            "          -2.8834e+01,  1.3415e+04],\n",
            "         [-4.8555e+01, -5.0367e+00, -2.2519e+01,  ..., -2.7969e+01,\n",
            "          -2.8701e+01,  1.3469e+04]]], grad_fn=<UnsafeViewBackward0>)\n",
            "logtis.size() : torch.Size([1, 200, 32101])\n",
            "loss : 6978.9580078125\n",
            "output : tensor([[[1.2420e-05, 1.1084e-06, 7.3320e-07,  ..., 2.0707e-04,\n",
            "          1.4493e-02, 0.0000e+00],\n",
            "         [1.2420e-05, 1.1084e-06, 7.3320e-07,  ..., 2.0707e-04,\n",
            "          1.4493e-02, 0.0000e+00],\n",
            "         [1.2420e-05, 1.1084e-06, 7.3320e-07,  ..., 2.0707e-04,\n",
            "          1.4493e-02, 0.0000e+00],\n",
            "         ...,\n",
            "         [3.2251e-18, 3.8631e-03, 3.7582e-09,  ..., 9.7075e-11,\n",
            "          1.7817e-14, 0.0000e+00],\n",
            "         [1.1244e-16, 2.0448e-03, 3.5469e-09,  ..., 1.2893e-10,\n",
            "          1.0790e-13, 0.0000e+00],\n",
            "         [1.1777e-15, 2.7704e-03, 5.4941e-09,  ..., 1.2380e-10,\n",
            "          1.2329e-13, 0.0000e+00]]], grad_fn=<SoftmaxBackward0>)\n",
            "output.size() : torch.Size([1, 200, 32101])\n",
            "argmax(ouput) : tensor([[32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 23591,\n",
            "         19828,   195,  1015,    10, 24409,    13,  8416,  3511, 12416,  1054,\n",
            "         23562,   410,  1807,   808,   326,   793,   708,    12,  1731,    16,\n",
            "            41,  3080,   345,   549,     5,    61, 12703,  8905,  7289, 10030,\n",
            "             5,  7243, 10555,  8052, 16919,  2199, 24436, 14428,   582,  8154,\n",
            "         31036, 15290, 15362,  6089,  6490, 15777, 27470,     1,  4471, 26958,\n",
            "          7512,  3256,   116, 22965, 15559, 17774,     1,  1703,     1, 27721,\n",
            "         13830, 20417,     1,  6863,     1, 10538, 27512,    13, 30390,  5556,\n",
            "         16646, 31942, 17560, 22587, 18955,     1, 10762, 28448,    95,     1,\n",
            "         30752, 17945, 28270, 26147,  2736,     1, 15620,     1,  1792,    31,\n",
            "             1,    58,     1, 26539,  9098,     1, 25562,  2580,     1,     1,\n",
            "           587,     1, 14500,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1, 32100,     1,     1,     1]])\n",
            "argmax(ouput).size() : torch.Size([1, 200])\n",
            "argmax(output).dtype() : torch.int64\n",
            "tensor([[32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099,\n",
            "         32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 32099, 23591,\n",
            "         19828,   195,  1015,    10, 24409,    13,  8416,  3511, 12416,  1054,\n",
            "         23562,   410,  1807,   808,   326,   793,   708,    12,  1731,    16,\n",
            "            41,  3080,   345,   549,     5,    61, 12703,  8905,  7289, 10030,\n",
            "             5,  7243, 10555,  8052, 16919,  2199, 24436, 14428,   582,  8154,\n",
            "         31036, 15290, 15362,  6089,  6490, 15777, 27470,     1,  4471, 26958,\n",
            "          7512,  3256,   116, 22965, 15559, 17774,     1,  1703,     1, 27721,\n",
            "         13830, 20417,     1,  6863,     1, 10538, 27512,    13, 30390,  5556,\n",
            "         16646, 31942, 17560, 22587, 18955,     1, 10762, 28448,    95,     1,\n",
            "         30752, 17945, 28270, 26147,  2736,     1, 15620,     1,  1792,    31,\n",
            "             1,    58,     1, 26539,  9098,     1, 25562,  2580,     1,     1,\n",
            "           587,     1, 14500,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1, 32100,     1,     1,     1]])\n",
            "generated : <extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0>posonsvettell State: hallway of coal startsdidted 1/2\" did gas took off natural started to begin in ( GeorgeP W.)7) Bushsburg regime.azăWhenthinglessly although gases flows become experiencingbooming momentum clearance behalf suggestssistedianu</s> suppliesinformatiileodor continues when Brig 1930′</s>cher</s> eyebrowively parliament</s> Administration</s> lemn déclin of zahărgas pumpscostingoyeznnouncing thereof</s> puts senzati up</s> investitii yeah speculate speeches detail</s>deal</s> avoid'</s>?</s> Haftung relate</s> pahar factors</s></s> wie</s> advancement</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>[EXP]</s></s></s>\n",
            "len(generated) : 203\n"
          ]
        }
      ],
      "source": [
        "outputs = model(input_ids=input_encoding[\"input_ids\"], attention_mask=input_encoding[\"attention_mask\"], labels=labels)\n",
        "\n",
        "print(f\"model : {model}\")\n",
        "print(f\"type(model) : {type(model)}\")\n",
        "print(f\"type(outputs) : {type(outputs)}\")\n",
        "\n",
        "logits = outputs.logits\n",
        "loss = outputs.loss\n",
        "print(f\"logits : {logits}\")\n",
        "print(f\"logtis.size() : {logits.size()}\")\n",
        "print(f\"loss : {loss}\")\n",
        "\n",
        "m = nn.Softmax(dim=1)\n",
        "output = m(logits)\n",
        "print(f\"output : {output}\")\n",
        "print(f\"output.size() : {output.size()}\")\n",
        "arg = torch.argmax(output, dim=2)\n",
        "print(f\"argmax(ouput) : {arg}\") # --> tensor\n",
        "print(f\"argmax(ouput).size() : {arg.size()}\")\n",
        "print(f\"argmax(output).dtype() : {arg.dtype}\")\n",
        "print(arg)\n",
        "generated = tokenizer.decode(arg[0])\n",
        "print(f\"generated : {generated}\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNWyFtg7uWzL"
      },
      "source": [
        "## 推論"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k20XUiAPuPB9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "inputs = tokenizer([\"summarize: Today is\"], return_tensors=\"pt\")\n",
        "input_length = len(inputs[\"input_ids\"][0])\n",
        "print(f\"inputs_len : {input_length}\")\n",
        "\n",
        "# greedy_search\n",
        "outputs = model.generate(**inputs, max_new_tokens=15, return_dict_in_generate=True, output_scores=True)\n",
        "# beam_search\n",
        "# outputs = model.generate(\n",
        "#     **inputs,\n",
        "#     max_new_tokens=5,\n",
        "#     num_beams=4,\n",
        "#     num_return_sequences=1,\n",
        "#     return_dict_in_generate=True,\n",
        "#     output_scores=True,\n",
        "# )\n",
        "print(f\"outputs.sequences : {outputs.sequences}\")\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "print(f\"generated_tokens : {generated_tokens}\")\n",
        "\n",
        "tokens_list = outputs.sequences[0]\n",
        "print(tokens_list)\n",
        "full_text = tokenizer.decode(tokens_list)\n",
        "generated = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"full text : {full_text}\\n\")\n",
        "print(f\"generated text: {generated}\\n\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoDbJrtWKA8k"
      },
      "source": [
        "# BART\n",
        "- add_special_token=Trueにしたとき、先頭に`<s>`が追加されるため-100のマスキング時にinput_length+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H9d_ni0KDdO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "print(\"Before\")\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "special_tokens_dict = {\n",
        "    'additional_special_tokens': ['[EXP]']\n",
        "    }\n",
        "\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "exp_id = tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "eos_id = tokenizer.convert_tokens_to_ids('</s>')\n",
        "pad_id = tokenizer.convert_tokens_to_ids('<pad>')\n",
        "print(\"After\")\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "print(f\"exp_id : {exp_id}\")\n",
        "print(f\"eos_id : {eos_id}\")\n",
        "print(f\"pad_id : {pad_id}\\n\")\n",
        "print(f\"len(tokenizer) : {len(tokenizer)}\")\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
        "# print(f\"before : {model.lm_head.weight.shape}\")\n",
        "# print(f\"before resizing : {model.lm_head.weight}\")\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# print(\"after : \", model.lm_head.weight.shape)\n",
        "# print(f\"after resizing : {model.lm_head.weight.shape}\")\n",
        "# print(f\"before changing weights : {model.lm_head.weight.shape}\")\n",
        "# print(f\"focus : {model.lm_head.weight[:-1, :]}\")\n",
        "# print(f\"focus.shape : {model.lm_head.weight[:-1, :].shape}\")\n",
        "# new_weights = torch.cat([model.lm_head.weight[:-1, :], torch.zeros(1, model.lm_head.weight.shape[1]) -10000])\n",
        "# model.lm_head.weight = torch.nn.Parameter(new_weights)\n",
        "# print(f\"after changing weights : {model.lm_head.weight}\")\n",
        "# print(f\"new_weights.shape : {model.lm_head.weight.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS-7CkYAKTI0"
      },
      "outputs": [],
      "source": [
        "input_text = 'Statement: When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration. Metadata: energy,history,job-accomplishments scott-surovell State delegate Virginia democrat a floor speech.'+ tokenizer.convert_ids_to_tokens(exp_id)\n",
        "justification_text = 'Surovell said the decline of coal \"started when natural gas took off  That started to begin in President (George W. ) Bushs administration. \"No doubt, natural gas has been gaining ground on coal in generating electricity. The trend started in the 1990s but clearly gained speed during the Bush administration when the production of natural gas -- a competitor of coal -- picked up. But analysts give little credit or blame to Bush for that trend. They note that other factors, such as technological innovation, entrepreneurship and policies of previous administrations, had more to do with laying the groundwork for the natural gas boom.'\n",
        "label_text = input_text  + justification_text + tokenizer.convert_ids_to_tokens(eos_id)\n",
        "print(f\"tokenized input_text : {tokenizer.tokenize(input_text)}\\n\")\n",
        "print(f\"tokenized label_text : {tokenizer.tokenize(label_text)}\")\n",
        "input_text_length = len(tokenizer.tokenize(input_text))\n",
        "justification_text_length = len(tokenizer.tokenize(justification_text))\n",
        "label_text_length = len(tokenizer.tokenize(label_text))\n",
        "print(f\"input_text_length : {input_text_length}\")\n",
        "\n",
        "input_encoding = tokenizer.encode_plus(\n",
        "    input_text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=label_text_length,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "\n",
        "print(f\"input_encoding : {input_encoding}\\n\")\n",
        "decoded_input_encoding = tokenizer.decode(input_encoding[\"input_ids\"][0])\n",
        "# print(f\"decoded input_encoding : {decoded_input_encoding}\\n\")\n",
        "\n",
        "label_encoding = tokenizer.encode_plus(\n",
        "    label_text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=label_text_length,\n",
        "    return_token_type_ids=False,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "\n",
        "print(f\"label_encoding : {label_encoding}\")\n",
        "# print(f\"label_encoding['input_ids'][0] : {label_encoding['input_ids'][0]}\")\n",
        "decoded_label_encoding = tokenizer.decode(label_encoding[\"input_ids\"][0])\n",
        "# print(f\"decoded label_encoding : {decoded_label_encoding}\")\n",
        "print(f\"input prompt : {decoded_input_encoding}\")\n",
        "print(f\"label : {decoded_label_encoding}\")\n",
        "\n",
        "labels = label_encoding[\"input_ids\"]\n",
        "print(f\"labels before -100 masking : {labels}\")\n",
        "# decoded_labels = tokenizer.decode(labels)\n",
        "# print(f\"decoded labels : {decoded_labels}\")\n",
        "labels[:, :input_text_length+1] = -100 # cross_entropy_ignore_index\n",
        "labels[:, label_text_length:] = -100\n",
        "print(f\"labels after -100 masking : {labels}\")\n",
        "print(f\"labels.shape : {labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fy4pRgFbKYRU"
      },
      "outputs": [],
      "source": [
        "outputs = model(input_ids=input_encoding[\"input_ids\"], attention_mask=input_encoding[\"attention_mask\"], labels=labels)\n",
        "\n",
        "print(f\"model : {model}\")\n",
        "print(f\"type(model) : {type(model)}\")\n",
        "print(f\"type(outputs) : {type(outputs)}\")\n",
        "\n",
        "logits = outputs.logits\n",
        "loss = outputs.loss\n",
        "print(f\"logits : {logits}\")\n",
        "print(f\"logtis.size() : {logits.size()}\")\n",
        "print(f\"loss : {loss}\")\n",
        "\n",
        "m = nn.Softmax(dim=1)\n",
        "output = m(logits)\n",
        "print(f\"output : {output}\")\n",
        "print(f\"output.size() : {output.size()}\")\n",
        "arg = torch.argmax(output, dim=2)\n",
        "print(f\"argmax(ouput) : {arg}\") # --> tensor\n",
        "print(f\"argmax(ouput).size() : {arg.size()}\")\n",
        "print(f\"argmax(output).dtype() : {arg.dtype}\")\n",
        "print(arg)\n",
        "generated = tokenizer.decode(arg[0])\n",
        "print(f\"generated : {generated}\")\n",
        "print(f\"len(generated) : {len(tokenizer.tokenize(generated))}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ_6sUIJtXs5"
      },
      "source": [
        "## 推論"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOaCTJIdtZ_R"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPwncK3t2LkoyWzPSCgYV3k",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b5395d1c6d2e4d0bbfe2f730d5ba5903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2437af86be70480b983248e50a931a32",
              "IPY_MODEL_f48a91dc6b7946a7b8eb76f2381cbb74",
              "IPY_MODEL_2a106f10fa84401fb4ea97beab6f5f24"
            ],
            "layout": "IPY_MODEL_1a1e37058555418ab5c36ff7b9a45248"
          }
        },
        "2437af86be70480b983248e50a931a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9297b0ad25fc461c9c8a4b9fbbbb4de6",
            "placeholder": "​",
            "style": "IPY_MODEL_2970346c22ca4d998149e31b63d6ba4f",
            "value": "Sanity Checking DataLoader 0:   0%"
          }
        },
        "f48a91dc6b7946a7b8eb76f2381cbb74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ac9b442659640fa847fe82096539f00",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4beb68466b184160b314f1d4e3ff6a4c",
            "value": 0
          }
        },
        "2a106f10fa84401fb4ea97beab6f5f24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_452fff34c777421187bea9c3c13228c4",
            "placeholder": "​",
            "style": "IPY_MODEL_af666b0ce46345ecb166149b4e73e294",
            "value": " 0/1 [00:00&lt;?, ?it/s]"
          }
        },
        "1a1e37058555418ab5c36ff7b9a45248": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "9297b0ad25fc461c9c8a4b9fbbbb4de6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2970346c22ca4d998149e31b63d6ba4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ac9b442659640fa847fe82096539f00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4beb68466b184160b314f1d4e3ff6a4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "452fff34c777421187bea9c3c13228c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af666b0ce46345ecb166149b4e73e294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}