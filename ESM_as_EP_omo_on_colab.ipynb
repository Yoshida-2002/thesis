{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yoshida-2002/thesis/blob/main/ESM_as_EP_omo_on_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 514,
      "metadata": {
        "id": "sJyBZO0Qkb9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39f51d2f-9c30-495f-998f-ba0b865ee1e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 515,
      "metadata": {
        "id": "HeQtIJt6k_ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "0c574e08-8e49-4425-bc9a-5024ee1dfe64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/B4yoshida/Enhanced Sequence Model with Pytorch Lightning\n",
            " \u001b[0m\u001b[01;34mdata\u001b[0m/  'ESM as EP omo on colab'   \u001b[01;34mlightning_logs\u001b[0m/   \u001b[01;34mplot_figures\u001b[0m/   \u001b[01;34mresult_log\u001b[0m/   \u001b[01;34msrc\u001b[0m/   \u001b[01;34mweights\u001b[0m/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/B4yoshida/Enhanced Sequence Model with Pytorch Lightning'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 515
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/B4yoshida/Enhanced Sequence Model with Pytorch Lightning\n",
        "%ls\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 516,
      "metadata": {
        "id": "ExMTTZMNlKMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "280f9f67-6bc3-4282-d0d9-55570244590c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.3.0.post0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch-lightning) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch-lightning) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pathを辞書にまとめて管理する"
      ],
      "metadata": {
        "id": "_fm0w0jc01Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path_dict = {\n",
        "\n",
        "#     # 生データのパス\n",
        "#     'raw_train_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/train2.tsv',\n",
        "#     'raw_test_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/test2.tsv',\n",
        "#     'raw_val_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/row/tsv/val2.tsv',\n",
        "\n",
        "#     # 欠損値処理を行った後のデータのパス\n",
        "#     'missing_value_processed_train_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_train2.tsv',\n",
        "#     'missing_value_processed_test_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_test2.tsv',\n",
        "#     'missing_value_processed_val_data' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/missing_value_processed/pocessed_val2.tsv',\n",
        "\n",
        "#     # 欠損値処理を行った後、さらなる整形を行ったデータ\n",
        "#     'input_for_lightning_model_train_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_train2.tsv',\n",
        "#     'input_for_lightning_model_test_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_test2.tsv',\n",
        "#     'input_for_lightning_model_val_base' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/base/input_val2.tsv',\n",
        "\n",
        "#     # 集合に分割した後のデータ (main)\n",
        "#     'main_input_for_lightning_model_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/train/true_set2.tsv',\n",
        "#     'main_input_for_lightning_model_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/train/false_set2.tsv',\n",
        "#     'main_input_for_lightning_model_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/test/true_set2.tsv',\n",
        "#     'main_input_for_lightning_model_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/test/false_set2.tsv',\n",
        "#     'main_input_for_lightning_model_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/val/true_set2.tsv',\n",
        "#     'main_input_for_lightning_model_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/main/val/false_set2.tsv',\n",
        "\n",
        "#     # 集合に分割した後のデータ (toy)\n",
        "#     'toy_input_for_lightning_model_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/train/true_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/train/false_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/test/true_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/test/false_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/val/true_set2.tsv',\n",
        "#     'toy_input_for_lightning_model_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/input_for_lightning_model/set/toy/val/false_set2.tsv',\n",
        "\n",
        "#     # モデルを保存するPath\n",
        "#     'saved_model_path' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/src/models',\n",
        "#     'TensorBoardLogger': 'lightning_logs',\n",
        "\n",
        "#     # 拡張したデータの保存場所(main)\n",
        "#     'main_extended_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/train/true_set2.tsv',\n",
        "#     'main_extended_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/train/false_set2.tsv',\n",
        "#     'main_extended_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/test/true_set2.tsv',\n",
        "#     'main_extended_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/test/false_set2.tsv',\n",
        "#     'main_extended_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/val/true_set2.tsv',\n",
        "#     'main_extended_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/val/false_set2.tsv',\n",
        "\n",
        "#     # 拡張したデータの保存場所(main)\n",
        "#     'toy_extended_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/train/true_set2.tsv',\n",
        "#     'toy_extended_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/train/false_set2.tsv',\n",
        "#     'toy_extended_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/test/true_set2.tsv',\n",
        "#     'toy_extended_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/test/false_set2.tsv',\n",
        "#     'toy_extended_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/val/true_set2.tsv',\n",
        "#     'toy_extended_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/val/false_set2.tsv',\n",
        "\n",
        "#   }\n"
      ],
      "metadata": {
        "id": "VrElytlN06OI"
      },
      "execution_count": 517,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_dict = {\n",
        "    'saved_model_path' :  '/content/drive/MyDrive/B4yoshida/Enhanced Sequence Model with Pytorch Lightning/src/models',\n",
        "\n",
        "    # モデルを保存するPath\n",
        "    'saved_model_path' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/src/models',\n",
        "    'TensorBoardLogger': 'lightning_logs',\n",
        "\n",
        "    # 拡張したデータの保存場所(main)\n",
        "    'main_extended_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/train/true_set2.tsv',\n",
        "    'main_extended_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/train/false_set2.tsv',\n",
        "    'main_extended_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/test/true_set2.tsv',\n",
        "    'main_extended_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/test/false_set2.tsv',\n",
        "    'main_extended_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/val/true_set2.tsv',\n",
        "    'main_extended_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/main/val/false_set2.tsv',\n",
        "\n",
        "    # 拡張したデータの保存場所(main)\n",
        "    'toy_extended_train_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/train/true_set2.tsv',\n",
        "    'toy_extended_train_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/train/false_set2.tsv',\n",
        "    'toy_extended_test_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/test/true_set2.tsv',\n",
        "    'toy_extended_test_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/test/false_set2.tsv',\n",
        "    'toy_extended_val_data_true' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/val/true_set2.tsv',\n",
        "    'toy_extended_val_data_false' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/toy/val/false_set2.tsv',\n",
        "\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "LWntLFailqZ5"
      },
      "execution_count": 518,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8eAFYfq3-CD"
      },
      "source": [
        "# ModelConfigを定義する\n",
        "- colab上では辞書\n",
        "- モジュール分けするときはjsonに"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dPiXl1ej7b3"
      },
      "source": [
        "## BERT/RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 519,
      "metadata": {
        "id": "kpgeqGQOkCpL"
      },
      "outputs": [],
      "source": [
        "# bert-base\n",
        "bert_base_config = {\n",
        "    'MODEL_NAME' : 'bert-base-uncased',\n",
        "    'TOKENIZER_NAME' : 'bert-base-uncased',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 1,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "\n",
        "    # 'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH_TRUE' : path_dict['toy_extended_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH_TRUE' : path_dict['toy_extended_val_data_true'],\n",
        "    'TEST_DATA_PATH_TRUE' : path_dict['toy_extended_test_data_true'],\n",
        "\n",
        "    'TRAINING_DATA_PATH_FALSE' : path_dict['toy_extended_train_data_false'],\n",
        "    'VALIDATION_DATA_PATH_FALSE' : path_dict['toy_extended_val_data_false'],\n",
        "    'TEST_DATA_PATH_FALSE' : path_dict['toy_extended_test_data_false'],\n",
        "\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    # 'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'ESM',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 'bert-base-uncased',\n",
        "    'MODEL_FOLDER' : ['bert','base-uncased']\n",
        "}\n",
        "\n",
        "# roberta-base\n",
        "roberta_base_config = {\n",
        "    'MODEL_NAME' : 'roberta-base',\n",
        "    'TOKENIZER_NAME' : 'roberta-base',\n",
        "    'MAX_LENGTH' : 512,\n",
        "    'TRAINING_BATCH_SIZE' : 16,\n",
        "    'VALIDATION_BATCH_SIZE' : 16,\n",
        "    'TEST_BATCH_SIZE' : 1,\n",
        "    'TRAINING_EPOCHS' : 2,\n",
        "    'SAVE_TOP_K' : 1,\n",
        "    'SAVED_MODEL_PATH' : path_dict['saved_model_path'],\n",
        "    # 'CSV_LOGGER_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/weights/csv_log',\n",
        "    'TRAINING_DATA_PATH_TRUE' : path_dict['toy_extended_train_data_true'],\n",
        "    'VALIDATION_DATA_PATH_TRUE' : path_dict['toy_extended_val_data_true'],\n",
        "    'TEST_DATA_PATH_TRUE' : path_dict['toy_extended_test_data_true'],\n",
        "\n",
        "    'TRAINING_DATA_PATH_FALSE' : path_dict['toy_extended_train_data_false'],\n",
        "    'VALIDATION_DATA_PATH_FALSE' : path_dict['toy_extended_val_data_false'],\n",
        "    'TEST_DATA_PATH_FALSE' : path_dict['toy_extended_test_data_false'],\n",
        "\n",
        "    'LR' : 7e-5,\n",
        "    'RANDOM_SEED' : 42,\n",
        "    # 'CROSS_ENTROPY_IGNORE_INDEX' : -100,\n",
        "    'COMPONENT_NAME' : 'ESM',\n",
        "    'PROMPT_COMPONENTS' : ['statement', 'metadata', 'justification'],\n",
        "    'TensorBoardLogger_NAME' : 'roberta-base',\n",
        "    'MODEL_FOLDER' : ['roberta', 'base']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model_Dict"
      ],
      "metadata": {
        "id": "QoWojRc90I8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model_Config_Dict = {\n",
        "    'bert_base_config' : bert_base_config,\n",
        "    'roberta_base_config' : roberta_base_config,\n",
        "}"
      ],
      "metadata": {
        "id": "IILYrc640NFr"
      },
      "execution_count": 520,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L_nsgKMlYCX"
      },
      "source": [
        "# データの読み込み書き込み"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i56IGn0Llab-"
      },
      "source": [
        "## data_reader.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 521,
      "metadata": {
        "id": "b-MSarQelcOy"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"data_reader module is written for read files\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def read_csv(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "\n",
        "    :param path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "def read_tsv(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "\n",
        "    :param path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return pd.read_csv(path, sep=\"\\t\", header=None)\n",
        "\n",
        "\n",
        "def read_npy(path: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    load a list of numpy elements into memory\n",
        "    :param path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return np.load(path, allow_pickle=True)\n",
        "\n",
        "\n",
        "def read_excel(path:str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    :param path: where to load data from\n",
        "    :return: pd.DataFrame\n",
        "    \"\"\"\n",
        "    return pd.read_excel(path, engine=\"openpyxl\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqU5U-mVmDOE"
      },
      "source": [
        "## data_writer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 522,
      "metadata": {
        "id": "8esOQSnAl9nz"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"data_writer module is written for write data in files\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def write_npy(path: str, data: list) -> None:\n",
        "    \"\"\"\n",
        "    save a list of numpy elements into disk\n",
        "    :param path:\n",
        "    :param data:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return np.save(path, data, allow_pickle=True)\n",
        "\n",
        "def write_dataframe_in_tsv(data: pd.DataFrame, path: str) -> None:\n",
        "\n",
        "  \"\"\"\n",
        "  save pd.Dataframe in tsv file\n",
        "  :param path:\n",
        "  :param data:\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  data.to_csv(path, sep='\\t', index=False, header=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shph_NDW3HpB"
      },
      "source": [
        "# class Prepare_tokenizerを定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 523,
      "metadata": {
        "id": "D0mJ9pIG3MHe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import warnings\n",
        "\n",
        "class Prepare_Tokenizer():\n",
        "    def __init__(self, config:dict):\n",
        "        self.config = config\n",
        "        self.TOKENIZER_NAME = self.config['TOKENIZER_NAME']\n",
        "        self.tokenizer = None\n",
        "        self.tokenizer_length = None\n",
        "        self.additional_special_tokens = None\n",
        "        self.eos_token = None\n",
        "        self.eos_token_id = None\n",
        "        self.pad_token = None\n",
        "        self.pad_token_id = None\n",
        "        self.exp_token = '[EXP]'\n",
        "        self.exp_token_id = None\n",
        "        self.initialization_flag = 0\n",
        "\n",
        "    def get_tokenizer(self):\n",
        "        if self.initialization_flag != 0:\n",
        "            raise ValueError(f\"tokenizer is already initialized. This instance is for {self.tokenizer}\")\n",
        "\n",
        "        if self.TOKENIZER_NAME in ['bert-base-uncased']:\n",
        "            self.initialization_flag = 1\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.TOKENIZER_NAME)\n",
        "            # print(f\"tokenizer : AutoTokenizer.from_pretrained({self.TOKENIZER_NAME})\")\n",
        "            # print(\"Before adding additional_special_tokens\")\n",
        "            # print(self.tokenizer.all_special_tokens)\n",
        "            # print(self.tokenizer.all_special_ids)\n",
        "            # special_tokens_dict = {\n",
        "            # 'additional_special_tokens': ['[EXP]'],\n",
        "            # \"eos_token\" : \"<|endoftext|>\",\n",
        "            # \"pad_token\" : \"<|endoftext|>\"\n",
        "            # }\n",
        "            # self.eos_token = \"<|endoftext|>\"\n",
        "            # self.pad_token = \"<|endoftext|>\"\n",
        "            # self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "            # self.exp_token_id = self.tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "            # self.eos_token_id = self.tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "            # self.pad_token_id = self.tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "            # print(\"After adding additional_special_tokens\")\n",
        "            # print(self.tokenizer.all_special_tokens)\n",
        "            # print(self.tokenizer.all_special_ids)\n",
        "            # print(f\"exp_id : {self.exp_token_id}\")\n",
        "            # print(f\"eos_id : {self.eos_token_id}\")\n",
        "            # print(f\"pad_id : {self.pad_token_id}\")\n",
        "            # self.tokenizer_length = len(self.tokenizer)\n",
        "            # print(f\"len(tokenizer) : {self.tokenizer_length}\")\n",
        "\n",
        "            return self.tokenizer\n",
        "\n",
        "        elif self.TOKENIZER_NAME in ['roberta-base']:\n",
        "            self.initialization_flag = 1\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.TOKENIZER_NAME)\n",
        "            # print(f\"tokenizer : AutoTokenizer.from_pretrained({self.TOKENIZER_NAME})\")\n",
        "            # print(\"Before adding additional_special_tokens\")\n",
        "            # print(self.tokenizer.all_special_tokens)\n",
        "            # print(self.tokenizer.all_special_ids)\n",
        "            # special_tokens_dict = {\n",
        "            # 'additional_special_tokens': ['[EXP]'],\n",
        "            # }\n",
        "            # self.eos_token = '</s>'\n",
        "            # self.pad_token = '<pad>'\n",
        "            # self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "            # self.exp_token_id = self.tokenizer.convert_tokens_to_ids('[EXP]')\n",
        "            # self.eos_token_id = self.tokenizer.convert_tokens_to_ids('</s>')\n",
        "            # self.pad_token_id = self.tokenizer.convert_tokens_to_ids('<pad>')\n",
        "            # print(\"After adding additional_special_tokens\")\n",
        "            # print(self.tokenizer.all_special_tokens)\n",
        "            # print(self.tokenizer.all_special_ids)\n",
        "            # print(f\"exp_id : {self.exp_token_id}\")\n",
        "            # print(f\"eos_id : {self.eos_token_id}\")\n",
        "            # print(f\"pad_id : {self.pad_token_id}\")\n",
        "            # self.tokenizer_length = len(self.tokenizer)\n",
        "            # print(f\"len(tokenizer) : {self.tokenizer_length}\")\n",
        "\n",
        "            return self.tokenizer\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Wrong Tokenizer Type : you have to indicate tokenizer type out of the folloing list.\\n [\"gpt2\", \"gpt2-medium\", \"t5-small\", \"t5-base\", \"bart-base\"]\\n')\n",
        "\n",
        "    def get_tokenizer_info(self):\n",
        "        if self.initialization_flag == 0:\n",
        "            print(\"there is no information. you need to call get_tokenizer() first.\")\n",
        "        else:\n",
        "            print(\"---------- tokenizer information ----------\")\n",
        "            print(f\"self.TOKENIZER_NAME : {self.TOKENIZER_NAME}\")\n",
        "            print(f\"self.tokenizer : {self.tokenizer}\")\n",
        "            print(f\"self.tokenizer_length : {self.tokenizer_length}\")\n",
        "            print(f\"self.additional_special_tokens : {self.additional_special_tokens}\")\n",
        "            print(f\"self.eos_token : {self.eos_token}\")\n",
        "            print(f\"self.eos_token_id : {self.eos_token_id}\")\n",
        "            print(f\"self.pad_token : {self.pad_token}\")\n",
        "            print(f\"self.pad_token_id : {self.pad_token_id}\")\n",
        "            print(f\"self.exp_token : {self.exp_token}\")\n",
        "            print(f\"self.exp_token_id : {self.exp_token_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkBEZqVYGCj7"
      },
      "source": [
        "# class Prepare_Modelを定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 524,
      "metadata": {
        "id": "HE6f_5MZGr0k"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, BertModel, RobertaModel\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Prepare_Model():\n",
        "\n",
        "    def __init__(self, config:dict, prepare_tokenizer):\n",
        "        self.MODEL_NAME = config['MODEL_NAME']\n",
        "        self.TOKENIZER_LENGTH = prepare_tokenizer.tokenizer_length\n",
        "        self.additional_model_config = None\n",
        "        self.prepare_tokenizer = prepare_tokenizer\n",
        "        self.model = None\n",
        "        self.initialization_flag = 0\n",
        "\n",
        "\n",
        "    def get_model(self):\n",
        "        if self.initialization_flag == 1:\n",
        "            raise ValueError('You cannot call get_model() because this is already initialized.\\n You are supossed to instantiate Prepare_Model_Class and then call get_model() again.')\n",
        "\n",
        "        if self.MODEL_NAME in ['bert-base-uncased']:\n",
        "\n",
        "            if self.prepare_tokenizer is None:\n",
        "                raise ValueError(f'prepare_tokenizer is None and this is not good for {self.MODEL_NAME}!')\n",
        "            self.initialization_flag = 1\n",
        "            self.model = BertModel.from_pretrained(self.MODEL_NAME, return_dict=True, num_labels=2)\n",
        "\n",
        "            return self.model\n",
        "\n",
        "        elif self.MODEL_NAME in ['roberta-base']:\n",
        "\n",
        "            if self.prepare_tokenizer is None:\n",
        "                raise ValueError(f'prepare_tokenizer is None and this is not good for {self.MODEL_NAME}!')\n",
        "\n",
        "            self.initialization_flag = 1\n",
        "            self.model = RobertaModel.from_pretrained(self.MODEL_NAME)\n",
        "\n",
        "            return self.model\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Wrong Model Type')\n",
        "\n",
        "\n",
        "    def get_model_info(self):\n",
        "        if self.initialization_flag == 0:\n",
        "            print(\"there is no information. you need to call get_model() first.\")\n",
        "        else:\n",
        "            print(\"---------- model information ----------\")\n",
        "            print(f\"self.MODEL_NAME : {self.MODEL_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0ZBjsDSnW4X"
      },
      "source": [
        "#  build_checkpoint_callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 525,
      "metadata": {
        "id": "Em-WoejQnYyF"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"helper module is written for write useful function in indexer package\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "def build_checkpoint_callback(config_dict, base_time, batch_size, set_type, filename=\"QTag-{epoch:02d}-{val_loss:.2f}\",\n",
        "                              monitor=\"val_loss\"):\n",
        "    \"\"\"\n",
        "\n",
        "    :param save_top_k:\n",
        "    :param filename:\n",
        "    :param monitor:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    dirpath = config_dict['SAVED_MODEL_PATH'] + '/' + config_dict['TensorBoardLogger_NAME'] + f'/{base_time}/{set_type}_batch_size={batch_size}'\n",
        "    print(f'dirpath : {dirpath}')\n",
        "    checkpoint_callback = ModelCheckpoint(monitor=monitor,  # monitored quantity\n",
        "                                          filename=filename,\n",
        "                                          save_top_k=config_dict['SAVE_TOP_K'],  # save the top k models\n",
        "                                          dirpath=dirpath,\n",
        "                                          mode=\"min\",  # mode of the monitored quantity for optimization\n",
        "                                          )\n",
        "    print(f\"checkpoint_callback : {checkpoint_callback}\")\n",
        "    return checkpoint_callback, dirpath"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YAKE!に関わるコード"
      ],
      "metadata": {
        "id": "YYVicvyYk00Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "true_data = read_tsv('/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/toy_prediction/2024-01-24 13:17 flip/gpt2-medium_2024-01-24 13:17 flip_true_batch_size=2_true.tsv')\n",
        "true_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "g5mDmvtMK5rt",
        "outputId": "bb818467-b82c-4e07-8908-aa9b1e71eac5"
      },
      "execution_count": 526,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     0            1                                                  2  \\\n",
              "0  1.0    half-true  When did the decline of coal start? It started...   \n",
              "1  2.0  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
              "2  4.0    half-true  The economic turnaround started at the end of ...   \n",
              "3  5.0         true  The Chicago Bears have had more starting quart...   \n",
              "4  7.0    half-true  I'm the only person on this stage who has work...   \n",
              "\n",
              "                                                   3  \\\n",
              "0  energy,history,job-accomplishments scott-surov...   \n",
              "1  foreign-policy barack-obama President Illinois...   \n",
              "2  economy,jobs charlie-crist None Florida democr...   \n",
              "3  education robin-vos Wisconsin Assembly speaker...   \n",
              "4  ethics barack-obama President Illinois democra...   \n",
              "\n",
              "                                                   4         5           6  \\\n",
              "0  Surovell said the decline of coal \"started whe...  0.350000  10540.json   \n",
              "1  Obama said he would have voted against the ame...  0.503171    324.json   \n",
              "2  Crist said that the economic \"turnaround start...  0.540769   9028.json   \n",
              "3  But Vos specifically used the word \"fired,\" wh...  0.518182  12465.json   \n",
              "4  However, it was not that bill, but another one...  0.503171    153.json   \n",
              "\n",
              "                                                   7  \n",
              "0   of the of the of the of the of the of the of ...  \n",
              "1   of the of the of the of the of the of the of ...  \n",
              "2  ... \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" ...  \n",
              "3   of the of the of the of the of the of the of ...  \n",
              "4   of the..........................................  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-32d13346-6791-4b9e-b302-e0682e8e7c21\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>half-true</td>\n",
              "      <td>When did the decline of coal start? It started...</td>\n",
              "      <td>energy,history,job-accomplishments scott-surov...</td>\n",
              "      <td>Surovell said the decline of coal \"started whe...</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>10540.json</td>\n",
              "      <td>of the of the of the of the of the of the of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>mostly-true</td>\n",
              "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
              "      <td>foreign-policy barack-obama President Illinois...</td>\n",
              "      <td>Obama said he would have voted against the ame...</td>\n",
              "      <td>0.503171</td>\n",
              "      <td>324.json</td>\n",
              "      <td>of the of the of the of the of the of the of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.0</td>\n",
              "      <td>half-true</td>\n",
              "      <td>The economic turnaround started at the end of ...</td>\n",
              "      <td>economy,jobs charlie-crist None Florida democr...</td>\n",
              "      <td>Crist said that the economic \"turnaround start...</td>\n",
              "      <td>0.540769</td>\n",
              "      <td>9028.json</td>\n",
              "      <td>... \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.0</td>\n",
              "      <td>true</td>\n",
              "      <td>The Chicago Bears have had more starting quart...</td>\n",
              "      <td>education robin-vos Wisconsin Assembly speaker...</td>\n",
              "      <td>But Vos specifically used the word \"fired,\" wh...</td>\n",
              "      <td>0.518182</td>\n",
              "      <td>12465.json</td>\n",
              "      <td>of the of the of the of the of the of the of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.0</td>\n",
              "      <td>half-true</td>\n",
              "      <td>I'm the only person on this stage who has work...</td>\n",
              "      <td>ethics barack-obama President Illinois democra...</td>\n",
              "      <td>However, it was not that bill, but another one...</td>\n",
              "      <td>0.503171</td>\n",
              "      <td>153.json</td>\n",
              "      <td>of the..........................................</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-32d13346-6791-4b9e-b302-e0682e8e7c21')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-32d13346-6791-4b9e-b302-e0682e8e7c21 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-32d13346-6791-4b9e-b302-e0682e8e7c21');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-55c705e9-75a0-43a1-8807-e208b70603d8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-55c705e9-75a0-43a1-8807-e208b70603d8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-55c705e9-75a0-43a1-8807-e208b70603d8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 526
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/LIAAD/yake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUwBXOyik4cs",
        "outputId": "30c667f5-82ad-4f20-9fd6-0fc198b9f91e"
      },
      "execution_count": 527,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/LIAAD/yake\n",
            "  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-d831_end\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/LIAAD/yake /tmp/pip-req-build-d831_end\n",
            "  Resolved https://github.com/LIAAD/yake to commit 0fa58cceb465162b6bd0cab7ec967edeb907fbcc\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from yake==0.4.8) (0.9.0)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.10/dist-packages (from yake==0.4.8) (8.1.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from yake==0.4.8) (1.23.5)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.10/dist-packages (from yake==0.4.8) (1.5.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from yake==0.4.8) (3.2.1)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.10/dist-packages (from yake==0.4.8) (1.0.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from segtok->yake==0.4.8) (2023.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yake\n",
        "\n",
        "text = \"Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning \"\\\n",
        "\"competitions. Details about the transaction remain somewhat vague, but given that Google is hosting its Cloud \"\\\n",
        "\"Next conference in San Francisco this week, the official announcement could come as early as tomorrow. \"\\\n",
        "\"Reached by phone, Kaggle co-founder CEO Anthony Goldbloom declined to deny that the acquisition is happening. \"\\\n",
        "\"Google itself declined 'to comment on rumors'. Kaggle, which has about half a million data scientists on its platform, \"\\\n",
        "\"was founded by Goldbloom  and Ben Hamner in 2010. \"\\\n",
        "\"The service got an early start and even though it has a few competitors like DrivenData, TopCoder and HackerRank, \"\\\n",
        "\"it has managed to stay well ahead of them by focusing on its specific niche. \"\\\n",
        "\"The service is basically the de facto home for running data science and machine learning competitions. \"\\\n",
        "\"With Kaggle, Google is buying one of the largest and most active communities for data scientists - and with that, \"\\\n",
        "\"it will get increased mindshare in this community, too (though it already has plenty of that thanks to Tensorflow \"\\\n",
        "\"and other projects). Kaggle has a bit of a history with Google, too, but that's pretty recent. Earlier this month, \"\\\n",
        "\"Google and Kaggle teamed up to host a $100,000 machine learning competition around classifying YouTube videos. \"\\\n",
        "\"That competition had some deep integrations with the Google Cloud Platform, too. Our understanding is that Google \"\\\n",
        "\"will keep the service running - likely under its current name. While the acquisition is probably more about \"\\\n",
        "\"Kaggle's community than technology, Kaggle did build some interesting tools for hosting its competition \"\\\n",
        "\"and 'kernels', too. On Kaggle, kernels are basically the source code for analyzing data sets and developers can \"\\\n",
        "\"share this code on the platform (the company previously called them 'scripts'). \"\\\n",
        "\"Like similar competition-centric sites, Kaggle also runs a job board, too. It's unclear what Google will do with \"\\\n",
        "\"that part of the service. According to Crunchbase, Kaggle raised $12.5 million (though PitchBook says it's $12.75) \"\\\n",
        "\"since its   launch in 2010. Investors in Kaggle include Index Ventures, SV Angel, Max Levchin, Naval Ravikant, \"\\\n",
        "\"Google chief economist Hal Varian, Khosla Ventures and Yuri Milner \""
      ],
      "metadata": {
        "id": "URH36Y0upNSY"
      },
      "execution_count": 528,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kw_extractor = yake.KeywordExtractor()\n",
        "keywords = kw_extractor.extract_keywords(text)\n",
        "\n",
        "for kw in keywords:\n",
        "\tprint(kw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk77t7oHpYQ_",
        "outputId": "b7a8d3ed-9f5d-41b1-ce2c-c4b1d911dfde"
      },
      "execution_count": 529,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Google', 0.026580863364597897)\n",
            "('Kaggle', 0.0289005976239829)\n",
            "('CEO Anthony Goldbloom', 0.029946071606210194)\n",
            "('San Francisco', 0.048810837074825336)\n",
            "('Anthony Goldbloom declined', 0.06176910090701819)\n",
            "('Google Cloud Platform', 0.06261974476422487)\n",
            "('co-founder CEO Anthony', 0.07357749587020043)\n",
            "('acquiring Kaggle', 0.08723571551039863)\n",
            "('CEO Anthony', 0.08915156857226395)\n",
            "('Anthony Goldbloom', 0.09123482372372106)\n",
            "('machine learning', 0.09147989238151344)\n",
            "('Kaggle co-founder CEO', 0.093805063905847)\n",
            "('data', 0.097574333771058)\n",
            "('Google Cloud', 0.10260128641464673)\n",
            "('machine learning competitions', 0.10773000650607861)\n",
            "('Francisco this week', 0.11519915079240485)\n",
            "('platform', 0.1183512305596321)\n",
            "('conference in San', 0.12392066376108138)\n",
            "('service', 0.12546743261462942)\n",
            "('Goldbloom', 0.14611408778815776)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yake_config = {\n",
        "    'language' : \"en\",\n",
        "    'max_ngram_size' : 3,\n",
        "    'deduplication_threshold' : 0.9,\n",
        "    'deduplication_algo' : 'seqm',\n",
        "    'windowSize' : 1,\n",
        "    'numOfKeywords' : 20,\n",
        "}"
      ],
      "metadata": {
        "id": "Ozm1rr9hpfqP"
      },
      "execution_count": 530,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_kw_extractor = yake.KeywordExtractor(lan=yake_config['language'], n=yake_config['max_ngram_size'], dedupLim=yake_config['deduplication_threshold'], dedupFunc=yake_config['deduplication_algo'], windowsSize=yake_config['windowSize'], top=yake_config['numOfKeywords'], features=None)\n",
        "keywords = custom_kw_extractor.extract_keywords(text)\n",
        "\n",
        "for kw in keywords:\n",
        "    print(kw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCuIZ1blqS24",
        "outputId": "fa7e8741-e6b7-4405-af97-2cddd60b7eff"
      },
      "execution_count": 531,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Google', 0.026580863364597897)\n",
            "('Kaggle', 0.0289005976239829)\n",
            "('CEO Anthony Goldbloom', 0.029946071606210194)\n",
            "('San Francisco', 0.048810837074825336)\n",
            "('Anthony Goldbloom declined', 0.06176910090701819)\n",
            "('Google Cloud Platform', 0.06261974476422487)\n",
            "('co-founder CEO Anthony', 0.07357749587020043)\n",
            "('acquiring Kaggle', 0.08723571551039863)\n",
            "('CEO Anthony', 0.08915156857226395)\n",
            "('Anthony Goldbloom', 0.09123482372372106)\n",
            "('machine learning', 0.09147989238151344)\n",
            "('Kaggle co-founder CEO', 0.093805063905847)\n",
            "('data', 0.097574333771058)\n",
            "('Google Cloud', 0.10260128641464673)\n",
            "('machine learning competitions', 0.10773000650607861)\n",
            "('Francisco this week', 0.11519915079240485)\n",
            "('platform', 0.1183512305596321)\n",
            "('conference in San', 0.12392066376108138)\n",
            "('service', 0.12546743261462942)\n",
            "('Goldbloom', 0.14611408778815776)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def extract_keywords()\n",
        "- 入力としてdata.iloc[]を受け取り, dataとconcatinated_inputを受け取る"
      ],
      "metadata": {
        "id": "iJCbqeylL_M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keywords(data, yake_config:dict, save_data:bool=False):\n",
        "    custom_kw_extractor = yake.KeywordExtractor(lan=yake_config['language'], n=yake_config['max_ngram_size'], dedupLim=yake_config['deduplication_threshold'], dedupFunc=yake_config['deduplication_algo'], windowsSize=yake_config['windowSize'], top=yake_config['numOfKeywords'], features=None)\n",
        "    # data[7]は生成された説明文をさす\n",
        "    keywords = custom_kw_extractor.extract_keywords(data[7])\n",
        "\n",
        "    concatinated_keywords = \"\"\n",
        "    for kw in keywords:\n",
        "        concatinated_keywords += kw[0] + \" \"\n",
        "\n",
        "    if save_data:\n",
        "        data[8] = concatinated_keywords\n",
        "\n",
        "    return data, concatinated_keywords\n"
      ],
      "metadata": {
        "id": "kcehPmAVMoRk"
      },
      "execution_count": 532,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, concatinated_keywords = extract_keywords(true_data.iloc[0], yake_config)\n",
        "print(concatinated_keywords)\n",
        "print(type(concatinated_keywords))\n",
        "print(len(concatinated_keywords))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA0HJVt9Pu4F",
        "outputId": "f1f878d8-f962-4b07-9ced-651eaac78fd7"
      },
      "execution_count": 533,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<class 'str'>\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NSに関わるコード\n",
        "- Dataset\n",
        "- LDM\n",
        "- LM"
      ],
      "metadata": {
        "id": "Iq6dF5b-5c2-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArPSSG2V5yQB"
      },
      "source": [
        "## class LIAR_PLUS_Dataset_For_NS(Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 534,
      "metadata": {
        "id": "DzYgf7ue5yQe"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# pylint: disable-msg=import-error\n",
        "# pylint: disable-msg=no-member\n",
        "# ========================================================\n",
        "\"\"\"dataset module is written for create data module\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from typing import Optional\n",
        "from time import sleep\n",
        "\n",
        "\n",
        "class LIAR_PLUS_Dataset_For_NS(Dataset):\n",
        "    \"\"\"\n",
        "    this class is for encoding input dataframe for BERT\n",
        "\n",
        "    Input dataframe needs to be consist of 4 columns, \"statement\", \"metadata\", \"justification\" and \"credit_score\"\n",
        "\n",
        "    later we wrap a lightning data module around it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data: pd.DataFrame, config_dict:dict):\n",
        "        self.data = data\n",
        "        self.config_dict = config_dict\n",
        "        self.prepare_tokenizer = Prepare_Tokenizer(self.config_dict)\n",
        "        self.tokenizer = self.prepare_tokenizer.get_tokenizer()\n",
        "        self.prepare_tokenizer.get_tokenizer_info()\n",
        "        self.max_token_len = config_dict['MAX_LENGTH']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "\n",
        "        data_row = self.data.iloc[index]\n",
        "\n",
        "        statement = data_row[2] # statement\n",
        "        metadata = data_row[3] # metadata\n",
        "\n",
        "        credit_score = data_row[5] # credit_score\n",
        "        justification_on_true = data_row[7]\n",
        "        justification_on_false = data_row[8]\n",
        "\n",
        "        if data_row[1] == \"true\":\n",
        "            label = torch.tensor([1,0], dtype=torch.float32)\n",
        "        elif data_row[1] == \"half-true\":\n",
        "            label = torch.tensor([1,0], dtype=torch.float32)\n",
        "        elif data_row[1] == \"mostly-true\":\n",
        "            label = torch.tensor([1,0], dtype=torch.float32)\n",
        "        elif data_row[1] == \"false\":\n",
        "            label = torch.tensor([0,1], dtype=torch.float32)\n",
        "        elif data_row[1] == \"barely-true\":\n",
        "            label = torch.tensor([0,1], dtype=torch.float32)\n",
        "        elif data_row[1] == \"pants-fire\":\n",
        "            label = torch.tensor([0,1], dtype=torch.float32)\n",
        "        else:\n",
        "            print(f\"Invalid label was found! {data_row[1]}\")\n",
        "            sleep(30)\n",
        "\n",
        "\n",
        "        #label = torch.tensor([1,0], dtype=torch.float32) if data_row[1] == ('true' or 'half-true' or 'mostly-true') else torch.tensor([0,1], dtype=torch.float32)\n",
        "        #print(\"label type : \" + str(label.dtype))\n",
        "        # print(f\"data_row[1] : {data_row[1]} , label : {label}\")\n",
        "\n",
        "\n",
        "        statement_encoding = self.tokenizer.encode_plus(\n",
        "            statement,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        metadata_encoding = self.tokenizer.encode_plus(\n",
        "            metadata,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        justification_on_true_encoding = self.tokenizer.encode_plus(\n",
        "            justification_on_true,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        justification_on_false_encoding = self.tokenizer.encode_plus(\n",
        "            justification_on_false,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return dict(\n",
        "            input_ids1=statement_encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask1=statement_encoding[\"attention_mask\"].flatten(),\n",
        "            input_ids2=metadata_encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask2=metadata_encoding[\"attention_mask\"].flatten(),\n",
        "            input_ids3=justification_on_true_encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask3=justification_on_true_encoding[\"attention_mask\"].flatten(),\n",
        "            input_ids4=justification_on_false_encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask4=justification_on_false_encoding[\"attention_mask\"].flatten(),\n",
        "            credit_score=credit_score,\n",
        "            labels=torch.Tensor(label) # この行ちょっと怪しい\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## class LIAR_PLUS_DataModule_For_NS(pl.LightningDataModule)"
      ],
      "metadata": {
        "id": "ThWVbV_15yQh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 535,
      "metadata": {
        "id": "H7chqSoZ5yQh"
      },
      "outputs": [],
      "source": [
        "class LIAR_PLUS_DataModule_For_NS(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, config_dict, train_df: pd.DataFrame, test_df: pd.DataFrame, val_df: pd.DataFrame):\n",
        "        super().__init__()\n",
        "        self.config_dict = config_dict\n",
        "        self.train_df = train_df\n",
        "        self.test_df = test_df\n",
        "        self.val_df = val_df\n",
        "        self.training_batch_size = config_dict[\"TRAINING_BATCH_SIZE\"]\n",
        "        self.validation_batch_size = config_dict[\"VALIDATION_BATCH_SIZE\"]\n",
        "        self.test_batch_size = config_dict[\"TEST_BATCH_SIZE\"]\n",
        "        self.max_token_len = self.config_dict[\"MAX_LENGTH\"]\n",
        "        self.train_dataset, self.test_dataset, self.val_dataset = None, None, None\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None) -> None:\n",
        "        self.train_dataset = LIAR_PLUS_Dataset_For_ESM(self.train_df, self.config_dict)\n",
        "        print(f\"train_dataset size : {len(self.train_dataset)}\\n\")\n",
        "        self.test_dataset = LIAR_PLUS_Dataset_For_ESM(self.test_df, self.config_dict)\n",
        "        print(f\"test_dataset size : {len(self.test_dataset)}\\n\")\n",
        "        self.val_dataset = LIAR_PLUS_Dataset_For_ESM(self.val_df, self.config_dict)\n",
        "        print(f\"val_dataset size : {len(self.val_dataset)}\\n\")\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.training_batch_size,\n",
        "            shuffle=True,\n",
        "            drop_last=True,\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.validation_batch_size,\n",
        "            drop_last=True,\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            drop_last=True,\n",
        "            num_workers=2\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmewwRuU5yQj"
      },
      "source": [
        "## NS.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 536,
      "metadata": {
        "id": "AOdnld9c5yQj"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# pylint: disable=too-many-arguments\n",
        "# pylint: disable=import-error\n",
        "# ========================================================\n",
        "\"\"\"This module is written for write BERT classifier.\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "from typing import List\n",
        "import pytorch_lightning as pl\n",
        "from torch import nn\n",
        "import torch\n",
        "import torchmetrics\n",
        "\n",
        "from transformers import AutoModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "class ESM(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    creates a pytorch lightning model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config_dict,\n",
        "                 n_warmup_steps: int = None,\n",
        "                 n_training_steps: int = None,\n",
        "                 n_classes: int = None,\n",
        "                 result_manager = None):\n",
        "        super().__init__()\n",
        "        self.config_dict = config_dict\n",
        "        self.result_manager = result_manager\n",
        "        self.prepare_tokenizer = Prepare_Tokenizer(self.config_dict)\n",
        "        self.tokenizer = self.prepare_tokenizer.get_tokenizer()\n",
        "        self.prepare_tokenizer.get_tokenizer_info()\n",
        "        self.prepare_model = Prepare_Model(config=self.config_dict, prepare_tokenizer=self.prepare_tokenizer)\n",
        "        self.model = self.prepare_model.get_model()\n",
        "        self.prepare_model.get_model_info()\n",
        "        #\n",
        "        # self.bert = BertModel.from_pretrained(self.config_dict['LANGUAGE_MODEL_PATH'], return_dict=True, num_labels=n_classes)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size*3, n_classes)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        nn.init.xavier_normal_(self.classifier.weight)\n",
        "        #\n",
        "        self.batch_size = self.cofing_dict['BATCH_SIZE']\n",
        "        self.n_training_steps = n_training_steps\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.accuracy = torchmetrics.classification.BinaryAccuracy()\n",
        "\n",
        "        self.training_step_outputs = []\n",
        "        self.validation_step_outputs = []\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, input_ids3, attention_mask3, input_ids4, attention_mask4, credit_score, labels=None):\n",
        "        \"\"\"\n",
        "\n",
        "        :param input_ids1:\n",
        "        :param attention_mask1:\n",
        "        :param input_ids2:\n",
        "        :param attention_mask2:\n",
        "        :param input_ids3:\n",
        "        :param attention_mask3:\n",
        "        :param labels:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # print(\"=================================================================\\n\")\n",
        "        output1 = self.model(input_ids1, attention_mask=attention_mask1)\n",
        "        output2 = self.model(input_ids2, attention_mask=attention_mask2)\n",
        "        output4 = self.model(input_ids4, attention_mask=attention_mask4)\n",
        "        # print(\"output1 type : \" + str(type(output1)))\n",
        "        # print(\"output2 type : \" + str(type(output2)))\n",
        "        # print(\"output3 type : \" + str(type(output3)))\n",
        "        # print(\"\\n\")\n",
        "\n",
        "        # output1 = self.dropout(output1.pooler_output)\n",
        "        # output2 = self.dropout(output2.pooler_output)\n",
        "        # output3 = self.dropout(output3.pooler_output)\n",
        "        # print(\"passed dropout layer\")\n",
        "\n",
        "        # print(\"output1 type : \" + str(output1.dtype))\n",
        "        # print(\"output1 shape : \" + str(output1.shape))\n",
        "        # print(\"is cuda : \" + str(output1.is_cuda))\n",
        "\n",
        "        # print(\"output2 type : \" + str(output2.dtype))\n",
        "        # print(\"output2 shape : \" + str(output2.shape))\n",
        "        # print(\"is cuda : \" + str(output2.is_cuda))\n",
        "\n",
        "        # print(\"output3 type : \" + str(output3.dtype))\n",
        "        # print(\"output3 shape : \" + str(output3.shape))\n",
        "        # print(\"is cuda : \" + str(output3.is_cuda))\n",
        "        # print(\"\\n\")\n",
        "\n",
        "        cat = torch.cat((output1.pooler_output, output2.pooler_output, output4.pooler_output), 1)\n",
        "        # print(\"cat shape : \" + str(cat.shape))\n",
        "        # print(\"cat type : \" + str(cat.dtype))\n",
        "        # print(\"is cuda : \" + str(cat.is_cuda))\n",
        "\n",
        "        # print(\"credit_score shape : \" + str(credit_score.shape))\n",
        "        # print(\"credit_score type : \" + str(credit_score.dtype))\n",
        "        # print(\"is cuda : \" + str(credit_score.is_cuda))\n",
        "\n",
        "        # ハードコードでcredit_scoreとcatのshapeを合わせる\n",
        "        reshaped_credit_score = torch.zeros(self.batch_size, self.model.config.hidden_size*3, dtype=torch.float32)\n",
        "        for row in range(self.batch_size):\n",
        "          for column in range(self.model.config.hidden_size*3):\n",
        "            reshaped_credit_score[row, column] = credit_score[row]\n",
        "\n",
        "        reshaped_credit_score = reshaped_credit_score.cuda()\n",
        "\n",
        "        # print(\"reshaped_credit_score shape : \" + str(reshaped_credit_score.shape))\n",
        "        # print(\"reshaped_credit_score tyep : \" +str(reshaped_credit_score.dtype))\n",
        "        # print(\"is cuda : \" + str(reshaped_credit_score.is_cuda))\n",
        "\n",
        "        out = torch.add(reshaped_credit_score, cat)\n",
        "        out = self.dropout(out)\n",
        "        # print(\"credit_score was added\")\n",
        "        # print(\"out shape : \" + str(out.shape))\n",
        "        # print(\"out item type : \" + str(out.dtype))\n",
        "        # print(\"is cuda : \" + str(out.is_cuda))\n",
        "\n",
        "        out = self.classifier(out)\n",
        "        # print(\"passed classifier\")\n",
        "        # print(\"out shape : \" + str(out.shape))\n",
        "        # print(\"out type : \" + str(out.dtype))\n",
        "        # print(\"is cuda : \" + str(out.is_cuda))\n",
        "\n",
        "        output = torch.sigmoid(out)\n",
        "        # print(\"passed sigmoid\")\n",
        "        # print(\"output shape : \" + str(out.shape))\n",
        "        # print(\"output type : \" + str(out.dtype))\n",
        "        # print(\"is cuda : \" + str(out.is_cuda))\n",
        "        # print(\"\\n\")\n",
        "\n",
        "        # print(f\"labels : {labels}\")\n",
        "\n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = self.criterion(output, labels)\n",
        "        #     print(\"loss(output of self.criterion) : \" + str(loss))\n",
        "        # print(\"=================================================================\\n\")\n",
        "        return loss, output\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "\n",
        "        :param batch:\n",
        "        :param batch_idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        input_ids1 = batch[\"input_ids1\"]\n",
        "        attention_mask1 = batch[\"attention_mask1\"]\n",
        "\n",
        "        input_ids2 = batch[\"input_ids2\"]\n",
        "        attention_mask2 = batch[\"attention_mask2\"]\n",
        "\n",
        "        input_ids3 = batch[\"input_ids3\"]\n",
        "        attention_mask3 = batch[\"attention_mask3\"]\n",
        "\n",
        "        input_ids4 = batch[\"input_ids4\"]\n",
        "        attention_mask4 = batch[\"attention_mask4\"]\n",
        "\n",
        "        credit_score = batch[\"credit_score\"]\n",
        "\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        loss, output = self(input_ids1, attention_mask1, input_ids2, attention_mask2, input_ids3, attention_mask3, input_ids4, attention_mask4, credit_score, labels)\n",
        "        self.log(\"train_loss_output\", loss, prog_bar=True, logger=True, on_step=True)\n",
        "        self.training_step_outputs.append(loss)\n",
        "\n",
        "        binary_outputs = torch.as_tensor((output - 0.5) > 0,dtype=torch.int)\n",
        "        self.log(\"train accuracy\", self.accuracy(binary_outputs,\n",
        "                                                 torch.as_tensor(labels, dtype=torch.int)),\n",
        "                 prog_bar=True, logger=True, on_step=True)\n",
        "\n",
        "        # print(f\"training_step_outputs : {self.training_step_outputs}\")\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "        # print(f'type(np_loss) : {type(np_loss)}')\n",
        "        # print(f'np_loss : {np_loss}')\n",
        "\n",
        "        self.result_manager.trainLoss_batch.append(np_loss)\n",
        "        return {\"loss\": loss, \"predictions\": output, \"labels\": labels}\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        # do something with all training_step outputs, for example:\n",
        "        if len(self.training_step_outputs) != 0:\n",
        "            epoch_mean = torch.stack(self.training_step_outputs).mean()\n",
        "            self.log(\"training_epoch_mean\", epoch_mean)\n",
        "            # free up the memory\n",
        "            self.training_step_outputs.clear()\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        :param batch:\n",
        "        :param batch_idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        input_ids1 = batch[\"input_ids1\"]\n",
        "        attention_mask1 = batch[\"attention_mask1\"]\n",
        "\n",
        "        input_ids2 = batch[\"input_ids2\"]\n",
        "        attention_mask2 = batch[\"attention_mask2\"]\n",
        "\n",
        "        input_ids3 = batch[\"input_ids3\"]\n",
        "        attention_mask3 = batch[\"attention_mask3\"]\n",
        "\n",
        "        input_ids4 = batch[\"input_ids4\"]\n",
        "        attention_mask4 = batch[\"attention_mask4\"]\n",
        "\n",
        "        credit_score = batch[\"credit_score\"]\n",
        "\n",
        "        labels = batch[\"labels\"]\n",
        "        loss, _ = self(input_ids1, attention_mask1, input_ids2, attention_mask2, input_ids3, attention_mask3, input_ids4, attention_mask4, credit_score, labels)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True, logger=True, on_step=True)\n",
        "        self.validation_step_outputs.append(loss)\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "        # print(f'type(np_loss) : {type(np_loss)}')\n",
        "        # print(f'np_loss : {np_loss}')\n",
        "        self.result_manager.valLoss_batch.append(np_loss)\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        # do something with all training_step outputs, for example:\n",
        "        if len(self.validation_step_outputs) != 0:\n",
        "            epoch_mean = torch.stack(self.validation_step_outputs).mean()\n",
        "            self.log(\"validation_epoch_mean\", epoch_mean)\n",
        "            # free up the memory\n",
        "            self.validation_step_outputs.clear()\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "\n",
        "        :param batch:\n",
        "        :param batch_idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        input_ids1 = batch[\"input_ids1\"]\n",
        "        attention_mask1 = batch[\"attention_mask1\"]\n",
        "\n",
        "        input_ids2 = batch[\"input_ids2\"]\n",
        "        attention_mask2 = batch[\"attention_mask2\"]\n",
        "\n",
        "        input_ids3 = batch[\"input_ids3\"]\n",
        "        attention_mask3 = batch[\"attention_mask3\"]\n",
        "\n",
        "        input_ids4 = batch[\"input_ids4\"]\n",
        "        attention_mask4 = batch[\"attention_mask4\"]\n",
        "\n",
        "        credit_score = batch[\"credit_score\"]\n",
        "\n",
        "        labels = batch[\"labels\"]\n",
        "        loss, outputs = self(input_ids1, attention_mask1, input_ids2, attention_mask2, input_ids3, attention_mask3, input_ids4, attention_mask4, credit_score, labels)\n",
        "        binary_outputs = torch.as_tensor((outputs - 0.5) > 0,\n",
        "                                         dtype=torch.int)\n",
        "        print(\"\\n\")\n",
        "        print(\"=================================================================\")\n",
        "        print(f\"labels : {labels}\")\n",
        "        print(f\"outputs : {outputs}\")\n",
        "        print(f\"binary_outputs : {binary_outputs}\")\n",
        "\n",
        "        print(\"=================================================================\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        self.log(\"test accuracy\", self.accuracy(binary_outputs,\n",
        "                                                torch.as_tensor(labels, dtype=torch.int)),\n",
        "                 prog_bar=True, logger=True, on_step=True)\n",
        "        self.log(\"test_loss\", loss, prog_bar=True, logger=True, on_step=True)\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "        # print(f'type(np_loss) : {type(np_loss)}')\n",
        "        # print(f'np_loss : {np_loss}')\n",
        "        self.result_manager.testLoss_batch.append(np_loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        optimizer = AdamW(self.parameters(), lr=self.config.lr)\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.n_warmup_steps,\n",
        "                                                    num_training_steps=self.n_training_steps)\n",
        "        return dict(optimizer=optimizer, lr_scheduler=dict(scheduler=scheduler, interval=\"step\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EPに関わるコード"
      ],
      "metadata": {
        "id": "veZpjBVa5OGT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR11tKO7nLkq"
      },
      "source": [
        "## class LIAR_PLUS_Dataset_For_EP(Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 537,
      "metadata": {
        "id": "0ESGrg3WnNvB"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# pylint: disable-msg=import-error\n",
        "# pylint: disable-msg=no-member\n",
        "# ========================================================\n",
        "\"\"\"dataset module is written for create data module\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from typing import Optional\n",
        "from time import sleep\n",
        "\n",
        "\n",
        "class LIAR_PLUS_Dataset_For_EP(Dataset):\n",
        "    \"\"\"\n",
        "    this class is for encoding input dataframe for BERT\n",
        "\n",
        "    Input dataframe needs to be consist of 4 columns, \"statement\", \"metadata\", \"justification\" and \"credit_score\"\n",
        "\n",
        "    later we wrap a lightning data module around it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data: pd.DataFrame, config_dict:dict, keywords:bool=False, gold_exp:bool=False):\n",
        "        self.data = data\n",
        "        self.config_dict = config_dict\n",
        "        self.prepare_tokenizer = Prepare_Tokenizer(self.config_dict)\n",
        "        self.tokenizer = self.prepare_tokenizer.get_tokenizer()\n",
        "        self.prepare_tokenizer.get_tokenizer_info()\n",
        "        self.max_token_len = config_dict['MAX_LENGTH']\n",
        "        self.keywords = keywords\n",
        "        self.gold_exp = gold_exp\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "\n",
        "        data_row = self.data.iloc[index]\n",
        "\n",
        "        statement = data_row[2] # statement\n",
        "        metadata = data_row[3] # metadata\n",
        "\n",
        "        credit_score = data_row[5] # credit_score\n",
        "        justification_on_true = data_row[7]\n",
        "        justification_on_false = data_row[8]\n",
        "\n",
        "        if data_row[1] == \"true\":\n",
        "            label = torch.tensor([1,0], dtype=torch.float32)\n",
        "        elif data_row[1] == \"half-true\":\n",
        "            label = torch.tensor([1,0], dtype=torch.float32)\n",
        "        elif data_row[1] == \"mostly-true\":\n",
        "            label = torch.tensor([1,0], dtype=torch.float32)\n",
        "        elif data_row[1] == \"false\":\n",
        "            label = torch.tensor([0,1], dtype=torch.float32)\n",
        "        elif data_row[1] == \"barely-true\":\n",
        "            label = torch.tensor([0,1], dtype=torch.float32)\n",
        "        elif data_row[1] == \"pants-fire\":\n",
        "            label = torch.tensor([0,1], dtype=torch.float32)\n",
        "        else:\n",
        "            print(f\"Invalid label was found! {data_row[1]}\")\n",
        "            sleep(30)\n",
        "\n",
        "\n",
        "        #label = torch.tensor([1,0], dtype=torch.float32) if data_row[1] == ('true' or 'half-true' or 'mostly-true') else torch.tensor([0,1], dtype=torch.float32)\n",
        "        #print(\"label type : \" + str(label.dtype))\n",
        "        # print(f\"data_row[1] : {data_row[1]} , label : {label}\")\n",
        "\n",
        "\n",
        "        statement_encoding = self.tokenizer.encode_plus(\n",
        "            statement,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        metadata_encoding = self.tokenizer.encode_plus(\n",
        "            metadata,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        justification_on_true_encoding = self.tokenizer.encode_plus(\n",
        "            justification_on_true,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        justification_on_false_encoding = self.tokenizer.encode_plus(\n",
        "            justification_on_false,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return dict(\n",
        "            input_ids1=statement_encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask1=statement_encoding[\"attention_mask\"].flatten(),\n",
        "            input_ids2=metadata_encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask2=metadata_encoding[\"attention_mask\"].flatten(),\n",
        "            input_ids3=justification_on_true_encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask3=justification_on_true_encoding[\"attention_mask\"].flatten(),\n",
        "            input_ids4=justification_on_false_encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask4=justification_on_false_encoding[\"attention_mask\"].flatten(),\n",
        "            credit_score=credit_score,\n",
        "            labels=torch.Tensor(label) # この行ちょっと怪しい\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## class LIAR_PLUS_DataModule_For_EP(pl.LightningDataModule)"
      ],
      "metadata": {
        "id": "LZNRYk1v-i7A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 538,
      "metadata": {
        "id": "7GNriUcdnQ-R"
      },
      "outputs": [],
      "source": [
        "class LIAR_PLUS_DataModule_For_EP(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, config_dict, train_and_test_config, train_df: pd.DataFrame, test_df: pd.DataFrame, val_df: pd.DataFrame):\n",
        "        super().__init__()\n",
        "        self.config_dict = config_dict\n",
        "        self.train_and_test_config = train_and_test_config\n",
        "        self.train_df = train_df\n",
        "        self.test_df = test_df\n",
        "        self.val_df = val_df\n",
        "        self.training_batch_size = config_dict[\"TRAINING_BATCH_SIZE\"]\n",
        "        self.validation_batch_size = config_dict[\"VALIDATION_BATCH_SIZE\"]\n",
        "        self.test_batch_size = config_dict[\"TEST_BATCH_SIZE\"]\n",
        "        self.max_token_len = self.config_dict[\"MAX_LENGTH\"]\n",
        "        self.keywords = self.train_and_test_config[\"KEYWORDS\"]\n",
        "        self.gold_exp = self.train_and_test_config[\"GOLD_EXP\"]\n",
        "        self.train_dataset, self.test_dataset, self.val_dataset = None, None, None\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None) -> None:\n",
        "        self.train_dataset = LIAR_PLUS_Dataset_For_EP(self.train_df, self.config_dict, self.keywords, self.gold_exp)\n",
        "        print(f\"train_dataset size : {len(self.train_dataset)}\\n\")\n",
        "        self.test_dataset = LIAR_PLUS_Dataset_For_EP(self.test_df, self.config_dict, self.keywords, self.gold_exp)\n",
        "        print(f\"test_dataset size : {len(self.test_dataset)}\\n\")\n",
        "        self.val_dataset = LIAR_PLUS_Dataset_For_EP(self.val_df, self.config_dict,  self.keywords, self.gold_exp)\n",
        "        print(f\"val_dataset size : {len(self.val_dataset)}\\n\")\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.training_batch_size,\n",
        "            shuffle=True,\n",
        "            drop_last=True,\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.validation_batch_size,\n",
        "            drop_last=True,\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            drop_last=True,\n",
        "            num_workers=2\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRhVu9eanc3J"
      },
      "source": [
        "## EP.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 539,
      "metadata": {
        "id": "9HuYgyu3dj0d"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# pylint: disable=too-many-arguments\n",
        "# pylint: disable=import-error\n",
        "# ========================================================\n",
        "\"\"\"This module is written for write BERT classifier.\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "from typing import List\n",
        "import pytorch_lightning as pl\n",
        "from torch import nn\n",
        "import torch\n",
        "import torchmetrics\n",
        "\n",
        "from transformers import AutoModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "class EP(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    creates a pytorch lightning model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config_dict,\n",
        "                 n_warmup_steps: int = None,\n",
        "                 n_training_steps: int = None,\n",
        "                 n_classes: int = None,\n",
        "                 result_manager = None):\n",
        "        super().__init__()\n",
        "        self.config_dict = config_dict\n",
        "        self.result_manager = result_manager\n",
        "        self.prepare_tokenizer_for_true = Prepare_Tokenizer(self.config_dict)\n",
        "        self.prepare_tokenizer_for_false = Prepare_Tokenizer(self.config_dict)\n",
        "        self.tokenizer = self.prepare_tokenizer_for_true.get_tokenizer()\n",
        "        self.prepare_tokenizer_for_true.get_tokenizer_info()\n",
        "        self.prepare_model_for_true = Prepare_Model(config=self.config_dict, prepare_tokenizer=self.prepare_tokenizer_for_true)\n",
        "        self.prepare_model_for_false = Prepare_Model(config=self.config_dict, prepare_tokenizer=self.prepare_tokenizer_for_false)\n",
        "        self.model_true = self.prepare_model_for_true.get_model()\n",
        "        self.model_false = self.prepare_model_for_false.get_model()\n",
        "        self.prepare_model_for_true.get_model_info()\n",
        "        self.prepare_model_for_false.get_model_info()\n",
        "        # self.bert = BertModel.from_pretrained(self.config_dict['LANGUAGE_MODEL_PATH'], return_dict=True, num_labels=n_classes)\n",
        "        self.fc = nn.Linear(self.model_true.config.hidden_size, self.model_true.config.hidden_size)\n",
        "        self.classifier = nn.Linear(self.model_true.config.hidden_size*3, n_classes)\n",
        "        self.linear_true = nn.Linear(self.model_true.config.hidden_size*3, 1)\n",
        "        self.linear_false = nn.Linear(self.model_false.config.hidden_size*3, 1)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        nn.init.xavier_normal_(self.classifier.weight)\n",
        "        #\n",
        "        self.batch_size = self.config_dict['TRAINING_BATCH_SIZE']\n",
        "        self.n_training_steps = n_training_steps\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.accuracy = torchmetrics.classification.BinaryAccuracy()\n",
        "        self.f1 = torchmetrics.classification.BinaryF1Score()\n",
        "\n",
        "        self.training_step_outputs = []\n",
        "        self.validation_step_outputs = []\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def _ESM(self, input_ids1, attention_mask1, input_ids2, attention_mask2, input_ids3, attention_mask3, credit_score, model_type):\n",
        "\n",
        "        # print(\"=================================================================\\n\")\n",
        "\n",
        "        print(f'credit_score : {credit_score}')\n",
        "\n",
        "        if model_type:\n",
        "            output1 = self.model_true(input_ids1, attention_mask=attention_mask1) # statement\n",
        "            output2 = self.model_true(input_ids2, attention_mask=attention_mask2) # metadata\n",
        "            output3 = self.model_true(input_ids3, attention_mask=attention_mask3) # justification_on_true\n",
        "        else:\n",
        "            output1 = self.model_false(input_ids1, attention_mask=attention_mask1) # statement\n",
        "            output2 = self.model_false(input_ids2, attention_mask=attention_mask2) # metadata\n",
        "            output3 = self.model_false(input_ids3, attention_mask=attention_mask3) # justification_on_false\n",
        "            # output1 = self.model_true(input_ids1, attention_mask=attention_mask1) # statement\n",
        "            # output2 = self.model_true(input_ids2, attention_mask=attention_mask2) # metadata\n",
        "            # output3 = self.model_true(input_ids3, attention_mask=attention_mask3) # justification_on_true\n",
        "\n",
        "        # print(\"output1 type : \" + str(type(output1)))\n",
        "        # print(\"output2 type : \" + str(type(output2)))\n",
        "        # print(\"output3 type : \" + str(type(output3)))\n",
        "        # print(\"\\n\")\n",
        "\n",
        "        # output1 = self.dropout(output1.pooler_output)\n",
        "        # output2 = self.dropout(output2.pooler_output)\n",
        "        # output3 = self.dropout(output3.pooler_output)\n",
        "        # print(\"passed dropout layer\")\n",
        "\n",
        "        # print(\"output1 type : \" + str(output1.dtype))\n",
        "        # print(\"output1 shape : \" + str(output1.shape))\n",
        "        # print(\"is cuda : \" + str(output1.is_cuda))\n",
        "\n",
        "        # print(\"output2 type : \" + str(output2.dtype))\n",
        "        # print(\"output2 shape : \" + str(output2.shape))\n",
        "        # print(\"is cuda : \" + str(output2.is_cuda))\n",
        "\n",
        "        # print(\"output3 type : \" + str(output3.dtype))\n",
        "        # print(\"output3 shape : \" + str(output3.shape))\n",
        "        # print(\"is cuda : \" + str(output3.is_cuda))\n",
        "        # print(\"\\n\")\n",
        "\n",
        "        cat = torch.cat((output1.pooler_output, output2.pooler_output, output3.pooler_output), 1)\n",
        "        # print(\"cat shape : \" + str(cat.shape))\n",
        "        # print(\"cat type : \" + str(cat.dtype))\n",
        "        # print(\"is cuda : \" + str(cat.is_cuda))\n",
        "\n",
        "        # print(\"credit_score shape : \" + str(credit_score.shape))\n",
        "        # print(\"credit_score type : \" + str(credit_score.dtype))\n",
        "        # print(\"is cuda : \" + str(credit_score.is_cuda))\n",
        "\n",
        "        # ハードコードでcredit_scoreとcatのshapeを合わせる\n",
        "        reshaped_credit_score = torch.zeros(self.batch_size, self.model_true.config.hidden_size*3, dtype=torch.float32)\n",
        "\n",
        "        print(\"reshaped_credit_score shape : \" + str(reshaped_credit_score.shape))\n",
        "        print(\"reshaped_credit_score tyep : \" +str(reshaped_credit_score.dtype))\n",
        "        for row in range(self.batch_size):\n",
        "          for column in range(self.model_true.config.hidden_size*3):\n",
        "            reshaped_credit_score[row, column] = credit_score[row]\n",
        "\n",
        "        reshaped_credit_score = reshaped_credit_score.cuda()\n",
        "\n",
        "\n",
        "        # print(\"is cuda : \" + str(reshaped_credit_score.is_cuda))\n",
        "\n",
        "        out = torch.add(reshaped_credit_score, cat)\n",
        "        intermidiate_score = self.dropout(out)\n",
        "        # print(\"credit_score was added\")\n",
        "        # print(\"out shape : \" + str(out.shape))\n",
        "        # print(\"out item type : \" + str(out.dtype))\n",
        "        # print(\"is cuda : \" + str(out.is_cuda))\n",
        "\n",
        "        return intermidiate_score\n",
        "\n",
        "    def _intermidiate_score_processor(self, intermidiate_score_true, intermidiate_score_false, processor_type):\n",
        "\n",
        "        if processor_type == 'independent':\n",
        "\n",
        "            true_score = self.linear_true(intermidiate_score_true)\n",
        "            false_score = self.linear_false(intermidiate_score_false)\n",
        "\n",
        "            # out =  self.classifier(intermidiate_score_true)\n",
        "            out = torch.cat([true_score, false_score], dim=1)\n",
        "\n",
        "            # print(f'true_score.dtype : {true_score.dtype}')\n",
        "            # print(f'true_score.shape : {true_score.shape}')\n",
        "\n",
        "            # print(f'false_score.dtype : {false_score.dtype}')\n",
        "            # print(f'false_score.shape : {false_score.shape}')\n",
        "\n",
        "            # print(f'out : {out}')\n",
        "            # print(f'out.dtype : {out.dtype}')\n",
        "            # print(f'out.shape : {out.shape}')\n",
        "\n",
        "            # out = self.classifier(out)\n",
        "            # print(\"passed classifier\")\n",
        "            # print(\"out shape : \" + str(out.shape))\n",
        "            # print(\"out type : \" + str(out.dtype))\n",
        "            # print(\"is cuda : \" + str(out.is_cuda))\n",
        "\n",
        "            scores = torch.sigmoid(out)\n",
        "            # print(f'scores : {scores}')\n",
        "            # print(f'scores.dtype : {scores.dtype}')\n",
        "            # print(f'scores.shape : {scores.shape}')\n",
        "\n",
        "            # out = self.sigmoid(out)\n",
        "            # print(\n",
        "            # print(\"passed sigmoid\")\n",
        "            # print(\"output shape : \" + str(out.shape))\n",
        "            # print(\"output type : \" + str(out.dtype))\n",
        "            # print(\"is cuda : \" + str(out.is_cuda))\n",
        "            # print(\"\\n\")\n",
        "\n",
        "            # print(f\"labels : {labels}\")\n",
        "        else:\n",
        "            ValueError('Wrong processor_type')\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, input_ids3, attention_mask3, input_ids4, attention_mask4, credit_score, labels=None):\n",
        "        \"\"\"\n",
        "\n",
        "        :param input_ids1:\n",
        "        :param attention_mask1:\n",
        "        :param input_ids2:\n",
        "        :param attention_mask2:\n",
        "        :param input_ids3:\n",
        "        :param attention_mask3:\n",
        "        :param labels:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        print(f'input_ids1 : {input_ids1}')\n",
        "\n",
        "        intermidiate_score_on_true = self._ESM(input_ids1, attention_mask1, input_ids2, attention_mask2, input_ids3, attention_mask3, credit_score, True)\n",
        "        intermidiate_score_on_false = self._ESM(input_ids1, attention_mask1, input_ids2, attention_mask2, input_ids4, attention_mask4, credit_score, False)\n",
        "\n",
        "        # print(f\"intermidiate_score_on_true : {intermidiate_score_on_true}\")\n",
        "        # print(f\"intermidiate_score_on_false : {intermidiate_score_on_false}\")\n",
        "\n",
        "        scores = self._intermidiate_score_processor(intermidiate_score_on_true, intermidiate_score_on_false, 'independent')\n",
        "        # print(f\"scores : {scores}\")\n",
        "\n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = self.criterion(scores, labels)\n",
        "        #     print(\"loss(output of self.criterion) : \" + str(loss))\n",
        "        # print(\"=================================================================\\n\")\n",
        "        return loss, scores\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "\n",
        "        :param batch:\n",
        "        :param batch_idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        input_ids1 = batch[\"input_ids1\"]\n",
        "        attention_mask1 = batch[\"attention_mask1\"]\n",
        "        # print(f\"input_ids1 : {input_ids1}\")\n",
        "        # print(f\"attention_mask1 : {attention_mask1}\")\n",
        "\n",
        "        input_ids2 = batch[\"input_ids2\"]\n",
        "        attention_mask2 = batch[\"attention_mask2\"]\n",
        "        # print(f\"input_ids2 : {input_ids2}\")\n",
        "        # print(f\"attention_mask2 : {attention_mask2}\")\n",
        "\n",
        "        input_ids3 = batch[\"input_ids3\"]\n",
        "        attention_mask3 = batch[\"attention_mask3\"]\n",
        "        # print(f\"input_ids3 : {input_ids3}\")\n",
        "        # print(f\"attention_mask3 : {attention_mask3}\")\n",
        "\n",
        "        input_ids4 = batch[\"input_ids4\"]\n",
        "        attention_mask4 = batch[\"attention_mask4\"]\n",
        "        # print(f\"input_ids4 : {input_ids4}\")\n",
        "        # print(f\"attention_mask4 : {attention_mask4}\")\n",
        "\n",
        "        credit_score = batch[\"credit_score\"]\n",
        "        print(f\"credit_score : {credit_score}\")\n",
        "\n",
        "        labels = batch[\"labels\"]\n",
        "        # print(f\"labels : {labels}\")\n",
        "\n",
        "        loss, output = self(input_ids1, attention_mask1, input_ids2, attention_mask2, input_ids3, attention_mask3, input_ids4, attention_mask4, credit_score, labels)\n",
        "        # print(f'loss : {loss}')\n",
        "        # print(f'output : {output}')\n",
        "\n",
        "        # self.log(\"train_loss_output\", loss, prog_bar=True, logger=True, on_step=True)\n",
        "        # self.training_step_outputs.append(loss)\n",
        "\n",
        "        binary_outputs = torch.as_tensor((output - 0.5) > 0,dtype=torch.int)\n",
        "        batch_accuracy = self.accuracy(binary_outputs, torch.as_tensor(labels, dtype=torch.int)).detach().cpu().numpy()\n",
        "        batch_f1 = self.f1(binary_outputs, torch.as_tensor(labels, dtype=torch.int)).detach().cpu().numpy()\n",
        "\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "        # print(f'type(np_loss) : {type(np_loss)}')\n",
        "        # print(f'np_loss : {np_loss}')\n",
        "        self.result_manager.trainLoss_batch.append(np_loss)\n",
        "        self.result_manager.trainAcc_batch.append(batch_accuracy)\n",
        "        self.result_manager.trainF1_batch.append(batch_f1)\n",
        "\n",
        "        return {\"loss\": loss, \"predictions\": output, \"labels\": labels}\n",
        "\n",
        "    # def on_train_epoch_end(self):\n",
        "    #     # do something with all training_step outputs, for example:\n",
        "    #     if len(self.training_step_outputs) != 0:\n",
        "    #         epoch_mean = torch.stack(self.training_step_outputs).mean()\n",
        "    #         self.log(\"training_epoch_mean\", epoch_mean)\n",
        "    #         # free up the memory\n",
        "    #         self.training_step_outputs.clear()\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        :param batch:\n",
        "        :param batch_idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        input_ids1 = batch[\"input_ids1\"]\n",
        "        attention_mask1 = batch[\"attention_mask1\"]\n",
        "\n",
        "        input_ids2 = batch[\"input_ids2\"]\n",
        "        attention_mask2 = batch[\"attention_mask2\"]\n",
        "\n",
        "        input_ids3 = batch[\"input_ids3\"]\n",
        "        attention_mask3 = batch[\"attention_mask3\"]\n",
        "\n",
        "        input_ids4 = batch[\"input_ids4\"]\n",
        "        attention_mask4 = batch[\"attention_mask4\"]\n",
        "\n",
        "        credit_score = batch[\"credit_score\"]\n",
        "\n",
        "        labels = batch[\"labels\"]\n",
        "        loss, output = self(input_ids1, attention_mask1, input_ids2, attention_mask2, input_ids3, attention_mask3, input_ids4, attention_mask4, credit_score, labels)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True, logger=True, on_step=True)\n",
        "        self.validation_step_outputs.append(loss)\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "\n",
        "        binary_outputs = torch.as_tensor((output - 0.5) > 0,dtype=torch.int)\n",
        "        batch_accuracy = self.accuracy(binary_outputs, torch.as_tensor(labels, dtype=torch.int)).detach().cpu().numpy()\n",
        "        batch_f1 = self.f1(binary_outputs, torch.as_tensor(labels, dtype=torch.int)).detach().cpu().numpy()\n",
        "\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "        # print(f'type(np_loss) : {type(np_loss)}')\n",
        "        # print(f'np_loss : {np_loss}')\n",
        "        self.result_manager.valLoss_batch.append(np_loss)\n",
        "        self.result_manager.valAcc_batch.append(batch_accuracy)\n",
        "        self.result_manager.valF1_batch.append(batch_f1)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # def on_validation_epoch_end(self):\n",
        "    #     # do something with all training_step outputs, for example:\n",
        "    #     if len(self.validation_step_outputs) != 0:\n",
        "    #         epoch_mean = torch.stack(self.validation_step_outputs).mean()\n",
        "    #         self.log(\"validation_epoch_mean\", epoch_mean)\n",
        "    #         # free up the memory\n",
        "    #         self.validation_step_outputs.clear()\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "\n",
        "        :param batch:\n",
        "        :param batch_idx:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        input_ids1 = batch[\"input_ids1\"]\n",
        "        attention_mask1 = batch[\"attention_mask1\"]\n",
        "\n",
        "        input_ids2 = batch[\"input_ids2\"]\n",
        "        attention_mask2 = batch[\"attention_mask2\"]\n",
        "\n",
        "        input_ids3 = batch[\"input_ids3\"]\n",
        "        attention_mask3 = batch[\"attention_mask3\"]\n",
        "\n",
        "        input_ids4 = batch[\"input_ids4\"]\n",
        "        attention_mask4 = batch[\"attention_mask4\"]\n",
        "\n",
        "        credit_score = batch[\"credit_score\"]\n",
        "\n",
        "        labels = batch[\"labels\"]\n",
        "        loss, output = self(input_ids1, attention_mask1, input_ids2, attention_mask2, input_ids3, attention_mask3, input_ids4, attention_mask4, credit_score, labels)\n",
        "\n",
        "        self.log(\"test_loss\", loss, prog_bar=True, logger=True, on_step=True)\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "\n",
        "        binary_outputs = torch.as_tensor((output - 0.5) > 0,dtype=torch.int)\n",
        "        batch_accuracy = self.accuracy(binary_outputs, torch.as_tensor(labels, dtype=torch.int)).detach().cpu().numpy()\n",
        "        batch_f1 = self.f1(binary_outputs, torch.as_tensor(labels, dtype=torch.int)).detach().cpu().numpy()\n",
        "\n",
        "        np_loss = loss.detach().cpu().numpy()\n",
        "        # print(f'type(np_loss) : {type(np_loss)}')\n",
        "        # print(f'np_loss : {np_loss}')\n",
        "        self.result_manager.testLoss_batch.append(np_loss)\n",
        "        self.result_manager.testAcc_batch.append(batch_accuracy)\n",
        "        self.result_manager.testF1_batch.append(batch_f1)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        optimizer = AdamW(self.parameters(), lr=self.config_dict['LR'])\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.n_warmup_steps,\n",
        "                                                    num_training_steps=self.n_training_steps)\n",
        "        return dict(optimizer=optimizer, lr_scheduler=dict(scheduler=scheduler, interval=\"step\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz1KK4JlnnBC"
      },
      "source": [
        "# calculate_warmup_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 540,
      "metadata": {
        "id": "VlEzHkzLn6nX"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ========================================================\n",
        "\"\"\"This module is written for write useful function.\"\"\"\n",
        "# ========================================================\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# Imports\n",
        "# ========================================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_warmup_steps(train_df: pd.DataFrame, num_epochs: int, batch_size: int):\n",
        "    steps_per_epoch = len(train_df) // batch_size\n",
        "    total_training_steps = steps_per_epoch * num_epochs\n",
        "    warmup_steps = total_training_steps // 5\n",
        "    return total_training_steps, warmup_steps"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class Result_Manager\n",
        "- 結果の保存や表示を行う\n",
        "- 次の指標を保存する\n",
        "## バッチごと\n",
        "- trainAcc\n",
        "- trainF1\n",
        "- trainloss\n",
        "- valAcc\n",
        "- valF1\n",
        "- valloss\n",
        "- testAcc\n",
        "- testF1\n",
        "- testloss\n",
        "\n",
        "\n",
        "## エポックごと\n",
        "- trainAcc\n",
        "- trainF1\n",
        "- trainloss\n",
        "- valAcc\n",
        "- valF1\n",
        "- valoss\n"
      ],
      "metadata": {
        "id": "ylG8KaRfYSd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "import datetime\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "yHeo6y028rRe"
      },
      "execution_count": 541,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class  Result_Manager():\n",
        "    def __init__(self, base_time, batch_size, model_config, set_type):\n",
        "        self.model_config = model_config\n",
        "        self.batch_size = batch_size\n",
        "        self.set_type = set_type\n",
        "        self.epoch_size = None\n",
        "        self.dt_now = base_time\n",
        "        self.metrics_name_list = [\n",
        "            'trainAcc_batch',\n",
        "            'trainF1_batch',\n",
        "            'trainLoss_batch',\n",
        "            'valAcc_batch',\n",
        "            'valF1_batch',\n",
        "            'valLoss_batch',\n",
        "            'testAcc_batch',\n",
        "            'testF1_batch',\n",
        "            'testLoss_batch',\n",
        "            'trainAcc_epoch',\n",
        "            'trainF1_epoch',\n",
        "            'trainLoss_epoch',\n",
        "            'valAcc_epoch',\n",
        "            'valF1_epoch',\n",
        "            'valLoss_epoch',\n",
        "        ]\n",
        "\n",
        "        self.trainAcc_batch = []\n",
        "        self.trainF1_batch = []\n",
        "        self.trainLoss_batch = []\n",
        "        self.valAcc_batch = []\n",
        "        self.valF1_batch = []\n",
        "        self.valLoss_batch = []\n",
        "        self.testAcc_batch = []\n",
        "        self.testF1_batch = []\n",
        "        self.testLoss_batch = []\n",
        "\n",
        "        self.trainAcc_epoch = []\n",
        "        self.trainF1_epoch = []\n",
        "        self.trainLoss_epoch = []\n",
        "        self.valAcc_epoch = []\n",
        "        self.valF1_epoch = []\n",
        "        self.valLoss_epoch = []\n",
        "\n",
        "        self.metrics_list = [\n",
        "            self.trainAcc_batch,\n",
        "            self.trainF1_batch,\n",
        "            self.trainLoss_batch,\n",
        "            self.valAcc_batch,\n",
        "            self.valF1_batch,\n",
        "            self.valLoss_batch,\n",
        "            self.testAcc_batch,\n",
        "            self.testF1_batch,\n",
        "            self.testLoss_batch,\n",
        "            self.trainAcc_epoch,\n",
        "            self.trainF1_epoch,\n",
        "            self.trainLoss_epoch,\n",
        "            self.valAcc_epoch,\n",
        "            self.valF1_epoch,\n",
        "            self.valLoss_epoch,\n",
        "        ]\n",
        "\n",
        "    def _build_folder(self, base_path: str):\n",
        "\n",
        "        for folder in self.model_config['MODEL_FOLDER']:\n",
        "            base_path = f'{base_path}/{folder}'\n",
        "            if not os.path.exists(base_path):\n",
        "                os.mkdir(base_path)\n",
        "\n",
        "        return base_path\n",
        "\n",
        "\n",
        "    def save_result_log(self, base_path: str):\n",
        "\n",
        "        base_path = f'{base_path}/result_log/{self.dt_now}'\n",
        "        if not os.path.exists(base_path):\n",
        "            os.mkdir(base_path)\n",
        "\n",
        "        model_path = self._build_folder(base_path)\n",
        "\n",
        "        folder = f'{model_path}/{self.set_type}_batch_size:{self.batch_size}'\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "\n",
        "        for idx in range(len(self.metrics_list)):\n",
        "\n",
        "            if len(self.metrics_list[idx]) != 0:\n",
        "                file_name = self.metrics_name_list[idx]+'.csv'\n",
        "                # print(f'file_name : {file_name}')\n",
        "                # np_metric = [item.detach().cpu().numpy().astype(float) for item in self.metrics_list[idx]]\n",
        "                np_metric = self.metrics_list[idx]\n",
        "                print(self.metrics_name_list[idx])\n",
        "                print(f'len(np_metric) : {len(np_metric)}')\n",
        "                data = pd.DataFrame({self.metrics_name_list[idx]:np_metric})\n",
        "                data.to_csv(f'{folder}/{file_name}', header=True, index=False)\n",
        "\n",
        "\n",
        "        # pd.Series(self.trainAcc_batch).to_csv(folder+'/trainAcc_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainF1_batch).to_csv(folder+'/trainF1_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainLoss_batch).to_csv(folder+'/trainLoss_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valAcc_batch).to_csv(folder+'/valAcc_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valF1_batch).to_csv(folder+'/valF1_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valLoss_batch).to_csv(folder+'/valLoss_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.testAcc_batch).to_csv(folder+'/testAcc_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.testF1_batch).to_csv(folder+'/testF1_batch.csv', header=False, index=False)\n",
        "        # pd.Series(self.testLoss_batch).to_csv(folder+'/testLoss_batch.csv', header=False, index=False)\n",
        "\n",
        "        # pd.Series(self.trainAcc_epoch).to_csv(folder+'/trainAcc_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainF1_epoch).to_csv(folder+'/trainF1_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.trainLoss_epoch).to_csv(folder+'/trainLoss_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valAcc_epoch).to_csv(folder+'/valAcc_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valF1_epoch).to_csv(folder+'/valF1_epoch.csv', header=False, index=False)\n",
        "        # pd.Series(self.valLoss_epoch).to_csv(folder+'/valLoss_epoch.csv', header=False, index=False)\n",
        "\n",
        "\n",
        "    def make_single_result_img(self, base_path:str):\n",
        "\n",
        "        base_path = f'{base_path}/plot_figures/{self.dt_now}'\n",
        "        if not os.path.exists(base_path):\n",
        "            os.mkdir(base_path)\n",
        "\n",
        "        model_path = self._build_folder(base_path)\n",
        "\n",
        "        folder = f'{model_path}/{self.set_type}_batch_size:{self.batch_size}'\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "\n",
        "        for idx in range(len(self.metrics_name_list)):\n",
        "\n",
        "            save_fig_path = f'/{folder}/' + self.metrics_name_list[idx] + '.png'\n",
        "\n",
        "            if len(self.metrics_list[idx]) == 0:\n",
        "                continue\n",
        "            else:\n",
        "                plt.figure()\n",
        "                np_metric = self.metrics_list[idx]\n",
        "                # print(f'type(np_metric) : {type(np_metric)}')\n",
        "                # print(self.metrics_name_list[idx])\n",
        "                # print(f'np_metric : {np_metric}')\n",
        "                data = pd.DataFrame({self.metrics_name_list[idx]:np_metric})\n",
        "                # print(f'type(data) : {type(data)}')\n",
        "                data = data.astype(float)\n",
        "                # print(f'type(data) : {type(data)}')\n",
        "                data.plot(\n",
        "                title=self.metrics_name_list[idx],\n",
        "                legend=True\n",
        "                )\n",
        "                plt.savefig(save_fig_path)\n",
        "                # plt.close('all')\n",
        "\n",
        "\n",
        "            # plt.figure()\n",
        "            # if len(self.metrics_list[idx]) != 0:\n",
        "            #     np_metric = self.metrics_list[idx]\n",
        "            #     print(f'type(np_metric) : {type(np_metric)}')\n",
        "            #     print(self.metrics_name_list[idx])\n",
        "            #     print(f'np_metric : {np_metric}')\n",
        "            #     data = pd.Series(np_metric)\n",
        "            #     print(f'type(data) : {type(data)}')\n",
        "            #     data = data.astype(float)\n",
        "            #     print(f'type(data) : {type(data)}')\n",
        "            #     data.plot(\n",
        "            #     title=self.metrics_name_list[idx],\n",
        "            #     legend=True\n",
        "            #     )\n",
        "            # plt.savefig(save_fig_path)\n",
        "            # plt.close('all')\n",
        "\n",
        "    # def make_imgs_into_one(self, base_path:str, metrics: list, figure_name:str):\n",
        "\n",
        "    #     base_path = base_path + '/plot_figures/' + self.dt_now + '/'\n",
        "    #     imgs = []\n",
        "\n",
        "    #     for i in range(len(metrics)):\n",
        "    #         j = base_path + metrics[i] + '.png'\n",
        "    #         imgs.append(j)\n",
        "    #         j = ''\n",
        "\n",
        "    #     save_fig_path = base_path + figure_name +'.png'\n",
        "    #     fig, axs = plt.subplots(1, len(metrics), figsize=(10, 5))\n",
        "\n",
        "    #     for i, im in zip(range(len(axs)), imgs):\n",
        "    #         img = Image.open(im)\n",
        "    #         axs[i].imshow(img)\n",
        "    #         axs[i].set_title(metrics[i])\n",
        "    #         axs[i].axis('off')\n",
        "\n",
        "    #     plt.show()\n",
        "    #     plt.savefig(save_fig_path)\n",
        "\n",
        "    # def make_data_into_one(self, base_path:str, metrics: list, figure_name:str):\n",
        "\n",
        "    #     base_path = base_path + '/result_log/' + self.dt_now + '/'\n",
        "    #     data = []\n",
        "\n",
        "    #     for i in range(len(metrics)):\n",
        "    #         data_path = base_path + i + '.csv'\n",
        "    #         data.append(pd.read_csv(data_path))\n",
        "    #         data_path = ''\n",
        "\n",
        "    #     save_fig_path = base_path + figure_name +'.png'\n",
        "    #     fig, axs = plt.subplots(1, len(metrics), figsize=(10, 5))\n",
        "\n",
        "    #     for i, im in zip(range(len(axs)), imgs):\n",
        "    #         img = Image.open(im)\n",
        "    #         axs[i].imshow(img) # cmap=カラーマップの選択\n",
        "    #         axs[i].set_title(metrics[i])\n",
        "    #         axs[i].axis('off')\n",
        "\n",
        "    #     plt.show()\n",
        "    #     plt.savefig(save_fig_path)\n",
        "\n",
        "    # def get_result_img(self, base_path:str, image_name:str):\n",
        "    #     imgs = base_path + '/plot_figures/' + self.dt_now + '/' + image_name\n",
        "    #     img = imread(fname=imgs, format='png')\n",
        "    #     plt.imshow(\n",
        "    #         img,\n",
        "    #     )\n",
        "    #     plt.show()"
      ],
      "metadata": {
        "id": "s0FmH8BGY5Zw"
      },
      "execution_count": 542,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# base_path = '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning'\n",
        "\n",
        "# my_Result_Manager = Result_Manager()\n",
        "# my_Result_Manager.save_result_log(base_path)\n",
        "# my_Result_Manager.make_single_result_img(base_path)\n",
        "# my_Result_Manager.make_imgs_into_one(base_path, ['trainLoss_batch', 'valLoss_batch','trainLoss_batch', 'valLoss_batch', 'trainLoss_batch', 'valLoss_batch'], 'example_image')"
      ],
      "metadata": {
        "id": "zx_rIKKtDDqm"
      },
      "execution_count": 543,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train_and_test.py"
      ],
      "metadata": {
        "id": "0bg6ig3P7DpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer"
      ],
      "metadata": {
        "id": "srlpkB_e_iAO"
      },
      "execution_count": 544,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train_and_test_config"
      ],
      "metadata": {
        "id": "SyN3KADV-0wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_test_config =  {\n",
        "    'MODEL_NAME' : ['bert_base_config', 'roberta_base_config'],\n",
        "    'BATCH_SIZE' : [1, 2, 4, 8], # train\n",
        "    'RANDOM_SEED' : 42,\n",
        "    'TRAINING_DATA_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/delete this.tsv',\n",
        "    'VALIDATION_DATA_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/delete this.tsv',\n",
        "    'TEST_DATA_PATH' : '/content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/data/processed/extended/delete this.tsv',\n",
        "    'KEYWORDS' : False,\n",
        "    'GOLD_EXP' : False,\n",
        "    'INTERMIDIATE_PROCESSING_TYPE':'independent', # 'independent' or 'aggregate'\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "3vB86Lag_jaR"
      },
      "execution_count": 545,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train_and_test_EP"
      ],
      "metadata": {
        "id": "9UpvSqbY_Dak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_test_EP(model_config, train_and_test_config, batch_size:int, base_time, set_type):\n",
        "\n",
        "    print(f'----------- training and testing has just started (batch_size : {batch_size}) -----------')\n",
        "    pl.seed_everything(train_and_test_config['RANDOM_SEED'])\n",
        "\n",
        "    train_data = read_tsv(train_and_test_config[\"TRAINING_DATA_PATH\"])\n",
        "    test_data = read_tsv(train_and_test_config[\"TEST_DATA_PATH\"])\n",
        "    val_data = read_tsv(train_and_test_config[\"VALIDATION_DATA_PATH\"])\n",
        "    model_config['TRAINING_BATCH_SIZE'] = batch_size\n",
        "    result_manager = Result_Manager(base_time, batch_size, model_config, set_type)\n",
        "    base_path = path_dict['TensorBoardLogger'] + f'/{base_time}'\n",
        "    folder = model_config['TensorBoardLogger_NAME'] + f'/batch_size:{batch_size}'\n",
        "\n",
        "    data_module =  LIAR_PLUS_DataModule_For_EP(\n",
        "                    model_config,\n",
        "                    train_and_test_config,\n",
        "                    train_data,\n",
        "                    test_data,\n",
        "                    val_data\n",
        "                    )\n",
        "\n",
        "    total_training_steps, warmup_steps = calculate_warmup_steps(train_data, model_config[\"TRAINING_EPOCHS\"],\n",
        "                                                        model_config[\"TRAINING_BATCH_SIZE\"])\n",
        "    print(f\"total_training_steps : {total_training_steps} , warmup_steps : {warmup_steps} \")\n",
        "    model = EP(\n",
        "            config_dict=model_config,\n",
        "            n_warmup_steps=warmup_steps,\n",
        "            n_training_steps=total_training_steps,\n",
        "            n_classes=2,\n",
        "            result_manager=result_manager)\n",
        "\n",
        "    checkpoint_callback, saved_model_path = build_checkpoint_callback(model_config, base_time, batch_size, set_type)\n",
        "    logger = TensorBoardLogger(base_path, name=folder)\n",
        "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        logger=logger,\n",
        "        callbacks=[checkpoint_callback, early_stopping_callback],\n",
        "        max_epochs=model_config['TRAINING_EPOCHS'])\n",
        "\n",
        "    trainer.fit(model, datamodule=data_module)\n",
        "    trainer.test(datamodule=data_module)\n",
        "\n",
        "    model.result_manager.save_result_log('/content/drive/MyDrive/B4yoshida/Enhanced Sequence Model with Pytorch Lightning')\n",
        "    model.result_manager.make_single_result_img('/content/drive/MyDrive/B4yoshida/Enhanced Sequence Model with Pytorch Lightning')\n"
      ],
      "metadata": {
        "id": "dcXkv54O_kWe"
      },
      "execution_count": 546,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_base_time"
      ],
      "metadata": {
        "id": "mNfr9k4U_RTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_time():\n",
        "\n",
        "    now_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "    base_time = str(now_time)[:-16]\n",
        "\n",
        "    print(f'you got base_time : {base_time}')\n",
        "\n",
        "    return base_time\n"
      ],
      "metadata": {
        "id": "VodoovBL_lK-"
      },
      "execution_count": 547,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train_and_test_EPの実行によるモデルの訓練"
      ],
      "metadata": {
        "id": "D75P2pfT_W7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bart-base-uncased\n",
        "base_time = 'delete this'\n",
        "for batch_size in [4]:\n",
        "\n",
        "    train_and_test_EP(\n",
        "        model_config=bert_base_config,\n",
        "        train_and_test_config=train_and_test_config,\n",
        "        batch_size=batch_size,\n",
        "        base_time=base_time,\n",
        "        set_type='base')"
      ],
      "metadata": {
        "id": "ie09fmSS_l_j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8790878ef6f44f9988876a70870d226f",
            "fd4ea4c8f59b427a982ec232de3cb8e3",
            "55c93e1245bf45a7aa4a38dc8f2b5a78",
            "6988af39dfc447b984b1ec039fc2d168",
            "9526ef9959a145c0a41ea9f4156ab469",
            "06d91b1561df467eb9a8408b83e5ea10",
            "d6e520ff2aa64d1d81fb174a1df94ca6",
            "e8234826c5714f9c9d4b4cb6e36aae79",
            "40a631e85de8419d91a2ea1c9dadda6c",
            "e1ec64b8a7d24a46b66728605737981e",
            "0287d7717b7243f7b90e4d68e9579824"
          ]
        },
        "outputId": "41b5a8c2-f5da-4ec3-9e27-2ed063ea5475"
      },
      "execution_count": 548,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- training and testing has just started (batch_size : 4) -----------\n",
            "total_training_steps : 10 , warmup_steps : 2 \n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : bert-base-uncased\n",
            "self.tokenizer : BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : None\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : None\n",
            "self.eos_token_id : None\n",
            "self.pad_token : None\n",
            "self.pad_token_id : None\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- model information ----------\n",
            "self.MODEL_NAME : bert-base-uncased\n",
            "---------- model information ----------\n",
            "self.MODEL_NAME : bert-base-uncased\n",
            "dirpath : /content/drive/MyDrive/B4yoshida/NILE with pytorch lightning/src/models/bert-base-uncased/delete this/base_batch_size=4\n",
            "checkpoint_callback : <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7d95bc7bbb20>\n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : bert-base-uncased\n",
            "self.tokenizer : BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : None\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : None\n",
            "self.eos_token_id : None\n",
            "self.pad_token : None\n",
            "self.pad_token_id : None\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : None\n",
            "train_dataset size : 20\n",
            "\n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : bert-base-uncased\n",
            "self.tokenizer : BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : None\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : None\n",
            "self.eos_token_id : None\n",
            "self.pad_token : None\n",
            "self.pad_token_id : None\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : None\n",
            "test_dataset size : 20\n",
            "\n",
            "---------- tokenizer information ----------\n",
            "self.TOKENIZER_NAME : bert-base-uncased\n",
            "self.tokenizer : BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.tokenizer_length : None\n",
            "self.additional_special_tokens : None\n",
            "self.eos_token : None\n",
            "self.eos_token_id : None\n",
            "self.pad_token : None\n",
            "self.pad_token_id : None\n",
            "self.exp_token : [EXP]\n",
            "self.exp_token_id : None\n",
            "val_dataset size : 20\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name         | Type           | Params\n",
            "------------------------------------------------\n",
            "0 | model_true   | BertModel      | 109 M \n",
            "1 | model_false  | BertModel      | 109 M \n",
            "2 | fc           | Linear         | 590 K \n",
            "3 | classifier   | Linear         | 4.6 K \n",
            "4 | linear_true  | Linear         | 2.3 K \n",
            "5 | linear_false | Linear         | 2.3 K \n",
            "6 | dropout      | Dropout        | 0     \n",
            "7 | criterion    | BCELoss        | 0     \n",
            "8 | accuracy     | BinaryAccuracy | 0     \n",
            "9 | f1           | BinaryF1Score  | 0     \n",
            "------------------------------------------------\n",
            "219 M     Trainable params\n",
            "0         Non-trainable params\n",
            "219 M     Total params\n",
            "878.257   Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8790878ef6f44f9988876a70870d226f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids1 : tensor([[  101,  2758,  1996,  8194,  2015,  2862,  2576,  2177,  6753,  2353,\n",
            "          1011, 12241, 20367, 11324,  2015,  2006,  5157,  1012,   102,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0]], device='cuda:0')\n",
            "credit_score : tensor([0.9000], device='cuda:0', dtype=torch.float64)\n",
            "credit_score : tensor([0.9000], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Using a target size (torch.Size([1, 2])) that is different to the input size (torch.Size([4, 2])) is deprecated. Please ensure they have the same size.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-548-4861a0e5e4ac>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     train_and_test_EP(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mmodel_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_base_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_and_test_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_and_test_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-546-fcd6eb52ad2b>\u001b[0m in \u001b[0;36mtrain_and_test_EP\u001b[0;34m(model_config, train_and_test_config, batch_size, base_time, set_type)\u001b[0m\n\u001b[1;32m     39\u001b[0m         max_epochs=model_config['TRAINING_EPOCHS'])\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    545\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         )\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m             \u001b[0mval_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_sanity_check_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# run step hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         )\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstep_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_processed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_redirection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-539-36bfcad54c1f>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredit_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-539-36bfcad54c1f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids1, attention_mask1, input_ids2, attention_mask2, input_ids3, attention_mask3, input_ids4, attention_mask4, credit_score, labels)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;31m#     print(\"loss(output of self.criterion) : \" + str(loss))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# print(\"=================================================================\\n\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3111\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3113\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   3114\u001b[0m             \u001b[0;34m\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3115\u001b[0m             \u001b[0;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([1, 2])) that is different to the input size (torch.Size([4, 2])) is deprecated. Please ensure they have the same size."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n"
      ],
      "metadata": {
        "id": "hNRlJOXKLe4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 以下、学習に大きくかかわらないコード"
      ],
      "metadata": {
        "id": "-so7br8tLqb9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2o4VkzDn81d"
      },
      "source": [
        "# traienr.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jpc7B1MoABL"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "# from utils import calculate_warmup_steps\n",
        "# from configuration.config import BaseConfig\n",
        "\n",
        "# from data_loader.data_reader import read_csv\n",
        "# from models import NegativeSamplingDataModule, SBERTModel, build_checkpoint_callback\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1ZSvIgHoJUZ"
      },
      "source": [
        "## colab上でのtrainer.pyの実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVwXNfCkoKFD"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "# from utils import calculate_warmup_steps\n",
        "# from configuration.config import BaseConfig\n",
        "\n",
        "# from data_loader.data_reader import read_csv\n",
        "# from models import NegativeSamplingDataModule, SBERTModel, build_checkpoint_callback\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_HJ-q_YoD1C"
      },
      "outputs": [],
      "source": [
        "train_data_path = '/content/drive/MyDrive/B4yoshida/Enhanced Sequence Model with Pytorch Lightning/data/processed/input_for_lightning_model/main/input_train2.tsv'\n",
        "test_data_path = '/content/drive/MyDrive/B4yoshida/Enhanced Sequence Model with Pytorch Lightning/data/processed/input_for_lightning_model/main/input_test2.tsv'\n",
        "val_data_path = '/content/drive/MyDrive/B4yoshida/Enhanced Sequence Model with Pytorch Lightning/data/processed/input_for_lightning_model/main/input_val2.tsv'\n",
        "\n",
        "# toy_train_data_path = '/content/drive/MyDrive/B4yoshida/Enhanced Sequence Model with Pytorch Lightning/data/processed/input_for_lightning_model/toy/toy_input_train2.tsv'\n",
        "# toy_test_data_path = '/content/drive/MyDrive/B4yoshida/Enhanced Sequence Model with Pytorch Lightning/data/processed/input_for_lightning_model/toy/toy_input_test2.tsv'\n",
        "# toy_val_data_path = '/content/drive/MyDrive/B4yoshida/Enhanced Sequence Model with Pytorch Lightning/data/processed/input_for_lightning_model/toy/toy_input_val2.tsv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSUQmhT2oUC-"
      },
      "outputs": [],
      "source": [
        "# CONFIG = BaseConfig_For_Dev(\n",
        "#     \"siamese-bert\", # model_name\n",
        "#     \"bert-base-uncased\", # language_model_path\n",
        "#     \"bert-base-uncased\", # language_model_tokenizer_path\n",
        "#     1, # save_top_k\n",
        "#     \"/content/drive/MyDrive/B4yoshida/Enhanced Sequence Model with Pytorch Lightning/src/models\", # saved_model_path\n",
        "#     \"/content/drive/MyDrive/B4yoshida/Enhanced Sequence Model with Pytorch Lightning/weights/csv_log\", # csv_model_path\n",
        "#     train_data_path, # train_data_path\n",
        "#     test_data_path, # test_data_path\n",
        "#     val_data_path, # val_data_path\n",
        "#     20, # n_epochs\n",
        "#     16, # batch_size\n",
        "#     256, # max_token_count\n",
        "#     7e-5, # lr\n",
        "#     42 # randdom_seed\n",
        "# )\n",
        "# # なぜかtest, valがtupleになっている\n",
        "# print(CONFIG.test_data_path)\n",
        "# print(CONFIG.train_data_path)\n",
        "# print(type(CONFIG.test_data_path[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTn_iuuMoW9b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# random_seed = CONFIG.random_seed\n",
        "# pl.seed_everything(random_seed)\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained(CONFIG.language_model_tokenizer_path)\n",
        "# print(\"tokenizer loaded .... \")\n",
        "# print(\"train_data_path : \" + CONFIG.train_data_path)\n",
        "# print(\"test_data_path : \" + CONFIG.test_data_path)\n",
        "# print(\"val_data_path : \" + CONFIG.val_data_path)\n",
        "\n",
        "# train_data = read_tsv(CONFIG.train_data_path)\n",
        "# test_data = read_tsv(CONFIG.test_data_path)\n",
        "# val_data = read_tsv(CONFIG.val_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GE_LUn3L5Ths"
      },
      "outputs": [],
      "source": [
        "# train_data[1].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82fd99FIocNm"
      },
      "outputs": [],
      "source": [
        "# data_module = LIAR_PLUS_DataModule(\n",
        "#     CONFIG,\n",
        "#     train_data,\n",
        "#     test_data,\n",
        "#     val_data,\n",
        "#     tokenizer,\n",
        "#     batch_size=CONFIG.batch_size)\n",
        "\n",
        "# total_training_steps, warmup_steps = calculate_warmup_steps(train_data, CONFIG.n_epochs,\n",
        "#                                                             CONFIG.batch_size)\n",
        "\n",
        "# print(f\"total_training_steps : {total_training_steps} , warmup_steps : {warmup_steps} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kuxtMjpoemJ"
      },
      "outputs": [],
      "source": [
        "# model = SBERTModel(\n",
        "#     config=CONFIG,\n",
        "#     n_warmup_steps=warmup_steps,\n",
        "#     n_training_steps=total_training_steps,\n",
        "#     n_classes=2\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PigsZdJaohPv"
      },
      "outputs": [],
      "source": [
        "# checkpoint_callback = build_checkpoint_callback(CONFIG.save_top_k, CONFIG.saved_model_path)\n",
        "\n",
        "# import datetime\n",
        "# t_delta = datetime.timedelta(hours=9)  # 9時間\n",
        "# JST = datetime.timezone(t_delta, 'JST')  # UTCから9時間差の「JST」タイムゾーン\n",
        "# dt = datetime.datetime.now(JST)\n",
        "\n",
        "# print(dt)\n",
        "# comment = \"「 another paper arch」\"\n",
        "# logger = TensorBoardLogger(\"lightning_logs\", name= f\"{dt} comment:{comment} n_epochs:{CONFIG.n_epochs} batch_size:{CONFIG.batch_size}\")\n",
        "# early_stopping_callback = EarlyStopping(monitor='val_loss', patience=15)\n",
        "# trainer = pl.Trainer(\n",
        "#     logger=logger,\n",
        "#     callbacks=[checkpoint_callback, early_stopping_callback],\n",
        "#     max_epochs=CONFIG.n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3Pwlo6NmfER"
      },
      "outputs": [],
      "source": [
        "# data_module.test_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEnYy12losg-"
      },
      "outputs": [],
      "source": [
        "# trainer.fit(model, datamodule=data_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQtFZz9pKDaC"
      },
      "outputs": [],
      "source": [
        "# trainer.test(dataloaders=data_module.test_dataloader())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorBoardで結果の確認"
      ],
      "metadata": {
        "id": "8J4pDnEV3vXS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJinUpPgtuJl"
      },
      "outputs": [],
      "source": [
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir=lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ランタイムの切断"
      ],
      "metadata": {
        "id": "OmNfX3_K32AS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLeZpAWLtu7W"
      },
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "sleep(120)\n",
        "\n",
        "from google.colab import runtime\n",
        "\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPA7ZbyGOg4W7nvfoUXghOm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8790878ef6f44f9988876a70870d226f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd4ea4c8f59b427a982ec232de3cb8e3",
              "IPY_MODEL_55c93e1245bf45a7aa4a38dc8f2b5a78",
              "IPY_MODEL_6988af39dfc447b984b1ec039fc2d168"
            ],
            "layout": "IPY_MODEL_9526ef9959a145c0a41ea9f4156ab469"
          }
        },
        "fd4ea4c8f59b427a982ec232de3cb8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06d91b1561df467eb9a8408b83e5ea10",
            "placeholder": "​",
            "style": "IPY_MODEL_d6e520ff2aa64d1d81fb174a1df94ca6",
            "value": "Sanity Checking DataLoader 0:   0%"
          }
        },
        "55c93e1245bf45a7aa4a38dc8f2b5a78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8234826c5714f9c9d4b4cb6e36aae79",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40a631e85de8419d91a2ea1c9dadda6c",
            "value": 0
          }
        },
        "6988af39dfc447b984b1ec039fc2d168": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1ec64b8a7d24a46b66728605737981e",
            "placeholder": "​",
            "style": "IPY_MODEL_0287d7717b7243f7b90e4d68e9579824",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "9526ef9959a145c0a41ea9f4156ab469": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "06d91b1561df467eb9a8408b83e5ea10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6e520ff2aa64d1d81fb174a1df94ca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8234826c5714f9c9d4b4cb6e36aae79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40a631e85de8419d91a2ea1c9dadda6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1ec64b8a7d24a46b66728605737981e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0287d7717b7243f7b90e4d68e9579824": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}